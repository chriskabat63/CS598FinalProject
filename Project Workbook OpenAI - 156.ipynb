{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title:  Project Workbook OpenAI\n",
    "\n",
    "Authors:  Matthew Lopes and Chris Kabat\n",
    "\n",
    "This notebook was created to allow for word/sentence embeddings to be created using Microsoft's Azure Open AI Cognitive Service to support our CS 598 DLH project. We do  actually create the embeddings in this notebook to avoid saving large files, but prepare the data for the creation of them. The paper we have chosen for the reproducibility project is:\n",
    "***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n",
    "\n",
    "Abstract:  The main goal of the paper is to extract Morbidity from clinical notes.  The idea was to use a combination of classical and deep learning methods to determine the best approach for classifying these notes in one or more of 16 morbidity conditions.  These models used a combination of NLP techniques including embeddings and bag of words implementations.  It also measured the effect including of stop words.  Lastly, it used ensemble techniques to tie together a number of the classical and deep learning models to provide the most accurate results.\n",
    "\n",
    "The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub.\n",
    "\n",
    "We are only creating embeddings for data that includes stop words.  Note, access to this service requires an Azure Subscription.  There is a cost for the service, so we only executed once and saved the dataframe with the stored embeddings.  You must also set the following environment variables:\n",
    "setx AZURE_OPENAI_API_KEY = \"(key)\"\n",
    "setx AZURE_OPENAI_ENDPOINT = \"(endpoint)\"\n",
    "\n",
    "In this workbook, we are taking the following steps:\n",
    "\n",
    "* Clean and tokenize the data\n",
    "* Retrieve and store the document and sentence vectors from the Azure Open AI Service.\n",
    "\n",
    " First we load the required libraries and get our environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai num2words matplotlib plotly scipy scikit-learn pandas tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "from num2words import num2words\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity, get_embeddings\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "np.random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "# define data path\n",
    "DATA_PATH = './obesity_data/'\n",
    "AOAI_PATH = './aoai/'\n",
    "\n",
    "alldocs_df = pd.read_pickle(DATA_PATH + '/alldocs_df.pkl')\n",
    "\n",
    "API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "RESOURCE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we see what models in the Azure Open AI Service we have access to.  This also tests the connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"scale_settings\": {\n",
      "        \"scale_type\": \"standard\"\n",
      "      },\n",
      "      \"model\": \"text-curie-001\",\n",
      "      \"owner\": \"organization-owner\",\n",
      "      \"id\": \"text-curie-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1673986855,\n",
      "      \"updated_at\": 1673986855,\n",
      "      \"object\": \"deployment\"\n",
      "    },\n",
      "    {\n",
      "      \"scale_settings\": {\n",
      "        \"scale_type\": \"standard\"\n",
      "      },\n",
      "      \"model\": \"text-davinci-002\",\n",
      "      \"owner\": \"organization-owner\",\n",
      "      \"id\": \"text-davinci-002\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1674680690,\n",
      "      \"updated_at\": 1674680690,\n",
      "      \"object\": \"deployment\"\n",
      "    },\n",
      "    {\n",
      "      \"scale_settings\": {\n",
      "        \"scale_type\": \"standard\"\n",
      "      },\n",
      "      \"model\": \"gpt-35-turbo\",\n",
      "      \"owner\": \"organization-owner\",\n",
      "      \"id\": \"gpt35kabat\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1678383593,\n",
      "      \"updated_at\": 1678383593,\n",
      "      \"object\": \"deployment\"\n",
      "    },\n",
      "    {\n",
      "      \"scale_settings\": {\n",
      "        \"scale_type\": \"standard\"\n",
      "      },\n",
      "      \"model\": \"text-davinci-003\",\n",
      "      \"owner\": \"organization-owner\",\n",
      "      \"id\": \"text-davinci-003\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1678892279,\n",
      "      \"updated_at\": 1678892279,\n",
      "      \"object\": \"deployment\"\n",
      "    },\n",
      "    {\n",
      "      \"scale_settings\": {\n",
      "        \"scale_type\": \"standard\"\n",
      "      },\n",
      "      \"model\": \"text-similarity-babbage-001\",\n",
      "      \"owner\": \"organization-owner\",\n",
      "      \"id\": \"text-similarity-babbage-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1682362667,\n",
      "      \"updated_at\": 1682362667,\n",
      "      \"object\": \"deployment\"\n",
      "    },\n",
      "    {\n",
      "      \"scale_settings\": {\n",
      "        \"scale_type\": \"standard\"\n",
      "      },\n",
      "      \"model\": \"text-similarity-ada-001\",\n",
      "      \"owner\": \"organization-owner\",\n",
      "      \"id\": \"text-similarity-ada-001\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1682362703,\n",
      "      \"updated_at\": 1682362703,\n",
      "      \"object\": \"deployment\"\n",
      "    },\n",
      "    {\n",
      "      \"scale_settings\": {\n",
      "        \"scale_type\": \"standard\"\n",
      "      },\n",
      "      \"model\": \"text-embedding-ada-002\",\n",
      "      \"owner\": \"organization-owner\",\n",
      "      \"id\": \"text-embedding-ada-002\",\n",
      "      \"status\": \"succeeded\",\n",
      "      \"created_at\": 1682385500,\n",
      "      \"updated_at\": 1682385500,\n",
      "      \"object\": \"deployment\"\n",
      "    }\n",
      "  ],\n",
      "  \"object\": \"list\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_key = API_KEY\n",
    "openai.api_base = RESOURCE_ENDPOINT\n",
    "openai.api_version = \"2022-12-01\"\n",
    "\n",
    "url = openai.api_base + \"/openai/deployments?api-version=2022-12-01\" \n",
    "\n",
    "r = requests.get(url, headers={\"api-key\": API_KEY})\n",
    "\n",
    "print(r.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we clean the data using some guidance from Microsoft's tutorial here: https://learn.microsoft.com/en-us/azure/cognitive-services/openai/tutorials/embeddings?tabs=command-line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Very minimal cleansing as discussed in the AOAI tutorial\n",
    "def normalize_text(s, sep_token = \" \\n \"):\n",
    "    s = re.sub(r'\\s+',  ' ', s).strip()\n",
    "    s = re.sub(r\". ,\",\"\",s)\n",
    "    # remove all instances of multiple spaces\n",
    "    s = s.replace(\"..\",\".\")\n",
    "    s = s.replace(\". .\",\".\")\n",
    "    s = s.replace(\"\\n\", \"\")\n",
    "    s = s.strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "alldocs_df['text_clean']= alldocs_df[\"text\"].apply(lambda x : normalize_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we deterimine the amount of tokens used and check to see if it fits within the service's token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# too big: 0\n",
      "Total Number of Tokens: 2012345\n"
     ]
    }
   ],
   "source": [
    "#Need to tokenize this for Azure Open AI, don't plan on splitting as they all fit within token limit\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "alldocs_df['n_tokens'] = alldocs_df[\"text_clean\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "print('# too big:',len(alldocs_df[alldocs_df.n_tokens>=8192]))\n",
    "print('Total Number of Tokens:',sum(alldocs_df['n_tokens']))   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code actually calls the service for each document using the ADA v2 embedding models from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This retrieves the embedding.  Since there is a cost, this is commented out.  To use this, you need to set two environment variables\n",
    "alldocs_df['ada_v2'] = alldocs_df[\"text_clean\"].apply(lambda x : get_embedding(x, engine = 'text-embedding-ada-002')) \n",
    "# engine should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\n",
    "alldocs_df.to_pickle(AOAI_PATH + '/alldocs_df_aoai.pkl') \n",
    "\n",
    "#commented embdding call because there is a charge - loading from pkl file to do analysis\n",
    "alldocs_df = pd.read_pickle(AOAI_PATH + '/alldocs_df_aoai.pkl') \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we prepare the data to be used by sentence instead of the entire document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sentences: 381\n"
     ]
    }
   ],
   "source": [
    "#Try and sentence tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "##Sentences - we can't do data cleansing until after sentence tokenized\n",
    "alldocs_df['sentence_tokenized'] = alldocs_df['text_clean'].apply(lambda x: sent_tokenize(x)) # this is a list of sentences\n",
    "\n",
    "alldocs_df['sentence_count'] = alldocs_df['sentence_tokenized'].apply(lambda x: len(x))\n",
    "sentence_max_aoai = np.max(alldocs_df['sentence_count'])\n",
    "print('Max Sentences:', sentence_max_aoai)\n",
    "\n",
    "\n",
    "#need to create tokens add '\\n' to reach max_sentences\n",
    "def token_and_pad_sentence(input_sentences, sentence_max):\n",
    "    pad_spaces = sentence_max - len(input_sentences)\n",
    "    result = input_sentences\n",
    "    if pad_spaces > 0:\n",
    "        for i in range(pad_spaces):\n",
    "            result.append('\\n')\n",
    "\n",
    "alldocs_df_expanded['sentence_tokenized'] = alldocs_df_expanded['sentence_tokenized'].apply(lambda x: token_and_pad_sentence(x, sentence_max))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we make sure the tokens will fit within the service limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# too big: 0\n",
      "Total Number of Tokens: 2012890\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Need to tokenize this for Azure Open AI, don't plan on splitting as they all fit within token limit\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def get_sentence_tokens(input_sentences):\n",
    "    tokens = 0\n",
    "    for isx, sentence in enumerate(input_sentences):\n",
    "        tokens = tokens + len(tokenizer.encode(sentence))\n",
    "    return tokens\n",
    "\n",
    "def get_max_sentence_tokens(input_sentences):\n",
    "    tokens = 0\n",
    "    for isx, sentence in enumerate(input_sentences):\n",
    "        sent_tokens  = len(tokenizer.encode(sentence))\n",
    "        if sent_tokens > tokens:\n",
    "            tokens = sent_tokens\n",
    "\n",
    "    return tokens\n",
    "\n",
    "alldocs_df['n_sent_tokens'] = alldocs_df[\"sentence_tokenized\"].apply(lambda x: get_sentence_tokens(x))\n",
    "alldocs_df['max_sent_tokens'] = alldocs_df[\"sentence_tokenized\"].apply(lambda x: get_max_sentence_tokens(x))\n",
    "\n",
    "print('# too big:',len(alldocs_df[alldocs_df.max_sent_tokens>=2046]))\n",
    "print('Total Number of Tokens:',sum(alldocs_df['n_sent_tokens']))   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we call the same embedding model but with sentences instead of the entire document.  Note, the service has limits on how often it can be called, so retry logic needed to be implemented.  We tried this for both the Ada and Babbage models.  Note, this generated a very large file (almost 7 GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get the sentence embeddings\n",
    "import time\n",
    "\n",
    "batch = 0\n",
    "\n",
    "def process_sentence(sentence):\n",
    "\n",
    "    done = False\n",
    "\n",
    "    return_array = None\n",
    "    cnt = 0\n",
    "\n",
    "    while not done:\n",
    "        try:\n",
    "            return_array = get_embedding(sentence, engine = 'text-similarity-babbage-001') #text-embedding-ada-002\n",
    "            done = True\n",
    "        except Exception as e:\n",
    "            print(f'Exception {batch} {str(e)}')\n",
    "            cnt = cnt + 1\n",
    "            if cnt > 5:\n",
    "                print('Too many retries')\n",
    "                done = True\n",
    "            else:\n",
    "                print('Sleeping')\n",
    "                time.sleep(60)\n",
    "    \n",
    "    return return_array\n",
    "\n",
    "def get_padded_embeddings(input_sentences, sentence_max):\n",
    "    global batch\n",
    "\n",
    "    #output_array = np.zeros((sentence_max, 1536))\n",
    "    output_array = np.zeros((sentence_max, 2048))\n",
    "    pad_zeros = sentence_max - len(input_sentences)\n",
    "    \n",
    "    batch = batch + 1\n",
    "    size = len(input_sentences)\n",
    "    print(f\"Running batch {batch}:Size:{size}\")\n",
    "\n",
    "    cnt = 0\n",
    "    done = False\n",
    "\n",
    "    for idx, sentence in enumerate(input_sentences):\n",
    "        output_array[idx,:] = process_sentence(sentence)\n",
    "\n",
    "    if pad_zeros > 0:\n",
    "        for i in range(pad_zeros):\n",
    "            idx = idx + 1\n",
    "            #output_array[idx,:] = np.zeros(1536)\n",
    "            output_array[idx,:] = np.zeros(2048)\n",
    "\n",
    "    return output_array\n",
    "\n",
    "#df_test = alldocs_df.head(2).copy()\n",
    "#df_test['ada_v2_sent'] = df_test[\"sentence_tokenized\"].apply(lambda x: get_padded_embeddings(x, sentence_max_aoai))\n",
    "#print(df_test['ada_v2_sent'])\n",
    "\n",
    "alldocs_df['ada_v2_sent'] = alldocs_df[\"sentence_tokenized\"].apply(lambda x: get_padded_embeddings(x, sentence_max_aoai))\n",
    "#alldocs_df['bab_v1_sent'] = alldocs_df[\"sentence_tokenized\"].apply(lambda x: get_padded_embeddings(x, sentence_max_aoai))\n",
    "alldocs_df.to_pickle(AOAI_PATH + '/alldocs_df_aoai.pkl') \n",
    "\n",
    "#commented embdding call because there is a charge - loading from pkl file to do analysis\n",
    "alldocs_df = pd.read_pickle(AOAI_PATH + '/alldocs_df_aoai.pkl') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we verify all of the data was returned as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There were a couple retries.  Make sure the shapes are all correct\n",
    "sum(alldocs_df['ada_v2_sent'].apply(lambda x: x.shape) != (381,1536))\n",
    "#sum(alldocs_df['bab_v1_sent'].apply(lambda x: x.shape) != (381,2048))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
