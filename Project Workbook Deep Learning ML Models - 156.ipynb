{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"3d1ehXFWM5aE"},"source":["This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n","***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n","\n","\n","\n"," "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gv360l2IkNfO"},"source":["The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub."]},{"cell_type":"code","execution_count":171,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1250,"status":"ok","timestamp":1679769727418,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"zyXrAo2dsJqf","outputId":"0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","import datetime\n","from datetime import timedelta\n","from tqdm import tqdm\n","import torchtext\n","\n","# set seed\n","seed = 24\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","# define data path\n","DATA_PATH = './obesity_data/'\n","RESULTS_PATH = './results/'\n","MODELS_PATH = './models/'\n","\n","if os.path.exists(RESULTS_PATH) == False:\n","    os.mkdir(RESULTS_PATH)\n","if os.path.exists(MODELS_PATH) == False:\n","    os.mkdir(MODELS_PATH)\n","\n","\n","test_df = pd.read_pickle(DATA_PATH + '/test.pkl') \n","train_df = pd.read_pickle(DATA_PATH + '/train.pkl') \n","#corpus = pd.read_pickle(DATA_PATH + '/corpus.pkl')\n","disease_list = test_df['disease'].unique().tolist()\n","embedding_list = ['GloVe', \"FastText\"]"]},{"cell_type":"code","execution_count":172,"metadata":{},"outputs":[],"source":["#rebuild the vocabulary\n","#import torchtext, torch, torch.nn.functional as F\n","#from torchtext.data.utils import get_tokenizer\n","#from torchtext.vocab import build_vocab_from_iterator\n","\n","#tokenizer = get_tokenizer(\"basic_english\")\n","#tokens = [tokenizer(doc) for doc in corpus]\n","\n","#voc = build_vocab_from_iterator(tokens, specials = ['<pad>'])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Deep Learning - Bag of Words - All Feature Selections - Averaged**\n","\n","![DL BagOfWords AllFeatures Averaged](images\\DL-BagOfWords-ByFeatureSelection.gif)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["****DL Model using word embeddings****\n","\n","First we start by creating a dataset.  Note this will have to take the disease as part of the init and filter just for those records."]},{"cell_type":"code","execution_count":173,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","class ClinicalNoteDataset(Dataset):\n","\n","    def __init__(self, dataframe, disease, dataformat):\n","        \"\"\"\n","        TODO: init the Dataset instance.  datafomat is just the column to use from the dataframe 'vector_tokenized' , 'one_hot'\n","        \"\"\"\n","        # your code here\n","        self.disease = disease\n","        self.dataformat = dataformat\n","        self.df = dataframe[dataframe['disease'] == disease].copy()\n","        self.df = self.df.reset_index()\n","\n","    def __len__(self):\n","        \"\"\"\n","        TODO: Denotes the total number of samples\n","        \"\"\"\n","        return len(self.df)\n","\n","    def __getitem__(self, i):\n","        \"\"\"\n","        TODO: Generates one sample of data\n","            return X, y for the i-th data.\n","        \"\"\"\n","        #Cannot make tensors yet, will need to happen in collate\n","        Y = self.df.iloc[i]['judgment']\n","        X = self.df.iloc[i][self.dataformat]\n","\n","        return X,Y\n","        \n","def vectorize_batch_GloVe(batch):\n","    embedding_size_used = 300\n","    vec = torchtext.vocab.GloVe(name='6B', dim=embedding_size_used)    \n","    Xi, Yi = batch[0]\n","    batch_size = len(batch)\n","\n","    X = torch.zeros(batch_size, len(Xi), embedding_size_used, dtype=torch.float)\n","    Y = torch.zeros((batch_size), dtype=torch.long)\n","    \n","    for i in range(len(batch)):\n","        x, y = batch[i]\n","        #vectors = vec.get_vecs_by_tokens(voc.lookup_tokens(x.tolist()))\n","        vectors = vec.get_vecs_by_tokens(x)\n","\n","        X[i] = vectors.float()\n","        Y[i] = torch.tensor(int(y == True))\n","\n","    return X,Y\n","\n","def vectorize_batch_FastText(batch):\n","    embedding_size_used = 300\n","    vec = torchtext.vocab.FastText()\n","    Xi, Yi = batch[0]\n","    batch_size = len(batch)\n","\n","    X = torch.zeros(batch_size, len(Xi), embedding_size_used, dtype=torch.float)\n","    Y = torch.zeros((batch_size), dtype=torch.long)\n","\n","    for i in range(len(batch)):\n","        x, y = batch[i]\n","        #vectors = vec.get_vecs_by_tokens(voc.lookup_tokens(x.tolist()))\n","        vectors = vec.get_vecs_by_tokens(x)\n","\n","        X[i] = vectors.float()\n","        Y[i] = torch.tensor(int(y == True))\n","\n","    return X,Y \n"," \n","          \n"]},{"cell_type":"code","execution_count":174,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["# of train batches: 6\n","# of val batches: 5\n","torch.Size([128, 1430, 300])\n","torch.Size([128])\n"]}],"source":["##Test DataLoader\n","batch_size = 128\n","train_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(train_df, 'Asthma', 'vector_tokenized'), batch_size = batch_size, shuffle=True, collate_fn=vectorize_batch_FastText)\n","val_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(test_df, 'Asthma', 'vector_tokenized'), batch_size = batch_size, shuffle=False, collate_fn=vectorize_batch_FastText)\n","\n","print(\"# of train batches:\", len(train_loader))\n","print(\"# of val batches:\", len(val_loader))\n","\n","train_iter = iter(train_loader)\n","x,y = next(train_iter)\n","\n","print(x.shape)\n","print(y.shape)\n"]},{"cell_type":"code","execution_count":175,"metadata":{},"outputs":[],"source":["class ClincalNoteEmbeddingNet(nn.Module):\n","    def __init__(self, embedding_type, max_tokens):\n","        super(ClincalNoteEmbeddingNet, self).__init__()\n","        \n","        self.embedding_type = embedding_type\n","        self.max_tokens = max_tokens\n","\n","        if(embedding_type == 'USE'):\n","            self.embedding_dimension = 512\n","        else:\n","            self.embedding_dimension = 300\n","\n","        self.hidden_dim1 = 128\n","        self.hidden_dim2 = 64\n","        self.num_layers = 1\n","\n","        #Because it is bidirectional, the output from LTSM is coming in twice the size of the hidden states required.\n","        #input is (batch, #of tokens * embedding_dimension)\n","        self.bilstm1 = nn.LSTM(input_size = self.embedding_dimension, hidden_size = self.hidden_dim1, bidirectional = True, batch_first = True, num_layers = self.num_layers) \n","        self.bilstm2 = nn.LSTM(input_size = self.hidden_dim1 * 2, hidden_size = self.hidden_dim2, bidirectional = True, batch_first = True, num_layers=self.num_layers)\n"," \n","        self.fc1 = nn.Linear(self.hidden_dim2 * self.max_tokens * 2, 2)\n"," \n","    def forward(self, x):\n","        x, states = self.bilstm1(x)\n","        x, states = self.bilstm2(x)\n","\n","        batch_size = x.shape[0]\n","        x = x.reshape(batch_size, -1)\n"," \n","        x = self.fc1(x)\n","        #x = F.softmax(x, dim=1) #tried this with NLLLLoss() but caused NaN issues\n","\n","        return x\n","\n"]},{"cell_type":"code","execution_count":176,"metadata":{},"outputs":[],"source":["def train_model(tmodel, train_dataloader, n_epoch=5, lr=0.003, device=None, model_name='unk'):\n","    import torch.optim as optim\n","    \n","    device = device or torch.device('cpu')\n","    tmodel.train()\n","\n","    loss_history = []\n","\n","    # your code here\n","    optimizer = optim.Adam(tmodel.parameters(), lr=0.01)\n","    # want to decay the learning rate as teh number of epochs get larger\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma = 0.1)\n","\n","    loss_func = nn.CrossEntropyLoss()\n","    #loss_func = nn.NLLLoss()\n","\n","    for epoch in range(n_epoch):\n","        epoch = epoch+1\n","        curr_epoch_loss = []\n","        start = time.time()\n","        for X, Y in tqdm(train_dataloader,desc=f\"Training {model_name}-Epoch {epoch}...\"):\n","            # your code here\n","            optimizer.zero_grad()\n","\n","            y_hat = tmodel(X)\n","\n","            loss = loss_func(y_hat, Y)\n","            #loss = loss_func(torch.log(y_hat), Y)\n","            \n","            loss.backward()\n","            optimizer.step()\n","\n","            curr_epoch_loss.append(loss.cpu().data.numpy())\n","\n","\n","        end = time.time()\n","        #if epoch % 10 == 0:\n","        print(f\"epoch{epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)},execution_time={str(datetime.timedelta(seconds = (end-start)))},lr={optimizer.param_groups[0]['lr']}\")\n","\n","        scheduler.step()\n","        loss_history += curr_epoch_loss\n","    return tmodel, loss_history\n","\n","def eval_model(emodel, dataloader, device=None, model_name='unk'):\n","    \"\"\"\n","    :return:\n","        pred_all: prediction of model on the dataloder.\n","        Y_test: truth labels. Should be an numpy array of ints\n","    TODO:\n","        evaluate the model using on the data in the dataloder.\n","        Add all the prediction and truth to the corresponding list\n","        Convert pred_all and Y_test to numpy arrays \n","    \"\"\"\n","    device = device or torch.device('cpu')\n","    emodel.eval()\n","    pred_all = []\n","    Y_test = []\n","    for X, Y in tqdm(dataloader, desc=f\"Evaluating {model_name}...\"):\n","        # your code here\n","        y_hat = emodel(X)\n","        \n","        pred_all.append(y_hat.detach().to('cpu'))\n","        Y_test.append(Y.detach().to('cpu'))\n","        \n","    pred_all = np.concatenate(pred_all, axis=0)\n","    Y_test = np.concatenate(Y_test, axis=0)\n","\n","    return pred_all, Y_test"]},{"cell_type":"code","execution_count":177,"metadata":{},"outputs":[],"source":["def evaluate_predictions(truth, pred):\n","    \"\"\"\n","    TODO: Evaluate the performance of the predictoin via AUROC, and F1 score\n","    each prediction in pred is a vector representing [p_0, p_1].\n","    When defining the scores we are interesed in detecting class 1 only\n","    (Hint: use roc_auc_score and f1_score from sklearn.metrics, be sure to read their documentation)\n","    return: auroc, f1\n","    \"\"\"\n","    from sklearn.metrics import roc_auc_score, f1_score\n","\n","    # your code here\n","    auroc = roc_auc_score(truth, pred[:,1])\n","    f1 = f1_score(truth, np.argmax(pred,axis=1))\n","    f1_macro = f1_score(truth, np.argmax(pred,axis=1),average='macro')\n","    f1_micro = f1_score(truth, np.argmax(pred,axis=1),average='micro')\n","\n","    return auroc, f1, f1_macro, f1_micro"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Need to create a loop to train and evaluate"]},{"cell_type":"code","execution_count":178,"metadata":{},"outputs":[],"source":["#Results will be stored as rows:diseases, cols:embedding,results(4 metrics), runtime\n","result_cols = ['Batch','Disease','Embedding','AUROC','F1','F1_MACRO', 'F1_MICRO', 'Exec Time', 'Total Run (secs)','Epochs']\n","result_time = datetime.datetime.now()\n","#result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","results_file = f'{RESULTS_PATH}DL_embedding_results.csv'\n","#results_file = f'{RESULTS_PATH}DL_embedding_results_{result_name}.csv'\n","\n","def trainAndEvaluate(batch_name, disease, dataformat, embedding, device, max_tokens, n_epoch):\n","            \n","    #Create a name for the model\n","    model_name = f\"{disease}_{embedding}_{batch_name}\"\n","    model_desc = f\"{disease}_{embedding}\"\n","\n","    #Load Data \n","    if embedding == 'GloVe':\n","        train_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(train_df, disease, dataformat), batch_size = batch_size, shuffle=True, collate_fn=vectorize_batch_GloVe)\n","        val_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(test_df, disease, dataformat), batch_size = batch_size, shuffle=False, collate_fn=vectorize_batch_GloVe)\n","    if embedding == 'FastText':\n","        train_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(train_df, disease, dataformat), batch_size = batch_size, shuffle=True, collate_fn=vectorize_batch_FastText)\n","        val_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(test_df, disease, dataformat), batch_size = batch_size, shuffle=False, collate_fn=vectorize_batch_FastText)\n","\n","    #Train model\n","    model = ClincalNoteEmbeddingNet(embedding, max_tokens = max_tokens)\n","    model = model.to(device)\n","\n","    start_train = time.time()\n","    model, loss_history = train_model(model, train_loader, n_epoch=n_epoch, device=device, model_name=model_desc)\n","    end_train = time.time()\n","    start_eval = time.time()\n","    pred, truth = eval_model(model, val_loader, device=device, model_name=model_desc)\n","    end_eval = time.time()\n","\n","    #Evaluate model\n","    auroc, f1, f1_macro, f1_micro = evaluate_predictions(truth, pred)\n","    runtime = f\"Trn,Eval,Ttl={str(datetime.timedelta(seconds = (end_train-start_train)))},{str(datetime.timedelta(seconds = (end_eval-start_eval)))},{str(datetime.timedelta(seconds = (end_eval-start_train)))}\"\n","    runtime_sec = end_eval-start_train\n","\n","    #Save model\n","    torch.save(model.state_dict(), f'{MODELS_PATH}{model_name}.pkl')\n","\n","    #Delete model\n","    del model\n","\n","    #Append to results\n","    if os.path.exists(results_file):\n","        results = pd.read_csv(results_file)\n","    else:\n","        results = pd.DataFrame(columns=result_cols)\n","\n","    result = pd.DataFrame(columns=result_cols,data=[[batch_name, disease,embedding,auroc,f1,f1_macro,f1_micro,runtime,runtime_sec,n_epoch]])\n","    results = pd.concat([results,result])\n","\n","    #Save results - overwrite so we can see progress\n","    results.to_csv(results_file)\n","\n","def iterateTrainAndEvaluate(disease_list, embedding_list, batch_name, dataformat, device, max_tokens, n_epoch):\n","\n","    for _,disease in enumerate(disease_list):\n","        for _,embedding in enumerate(embedding_list):\n","            trainAndEvaluate(batch_name, disease, dataformat, embedding, device, max_tokens, n_epoch)"]},{"cell_type":"code","execution_count":179,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 1...: 100%|██████████| 11/11 [01:34<00:00,  8.63s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch1: curr_epoch_loss=58.36565017700195,execution_time=0:01:34.931840,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 2...: 100%|██████████| 11/11 [03:02<00:00, 16.63s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch2: curr_epoch_loss=11.74215030670166,execution_time=0:03:02.923044,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 3...: 100%|██████████| 11/11 [00:36<00:00,  3.35s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch3: curr_epoch_loss=3.6192166805267334,execution_time=0:00:36.884581,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 4...: 100%|██████████| 11/11 [00:36<00:00,  3.36s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch4: curr_epoch_loss=1.3278623819351196,execution_time=0:00:36.943193,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 5...: 100%|██████████| 11/11 [00:37<00:00,  3.39s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch5: curr_epoch_loss=0.46902206540107727,execution_time=0:00:37.271253,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 6...: 100%|██████████| 11/11 [00:37<00:00,  3.40s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch6: curr_epoch_loss=0.21515536308288574,execution_time=0:00:37.441962,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 7...: 100%|██████████| 11/11 [00:37<00:00,  3.38s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch7: curr_epoch_loss=0.056829001754522324,execution_time=0:00:37.165586,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 8...: 100%|██████████| 11/11 [00:37<00:00,  3.41s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch8: curr_epoch_loss=0.02380826324224472,execution_time=0:00:37.495302,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 9...: 100%|██████████| 11/11 [00:37<00:00,  3.41s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch9: curr_epoch_loss=0.016421562060713768,execution_time=0:00:37.488248,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 10...: 100%|██████████| 11/11 [00:37<00:00,  3.41s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch10: curr_epoch_loss=0.012744315899908543,execution_time=0:00:37.489407,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 11...: 100%|██████████| 11/11 [00:36<00:00,  3.36s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch11: curr_epoch_loss=0.011473371647298336,execution_time=0:00:36.983300,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 12...: 100%|██████████| 11/11 [00:37<00:00,  3.42s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch12: curr_epoch_loss=0.007736149709671736,execution_time=0:00:37.587600,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 13...: 100%|██████████| 11/11 [00:37<00:00,  3.39s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch13: curr_epoch_loss=0.005405925679951906,execution_time=0:00:37.316839,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 14...: 100%|██████████| 11/11 [00:37<00:00,  3.38s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch14: curr_epoch_loss=0.004467190708965063,execution_time=0:00:37.228370,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 15...: 100%|██████████| 11/11 [00:43<00:00,  3.96s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch15: curr_epoch_loss=0.004434292670339346,execution_time=0:00:43.610559,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 16...: 100%|██████████| 11/11 [00:38<00:00,  3.48s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch16: curr_epoch_loss=0.003169330768287182,execution_time=0:00:38.239672,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 17...: 100%|██████████| 11/11 [00:39<00:00,  3.55s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch17: curr_epoch_loss=0.005786493420600891,execution_time=0:00:39.065514,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 18...: 100%|██████████| 11/11 [00:39<00:00,  3.55s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch18: curr_epoch_loss=0.006767213810235262,execution_time=0:00:39.033421,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 19...: 100%|██████████| 11/11 [00:38<00:00,  3.51s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch19: curr_epoch_loss=0.004109058063477278,execution_time=0:00:38.609541,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 20...: 100%|██████████| 11/11 [00:37<00:00,  3.43s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch20: curr_epoch_loss=0.0021381040569394827,execution_time=0:00:37.749996,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 21...: 100%|██████████| 11/11 [00:38<00:00,  3.50s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch21: curr_epoch_loss=0.0016361925518140197,execution_time=0:00:38.469018,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 22...: 100%|██████████| 11/11 [00:38<00:00,  3.46s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch22: curr_epoch_loss=0.0014800024218857288,execution_time=0:00:38.095904,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 23...: 100%|██████████| 11/11 [00:38<00:00,  3.50s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch23: curr_epoch_loss=0.0013235789956524968,execution_time=0:00:38.505826,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 24...: 100%|██████████| 11/11 [00:37<00:00,  3.43s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch24: curr_epoch_loss=0.0012768600136041641,execution_time=0:00:37.745657,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 25...: 100%|██████████| 11/11 [00:38<00:00,  3.49s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch25: curr_epoch_loss=0.0011258804006502032,execution_time=0:00:38.339129,lr=0.01\n"]},{"name":"stderr","output_type":"stream","text":["Training Asthma_GloVe-Epoch 26...:  55%|█████▍    | 6/11 [00:22<00:18,  3.79s/it]"]}],"source":["#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device = 'cpu'\n","print(f'Using device: {device}')\n","\n","#Override these if need be\n","#disease_list = ['Asthma', 'CAD', 'CHF', 'Depression', 'Diabetes', 'Gallstones', 'GERD', 'Gout', 'Hypercholesterolemia', 'Hypertension', 'Hypertriglyceridemia', 'OA', 'OSA', 'PVD', 'Venous Insufficiency', 'Obesity']\n","disease_list = ['Asthma', 'CAD']\n","embedding_list = ['GloVe', \"FastText\"]\n","\n","#training parameters\n","n_epoch = 200\n","#lr = 0.005 #handled in the training loop\n","batch_size = 64\n","\n","#These should not change\n","dataformat = 'vector_tokenized'\n","max_tokens = 1430\n","\n","result_time = datetime.datetime.now()\n","result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","batch_name = f'DL_embedding_results_{result_name}'\n","\n","iterateTrainAndEvaluate(disease_list, embedding_list, batch_name, dataformat, device, max_tokens, n_epoch)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results = pd.read_csv(results_file).sort_values(['disease','F1'])\n","results"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Deep Learning - Word Embeddings - All Features - With Stop Words**\n","\n","![DL BagOfWords AllFeatures Averaged](images\\dl-we-swyes.gif)"]}],"metadata":{"colab":{"provenance":[{"file_id":"10pK5od01jfTHJyLN94dJxEube3sJszFm","timestamp":1678482671183}]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"}}},"nbformat":4,"nbformat_minor":0}
