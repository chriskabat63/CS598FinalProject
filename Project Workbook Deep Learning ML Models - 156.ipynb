{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"3d1ehXFWM5aE"},"source":["This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n","***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n","\n","\n","\n"," "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gv360l2IkNfO"},"source":["The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub."]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1250,"status":"ok","timestamp":1679769727418,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"zyXrAo2dsJqf","outputId":"0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# set seed\n","seed = 24\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","# define data path\n","DATA_PATH = './obesity_data/'\n","\n","test_df = pd.read_pickle(DATA_PATH + '/test.pkl') \n","train_df = pd.read_pickle(DATA_PATH + '/train.pkl') \n","corpus = pd.read_pickle(DATA_PATH + '/corpus.pkl')"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["#rebuild the vocabulary\n","import torchtext, torch, torch.nn.functional as F\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","tokenizer = get_tokenizer(\"basic_english\")\n","tokens = [tokenizer(doc) for doc in corpus]\n","\n","voc = build_vocab_from_iterator(tokens, specials = ['<pad>'])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Deep Learning - Bag of Words - All Feature Selections - Averaged**\n","\n","![DL BagOfWords AllFeatures Averaged](images\\DL-BagOfWords-ByFeatureSelection.gif)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["****DL Model using word embeddings****\n","\n","First we start by creating a dataset.  Note this will have to take the disease as part of the init and filter just for those records."]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","class ClinicalNoteDataset(Dataset):\n","\n","    def __init__(self, dataframe, disease, embedding):\n","        \"\"\"\n","        TODO: init the Dataset instance.\n","        \"\"\"\n","        # your code here\n","        self.disease = disease\n","        self.embedding = embedding\n","        self.df = dataframe[dataframe['disease'] == disease]\n","\n","    def __len__(self):\n","        \"\"\"\n","        TODO: Denotes the total number of samples\n","        \"\"\"\n","        return len(self.df)\n","\n","    def __getitem__(self, i):\n","        \"\"\"\n","        TODO: Generates one sample of data\n","            return X, y for the i-th data.\n","        \"\"\"\n","        Y = torch.tensor(self.df.iloc[i]['judgment'])\n","\n","        X = self.df.iloc[i]['one_hot']\n","\n","        if self.embedding == 'GloVe':\n","            vec = torchtext.vocab.GloVe(name='6B', dim=300)\n","            X = vec.get_vecs_by_tokens(voc.lookup_tokens(X))\n","        elif self.embedding == 'FastText':\n","            vec = torchtext.vocab.FastText()\n","            X = vec.get_vecs_by_tokens(voc.lookup_tokens(X))\n","        else:\n","            X = torch.tensor(X)\n","            \n","        return (X,Y)\n","        \n","\n"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["# of train batches: 6\n","# of val batches: 5\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\chkabat\\AppData\\Local\\Temp/ipykernel_27072/961703677.py:26: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n","  Y = torch.tensor(self.df.iloc[i]['judgment'])\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([128, 1430, 300])\n","torch.Size([128])\n"]}],"source":["##Test DataLoader\n","batch_size = 128\n","train_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(train_df, 'Asthma', 'GloVe'), batch_size = batch_size, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(test_df, 'Asthma', 'GloVe'), batch_size = batch_size, shuffle=False)\n","\n","print(\"# of train batches:\", len(train_loader))\n","print(\"# of val batches:\", len(val_loader))\n","\n","train_iter = iter(train_loader)\n","x,y = next(train_iter)\n","\n","print(x.shape)\n","print(y.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ClincalNoteNet(nn.Module):\n","    def __init__(self):\n","        super(ClincalNoteNet, self).__init__()\n","        \n","        # your code here\n","        self.fc1 = nn.Linear(1473,64)\n","        self.fc2 = nn.Linear(64,32)\n","        self.dropout = nn.Dropout(.5)\n","        self.fc3 = nn.Linear(32,1)\n","        \n","        self.net = nn.Sequential(self.fc1,\n","                                 nn.ReLU(),\n","                                 self.fc2,\n","                                 nn.ReLU(),\n","                                 self.dropout,\n","                                 self.fc3,\n","                                 nn.Sigmoid())\n","\n","    def forward(self, x):\n","        # your code here\n","        return self.net(x)\n","\n","# initialize the NN\n","model = ClincalNoteNet()\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model(model, train_dataloader, n_epoch=5, lr=0.003, device=None):\n","    import torch.optim as optim\n","    \"\"\"\n","    :param model: The instance of FreqNet that we are training\n","    :param train_dataloader: the DataLoader of the training data\n","    :param n_epoch: number of epochs to train\n","    :return:\n","        model: trained model\n","        loss_history: recorded training loss history - should be just a list of float\n","    TODO:\n","        Specify the optimizer (*optimizer*) to be optim.Adam\n","        Specify the loss function (*loss_func*) to be CrossEntropyLoss\n","        Within the loop, do the normal training procedures:\n","            pass the input through the model\n","            pass the output through loss_func to compute the loss\n","            zero out currently accumulated gradient, use loss.basckward to backprop the gradients, then call optimizer.step\n","    \"\"\"\n","    device = device or torch.device('cpu')\n","    model.train()\n","\n","    loss_history = []\n","\n","    # your code here\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    for epoch in range(n_epoch):\n","        curr_epoch_loss = []\n","        for (X, K_beat, K_rhythm, K_freq), Y in train_dataloader:\n","            # your code here\n","            optimizer.zero_grad()\n","            y_hat, _ = model(X, K_beat, K_rhythm, K_freq)\n","\n","            loss = loss_func(y_hat, Y)\n","            \n","            loss.backward()\n","            optimizer.step()\n","            \n","            curr_epoch_loss.append(loss.cpu().data.numpy())\n","        print(f\"epoch{epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n","        loss_history += curr_epoch_loss\n","    return model, loss_history\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Need to create a loop to train and evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device('cpu')\n","n_epoch = 4\n","lr = 0.003\n","n_channel = 4\n","n_dim=3000\n","T=50\n","\n","model = FreqNet(n_channel, n_dim, T)\n","model = model.to(device)\n","\n","model, loss_history = train_model(model, train_loader, n_epoch=n_epoch, lr=lr, device=device)\n","pred, truth = eval_model(model, test_loader, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def evaluate_predictions(truth, pred):\n","    \"\"\"\n","    TODO: Evaluate the performance of the predictoin via AUROC, and F1 score\n","    each prediction in pred is a vector representing [p_0, p_1].\n","    When defining the scores we are interesed in detecting class 1 only\n","    (Hint: use roc_auc_score and f1_score from sklearn.metrics, be sure to read their documentation)\n","    return: auroc, f1\n","    \"\"\"\n","    from sklearn.metrics import roc_auc_score, f1_score\n","\n","    # your code here\n","    auroc = roc_auc_score(truth, pred[:,1])\n","    f1 = f1_score(truth, np.argmax(pred,axis=1))\n","\n","    return auroc, f1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred, truth = eval_model(model, test_loader, device=device)\n","auroc, f1 = evaluate_predictions(truth, pred)\n","print(f\"AUROC={auroc} and F1={f1}\")\n","\n","assert auroc > 0.8 and f1 > 0.7, \"Performance is too low {}. Something's probably off.\".format((auroc, f1))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Deep Learning - Word Embeddings - All Features - Averaged with Stop Words**\n","\n","![DL BagOfWords AllFeatures Averaged](images\\dl-we-swyes.gif)"]}],"metadata":{"colab":{"provenance":[{"file_id":"10pK5od01jfTHJyLN94dJxEube3sJszFm","timestamp":1678482671183}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
