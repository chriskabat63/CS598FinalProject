{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"3d1ehXFWM5aE"},"source":["This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n","***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n","\n","\n","\n"," "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gv360l2IkNfO"},"source":["The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub."]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1250,"status":"ok","timestamp":1679769727418,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"zyXrAo2dsJqf","outputId":"0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","import datetime\n","from tqdm import tqdm\n","import torchtext\n","\n","# set seed\n","seed = 24\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","# define data path\n","DATA_PATH = './obesity_data/'\n","\n","test_df = pd.read_pickle(DATA_PATH + '/test.pkl') \n","train_df = pd.read_pickle(DATA_PATH + '/train.pkl') \n","#corpus = pd.read_pickle(DATA_PATH + '/corpus.pkl')\n","disease_list = test_df['disease'].unique().tolist()\n","embedding_list = ['GloVe', \"FastText\"]"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["#rebuild the vocabulary\n","#import torchtext, torch, torch.nn.functional as F\n","#from torchtext.data.utils import get_tokenizer\n","#from torchtext.vocab import build_vocab_from_iterator\n","\n","#tokenizer = get_tokenizer(\"basic_english\")\n","#tokens = [tokenizer(doc) for doc in corpus]\n","\n","#voc = build_vocab_from_iterator(tokens, specials = ['<pad>'])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Deep Learning - Bag of Words - All Feature Selections - Averaged**\n","\n","![DL BagOfWords AllFeatures Averaged](images\\DL-BagOfWords-ByFeatureSelection.gif)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["****DL Model using word embeddings****\n","\n","First we start by creating a dataset.  Note this will have to take the disease as part of the init and filter just for those records."]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","class ClinicalNoteDataset(Dataset):\n","\n","    def __init__(self, dataframe, disease, dataformat):\n","        \"\"\"\n","        TODO: init the Dataset instance.  datafomat is just the column to use from the dataframe 'vector_tokenized' , 'one_hot'\n","        \"\"\"\n","        # your code here\n","        self.disease = disease\n","        self.dataformat = dataformat\n","        self.df = dataframe[dataframe['disease'] == disease].copy()\n","        self.df = self.df.reset_index()\n","\n","    def __len__(self):\n","        \"\"\"\n","        TODO: Denotes the total number of samples\n","        \"\"\"\n","        return len(self.df)\n","\n","    def __getitem__(self, i):\n","        \"\"\"\n","        TODO: Generates one sample of data\n","            return X, y for the i-th data.\n","        \"\"\"\n","        #Cannot make tensors yet, will need to happen in collate\n","        Y = self.df.iloc[i]['judgment']\n","        X = self.df.iloc[i][self.dataformat]\n","\n","        return X,Y\n","        \n","def vectorize_batch_GloVe(batch):\n","    embedding_size_used = 300\n","    vec = torchtext.vocab.GloVe(name='6B', dim=embedding_size_used)    \n","    Xi, Yi = batch[0]\n","    batch_size = len(batch)\n","\n","    X = torch.zeros(batch_size, len(Xi), embedding_size_used, dtype=torch.float)\n","    Y = torch.zeros((batch_size), dtype=torch.long)\n","    \n","    for i in range(len(batch)):\n","        x, y = batch[i]\n","        #vectors = vec.get_vecs_by_tokens(voc.lookup_tokens(x.tolist()))\n","        vectors = vec.get_vecs_by_tokens(x)\n","\n","        X[i] = torch.tensor(vectors).float()\n","        Y[i] = torch.tensor(y)\n","\n","    return X,Y\n","\n","def vectorize_batch_FastText(batch):\n","    embedding_size_used = 300\n","    vec = torchtext.vocab.FastText()\n","    Xi, Yi = batch[0]\n","    batch_size = len(batch)\n","\n","    X = torch.zeros(batch_size, len(Xi), embedding_size_used, dtype=torch.float)\n","    Y = torch.zeros((batch_size), dtype=torch.long)\n","\n","    for i in range(len(batch)):\n","        x, y = batch[i]\n","        #vectors = vec.get_vecs_by_tokens(voc.lookup_tokens(x.tolist()))\n","        vectors = vec.get_vecs_by_tokens(x)\n","\n","        X[i] = torch.tensor(vectors).float()\n","        Y[i] = torch.tensor(y)\n","\n","    return X,Y \n"," \n","          \n"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["# of train batches: 6\n","# of val batches: 5\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_32776\\3692709997.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  X[i] = torch.tensor(vectors).float()\n","C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_32776\\3692709997.py:67: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n","  Y[i] = torch.tensor(y)\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([128, 1430, 300])\n","torch.Size([128])\n"]}],"source":["##Test DataLoader\n","batch_size = 128\n","train_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(train_df, 'Asthma', 'vector_tokenized'), batch_size = batch_size, shuffle=True, collate_fn=vectorize_batch_FastText)\n","val_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(test_df, 'Asthma', 'vector_tokenized'), batch_size = batch_size, shuffle=False, collate_fn=vectorize_batch_FastText)\n","\n","print(\"# of train batches:\", len(train_loader))\n","print(\"# of val batches:\", len(val_loader))\n","\n","train_iter = iter(train_loader)\n","x,y = next(train_iter)\n","\n","print(x.shape)\n","print(y.shape)\n"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[-0.0511, -0.2768, -0.0731,  ...,  0.5828,  0.1450, -0.1671],\n","         [ 0.1799,  0.1318,  0.1369,  ...,  0.1043,  0.1077, -0.2493],\n","         [ 0.3602, -0.2264, -0.6756,  ..., -0.0103,  0.0824,  0.3035],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.1799,  0.1318,  0.1369,  ...,  0.1043,  0.1077, -0.2493],\n","         [-0.4686, -0.0940, -0.0310,  ...,  0.1116,  0.3388,  0.0599],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.1799,  0.1318,  0.1369,  ...,  0.1043,  0.1077, -0.2493],\n","         [-0.1977,  0.1760, -0.0379,  ...,  0.3984,  0.4611,  0.3104],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        ...,\n","\n","        [[-0.1630, -0.3515, -0.0009,  ..., -0.1998,  0.4345, -0.0413],\n","         [ 0.1799,  0.1318,  0.1369,  ...,  0.1043,  0.1077, -0.2493],\n","         [ 0.3723, -0.1415, -0.0959,  ...,  0.4261,  0.3482, -0.1861],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[-0.0772, -0.0637, -0.2811,  ..., -0.1106,  0.4727,  0.4480],\n","         [ 0.1799,  0.1318,  0.1369,  ...,  0.1043,  0.1077, -0.2493],\n","         [-0.1977,  0.1760, -0.0379,  ...,  0.3984,  0.4611,  0.3104],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.1799,  0.1318,  0.1369,  ...,  0.1043,  0.1077, -0.2493],\n","         [ 0.1962,  0.0605, -0.4061,  ...,  0.2213,  0.4103,  0.2371],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n"]}],"source":["print(x)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["class ClincalNoteEmbeddingNet(nn.Module):\n","    def __init__(self, embedding_type, max_tokens):\n","        super(ClincalNoteEmbeddingNet, self).__init__()\n","        \n","        self.embedding_type = embedding_type\n","        self.max_tokens = max_tokens\n","\n","        if(embedding_type == 'USE'):\n","            self.embedding_dimension = 512\n","        else:\n","            self.embedding_dimension = 300\n","\n","        self.hidden_dim1 = 128\n","        self.hidden_dim2 = 64\n","        self.num_layers = 1\n","\n","        #Because it is bidirectional, the output from LTSM is coming in twice the size of the hidden states required.\n","        #input is (batch, #of tokens * embedding_dimension)\n","        self.bilstm1 = nn.LSTM(input_size = self.embedding_dimension, hidden_size = self.hidden_dim1, bidirectional = True, batch_first = True, num_layers = self.num_layers) \n","        self.bilstm2 = nn.LSTM(input_size = self.hidden_dim1 * 2, hidden_size = self.hidden_dim2, bidirectional = True, batch_first = True, num_layers=self.num_layers)\n","        #self.fc1 = nn.Linear(self.hidden_dim2*2,2)\n","        self.fc1 = nn.Linear(self.hidden_dim2 * self.max_tokens * 2, 2)\n","        #self.fc2 = nn.Linear(self.hidden_dim2, 2)\n","\n","        #self.relu1 = nn.ReLU()\n","        #self.relu2 = nn.ReLU()\n","        #self.do = nn.Dropout()\n","    \n","    #def _init_hidden(self, current_batch_size):\n","    #    \"\"\"Sets initial hidden and cell states (for LSTM).\"\"\"\n","    #    num_layers = self.num_layers\n","    #    h0 = torch.zeros(num_layers * 2, current_batch_size, self.hidden_dim2)\n","    #    c0 = torch.zeros(num_layers * 2, current_batch_size, self.hidden_dim2)\n","\n","    #    return h0, c0        \n","\n","    def forward(self, x):\n","        #reshape as it is a 2 dimensional embedding\n","        \n","        #h, c = self._init_hidden(current_batch_size=batch_size)\n","\n","        x, states = self.bilstm1(x)\n","        x, states = self.bilstm2(x)\n","\n","        #output = self.relu1(output)\n","        #output, states = self.bilstm2(output,states)\n","        #output = self.relu2(output)\n","        #output = self.do(output)\n","        #out[:, -1, :]\n","\n","        #print(x.shape)\n","        batch_size = x.shape[0]\n","        x = x.reshape(batch_size, -1)\n","        #print(x.shape)\n","\n","        x = self.fc1(x)\n","        #x = self.fc2(x)\n","        #output = F.softmax(output, dim=1)\n","\n","        return x\n","\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["def train_model(tmodel, train_dataloader, n_epoch=5, lr=0.003, device=None):\n","    import torch.optim as optim\n","    \n","    device = device or torch.device('cpu')\n","    tmodel.train()\n","\n","    loss_history = []\n","\n","    # your code here\n","    optimizer = optim.Adam(tmodel.parameters(), lr=lr)\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    for epoch in range(n_epoch):\n","        curr_epoch_loss = []\n","        start = time.time()\n","        for X, Y in tqdm(train_dataloader,desc=\"Training...\"):\n","            # your code here\n","            optimizer.zero_grad()\n","\n","            y_hat = tmodel(X)\n","\n","            loss = loss_func(y_hat, Y)\n","            \n","            loss.backward()\n","            optimizer.step()\n","            \n","            curr_epoch_loss.append(loss.cpu().data.numpy())\n","        end = time.time()\n","        print(f\"epoch{epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)},execution_time={str(datetime.timedelta(seconds = (end-start)))}\")\n","        loss_history += curr_epoch_loss\n","    return tmodel, loss_history\n","\n","def eval_model(emodel, dataloader, device=None):\n","    \"\"\"\n","    :return:\n","        pred_all: prediction of model on the dataloder.\n","        Y_test: truth labels. Should be an numpy array of ints\n","    TODO:\n","        evaluate the model using on the data in the dataloder.\n","        Add all the prediction and truth to the corresponding list\n","        Convert pred_all and Y_test to numpy arrays \n","    \"\"\"\n","    device = device or torch.device('cpu')\n","    emodel.eval()\n","    pred_all = []\n","    Y_test = []\n","    for X, Y in tqdm(dataloader, desc=\"Evaluating...\"):\n","        # your code here\n","        y_hat = emodel(X)\n","        \n","        pred_all.append(y_hat.detach().to('cpu'))\n","        Y_test.append(Y.detach().to('cpu'))\n","        \n","    pred_all = np.concatenate(pred_all, axis=0)\n","    Y_test = np.concatenate(Y_test, axis=0)\n","\n","    return pred_all, Y_test"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["def evaluate_predictions(truth, pred):\n","    \"\"\"\n","    TODO: Evaluate the performance of the predictoin via AUROC, and F1 score\n","    each prediction in pred is a vector representing [p_0, p_1].\n","    When defining the scores we are interesed in detecting class 1 only\n","    (Hint: use roc_auc_score and f1_score from sklearn.metrics, be sure to read their documentation)\n","    return: auroc, f1\n","    \"\"\"\n","    from sklearn.metrics import roc_auc_score, f1_score\n","\n","    # your code here\n","    auroc = roc_auc_score(truth, pred[:,1])\n","    f1 = f1_score(truth, np.argmax(pred,axis=1))\n","    f1_macro = f1_score(truth, np.argmax(pred,axis=1),average='macro')\n","    f1_micro = f1_score(truth, np.argmax(pred,axis=1),average='micro')\n","\n","    return auroc, f1, f1_macro, f1_micro"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Need to create a loop to train and evaluate"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Training...:   0%|          | 0/21 [00:00<?, ?it/s]C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_32776\\3692709997.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  X[i] = torch.tensor(vectors).float()\n","C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_32776\\3692709997.py:67: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n","  Y[i] = torch.tensor(y)\n","Training...: 100%|██████████| 21/21 [01:34<00:00,  4.51s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch0: curr_epoch_loss=3.7445526123046875,execution_time=0:01:34.806865\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating...:  44%|████▍     | 8/18 [00:27<00:34,  3.47s/it]"]}],"source":["device = torch.device('cpu')\n","#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","#print(f'Using device: {device}')\n","\n","n_epoch = 1\n","lr = 0.003\n","batch_size = 32\n","disease_input = 'Asthma'\n","dataformat = 'vector_tokenized'\n","#embedding_type = 'GloVe'\n","embedding_type = 'FastText'\n","max_tokens = 1430\n","\n","if embedding_type == 'GloVe':\n","    train_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(train_df, disease_input, dataformat), batch_size = batch_size, shuffle=True, collate_fn=vectorize_batch_GloVe)\n","    val_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(test_df, disease_input, dataformat), batch_size = batch_size, shuffle=False, collate_fn=vectorize_batch_GloVe)\n","if embedding_type == 'FastText':\n","    train_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(train_df, disease_input, dataformat), batch_size = batch_size, shuffle=True, collate_fn=vectorize_batch_FastText)\n","    val_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(test_df, disease_input, dataformat), batch_size = batch_size, shuffle=False, collate_fn=vectorize_batch_FastText)\n","\n","\n","model = ClincalNoteEmbeddingNet(embedding_type, max_tokens = max_tokens)\n","model = model.to(device)\n","\n","start_train = time.time()\n","model, loss_history = train_model(model, train_loader, n_epoch=n_epoch, lr=lr, device=device)\n","end_train = time.time()\n","start_eval = time.time()\n","pred, truth = eval_model(model, val_loader, device=device)\n","end_eval = time.time()\n","\n","print(f\"Train,Eval,Total execution_time={str(datetime.timedelta(seconds = (end_train-start_train)))},{str(datetime.timedelta(seconds = (end_eval-start_eval)))},{str(datetime.timedelta(seconds = (end_eval-start_train)))}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["AUROC=0.5143474236346807 and F1=0.0 and F1_macro=0.45841584158415843 and F1_micro=0.8464351005484461\n"]},{"data":{"text/plain":["array([[ 2.5655508, -2.5954227],\n","       [ 1.0780275, -1.10735  ],\n","       [ 1.9738563, -1.9915907],\n","       ...,\n","       [ 2.1448095, -2.1703727],\n","       [ 0.5936565, -0.5840215],\n","       [ 1.7524635, -1.7765573]], dtype=float32)"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["auroc, f1, f1_macro, f1_micro = evaluate_predictions(truth, pred)\n","\n","print(f\"AUROC={auroc} and F1={f1} and F1_macro={f1_macro} and F1_micro={f1_micro}\")\n","\n","pred"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Deep Learning - Word Embeddings - All Features - With Stop Words**\n","\n","![DL BagOfWords AllFeatures Averaged](images\\dl-we-swyes.gif)"]}],"metadata":{"colab":{"provenance":[{"file_id":"10pK5od01jfTHJyLN94dJxEube3sJszFm","timestamp":1678482671183}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
