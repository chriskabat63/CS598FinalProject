{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d1ehXFWM5aE"
   },
   "source": [
    "This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n",
    "***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv360l2IkNfO"
   },
   "source": [
    "The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1250,
     "status": "ok",
     "timestamp": 1679769727418,
     "user": {
      "displayName": "Matthew Lopes",
      "userId": "01980291092524472313"
     },
     "user_tz": 240
    },
    "id": "zyXrAo2dsJqf",
    "outputId": "0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import torchtext\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold,train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import tensorflow_hub as hub\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "# define data path\n",
    "DATA_PATH = './obesity_data/'\n",
    "RESULTS_PATH = './results/'\n",
    "MODELS_PATH = './models/'\n",
    "\n",
    "if os.path.exists(RESULTS_PATH) == False:\n",
    "    os.mkdir(RESULTS_PATH)\n",
    "if os.path.exists(MODELS_PATH) == False:\n",
    "    os.mkdir(MODELS_PATH)\n",
    "\n",
    "\n",
    "#test_df = pd.read_pickle(DATA_PATH + '/test.pkl') \n",
    "#train_df = pd.read_pickle(DATA_PATH + '/train.pkl') \n",
    "#corpus = pd.read_pickle(DATA_PATH + '/corpus.pkl')\n",
    "all_df = pd.read_pickle(DATA_PATH + '/all_df.pkl') \n",
    "all_df_expanded = pd.read_pickle(DATA_PATH + '/all_df_expanded.pkl')\n",
    "\n",
    "#Get this from the create embeddings file\n",
    "max_tokens = 1416\n",
    "max_sentences = 380\n",
    "\n",
    "#Download info for USE\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "\n",
    "disease_list = all_df['disease'].unique().tolist()\n",
    "embedding_list = ['GloVe', \"FastText\"]\n",
    "result_cols = ['Batch','Disease','Embedding','AUROC','F1','F1_MACRO', 'F1_MICRO', 'Exec Time', 'Total Run (secs)','Epochs', 'LR', 'CV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Common training and evaluation code***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=1e-10\n",
    "\n",
    "def train_model(tmodel, train_dataloader, n_epoch=5, lr=0.003, device=None, model_name='unk', use_decay=False):\n",
    "    import torch.optim as optim\n",
    "    \n",
    "    device = device or torch.device('cpu')\n",
    "    tmodel.train()\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    # your code here\n",
    "    optimizer = optim.Adam(tmodel.parameters(), lr=lr)\n",
    "    # want to decay the learning rate as teh number of epochs get larger\n",
    "    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma = 0.1)\n",
    "    if use_decay:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "            factor=0.1, patience=10, threshold=0.0001, threshold_mode='abs')\n",
    "\n",
    "    #loss_func = nn.BCELoss()\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    #loss_func = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        epoch = epoch+1\n",
    "        curr_epoch_loss = []\n",
    "        start = time.time()\n",
    "        for X, Y in tqdm(train_dataloader,desc=f\"Training {model_name}-Lr{str(lr)}-Epoch{epoch}...\"):\n",
    "            # your code here\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_hat = tmodel(X)\n",
    "\n",
    "            loss = loss_func(y_hat, Y)\n",
    "            #loss = loss_func(torch.log(y_hat+ eps), Y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if use_decay:\n",
    "                scheduler.step(loss)\n",
    "            \n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "\n",
    "\n",
    "        end = time.time()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"epoch{epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)},execution_time={str(datetime.timedelta(seconds = (end-start)))},lr={optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        #scheduler.step()\n",
    "        loss_history += curr_epoch_loss\n",
    "    return tmodel, loss_history\n",
    "\n",
    "def eval_model(emodel, dataloader, device=None, model_name='unk'):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "        pred_all: prediction of model on the dataloder.\n",
    "        Y_test: truth labels. Should be an numpy array of ints\n",
    "    TODO:\n",
    "        evaluate the model using on the data in the dataloder.\n",
    "        Add all the prediction and truth to the corresponding list\n",
    "        Convert pred_all and Y_test to numpy arrays \n",
    "    \"\"\"\n",
    "    device = device or torch.device('cpu')\n",
    "    emodel.eval()\n",
    "    pred_all = []\n",
    "    Y_test = []\n",
    "    for X, Y in tqdm(dataloader, desc=f\"Evaluating {model_name}...\"):\n",
    "        # your code here\n",
    "        y_hat = emodel(X)\n",
    "        \n",
    "        pred_all.append(y_hat.detach().to('cpu'))\n",
    "        Y_test.append(Y.detach().to('cpu'))\n",
    "        \n",
    "    pred_all = np.concatenate(pred_all, axis=0)\n",
    "    Y_test = np.concatenate(Y_test, axis=0)\n",
    "\n",
    "    return pred_all, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(truth, pred):\n",
    "    \"\"\"\n",
    "    TODO: Evaluate the performance of the predictoin via AUROC, and F1 score\n",
    "    each prediction in pred is a vector representing [p_0, p_1].\n",
    "    When defining the scores we are interesed in detecting class 1 only\n",
    "    (Hint: use roc_auc_score and f1_score from sklearn.metrics, be sure to read their documentation)\n",
    "    return: auroc, f1\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "    # your code here\n",
    "    auroc = roc_auc_score(truth, pred[:,1])\n",
    "    f1 = f1_score(truth, np.argmax(pred,axis=1))\n",
    "    f1_macro = f1_score(truth, np.argmax(pred,axis=1),average='macro')\n",
    "    f1_micro = f1_score(truth, np.argmax(pred,axis=1),average='micro')\n",
    "\n",
    "    return auroc, f1, f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndEvaluate(train_loader, val_loader, model, model_desc, batch_name, results_file, disease, lr,  dataformat, embedding, device, n_epoch, cv, use_decay):\n",
    "            \n",
    "    return_val = False\n",
    "\n",
    "    start_train = time.time()\n",
    "    model, loss_history = train_model(model, train_loader, n_epoch=n_epoch, lr = lr, device=device, model_name=model_desc, use_decay=use_decay)\n",
    "    end_train = time.time()\n",
    "\n",
    "    try:\n",
    "        #Evaluate model\n",
    "        start_eval = time.time()\n",
    "        pred, truth = eval_model(model, val_loader, device=device, model_name=model_desc)\n",
    "        end_eval = time.time()\n",
    "\n",
    "        auroc, f1, f1_macro, f1_micro = evaluate_predictions(truth, pred)\n",
    "        runtime = f\"Trn,Eval,Ttl={str(datetime.timedelta(seconds = (end_train-start_train)))},{str(datetime.timedelta(seconds = (end_eval-start_eval)))},{str(datetime.timedelta(seconds = (end_eval-start_train)))}\"\n",
    "        runtime_sec = end_eval-start_train\n",
    "\n",
    "        return_val = True\n",
    "\n",
    "    except:\n",
    "        auroc = -1\n",
    "        f1=-1\n",
    "        f1_macro = -1\n",
    "        f1_micro = -1\n",
    "        runtime_sec = end_train-start_train\n",
    "        runtime = 'Failure'\n",
    "        print(\"Failure!\")\n",
    "\n",
    "\n",
    "    #Append to results\n",
    "    if os.path.exists(results_file):\n",
    "        results = pd.read_csv(results_file)\n",
    "    else:\n",
    "        results = pd.DataFrame(columns=result_cols)\n",
    "\n",
    "    result = pd.DataFrame(columns=result_cols,data=[[batch_name, disease,embedding,auroc,f1,f1_macro,f1_micro,runtime,runtime_sec,n_epoch,lr,str(cv)]])\n",
    "    results = pd.concat([results,result])\n",
    "\n",
    "    #Save results - overwrite so we can see progress\n",
    "    results.to_csv(results_file, index=False)\n",
    "\n",
    "    return return_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** DL TF-IDF ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(all_df['tok_lem_text']):\n",
    "    Final_words = []\n",
    "    #print(entry)\n",
    "    for word in entry:\n",
    "        #print(word)\n",
    "        Final_words.append(word)\n",
    "    all_df.loc[index, 'text_final'] = str(Final_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDFClinicalNotesDataset(Dataset):\n",
    "    def __init__(self, X_array, y):\n",
    "        df = pd.DataFrame(index=y.index)\n",
    "        \n",
    "        df['tfidf_vector'] = [vector.tolist() for vector in X_array]\n",
    "        \n",
    "        self.tfidf_vector = df.tfidf_vector.tolist()\n",
    "        self.targets = y.tolist()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.tfidf_vector[i], self.targets[i])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tfidf = torch.tensor([item[0] for item in batch]).float()\n",
    "    target = torch.tensor([int(item[1]==True) for item in batch]).long()\n",
    "\n",
    "    return tfidf, target        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClincalNoteTDFNet(nn.Module):\n",
    "    def __init__(self, tokens):\n",
    "        super(ClincalNoteTDFNet, self).__init__()\n",
    "        \n",
    "        self.tokens = tokens\n",
    "\n",
    "        self.hidden_dim1 = 128\n",
    "        self.hidden_dim2 = 64\n",
    "        self.hidden_dim3 = 32\n",
    "        self.num_layers = 1\n",
    "\n",
    "        #Because it is bidirectional, the output from LTSM is coming in twice the size of the hidden states required.\n",
    "        #input is (batch, #of tokens * embedding_dimension)\n",
    "        self.bilstm1 = nn.LSTM(input_size = self.tokens, hidden_size = self.hidden_dim1, bidirectional = True, batch_first = True, num_layers = self.num_layers, bias = False) \n",
    "        self.bilstm2 = nn.LSTM(input_size = self.hidden_dim1 * 2, hidden_size = self.hidden_dim2, bidirectional = True, batch_first = True, num_layers=self.num_layers, bias = False)\n",
    " \n",
    "        self.fc1 = nn.Linear(self.hidden_dim2 * 2, self.hidden_dim2)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim2, 2)\n",
    "\n",
    " \n",
    "    def forward(self, x):\n",
    "\n",
    "        x, states = self.bilstm1(x)\n",
    "        x, states = self.bilstm2(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "\n",
    "def getVocab(X_train, y_train, feature, max_tokens):\n",
    " \n",
    "    ## Step 1: Determine the Initial Vocabulary\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab = list(tokenizer.word_index.keys())\n",
    "\n",
    "    ## Step 2: Create term  matrix\n",
    "    vectors = tokenizer.texts_to_matrix(X_train, mode='count')\n",
    "\n",
    "    ## Do feature selection on term matrix (column headers are words)\n",
    "    X = vectors\n",
    "    y = y_train\n",
    "\n",
    "    ##Choose algorithm\n",
    "    if feature == 'SelectKBest':\n",
    "        selector = SelectKBest(score_func=f_classif, k=max_tokens).fit(X,y)\n",
    "    else: \n",
    "        if feature == 'InfoGainAttributeVal':\n",
    "            #This should be similar to the InfoGain?\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=max_tokens).fit(X,y)\n",
    "        else:\n",
    "            #default to ExtraTreeClassifier\n",
    "            estimator = ExtraTreeClassifier(random_state = seed)\n",
    "            #selector = SelectFromModel(estimator, max_features = tokens,threshold=-np.inf)\n",
    "            selector = SelectFromModel(estimator, max_features = max_tokens)\n",
    "            selector = selector.fit(X, y)\n",
    "\n",
    "    support_idx = selector.get_support(True)\n",
    "    \n",
    "    #print(\"Vocab:\", [vocab[i-1].replace(\"'\",\"\") for i in support_idx])\n",
    "    tokenizer2 = Tokenizer()\n",
    "    tokenizer2.fit_on_texts([vocab[i-1].replace(\"'\",\"\") for i in support_idx])\n",
    "    new_vocab = list(tokenizer2.word_index.keys())\n",
    "\n",
    "    return new_vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateTrainAndEvaluateTFIDF(df, k, disease_list, feature_list, lr_list, \n",
    "                            batch_name, batch_size, results_file, dataformat, device, tokens, epoch_list, cv = False, use_decay=False):\n",
    "\n",
    "    for _,disease in enumerate(disease_list):\n",
    "        for _,feature in enumerate(feature_list):\n",
    "            for _,lr in enumerate(lr_list):\n",
    "                for _, n_epoch in enumerate(epoch_list):\n",
    "                    #Create a name for the model\n",
    "                    model_name = f\"{disease}_{feature}_{batch_name}\"\n",
    "\n",
    "                    disease_df = df[df['disease'] == disease].copy()\n",
    "\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(disease_df[dataformat], disease_df['judgment'], test_size=0.2, random_state=seed)\n",
    "\n",
    "                    if feature != 'All':\n",
    "                        vocab = getVocab(X_train,y_train, feature, tokens)\n",
    "                        Tfidf_vect = TfidfVectorizer(max_features=tokens,vocabulary = vocab)\n",
    "                    else:\n",
    "                        Tfidf_vect = TfidfVectorizer(max_features=tokens)\n",
    "\n",
    "                    X_train_values_list = Tfidf_vect.fit_transform(X_train).toarray()\n",
    "                    X_training = pd.DataFrame(X_train_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "                    X_training = np.asarray(X_training, dtype=float)\n",
    "                    X_training = torch.from_numpy(X_training).to(device)\n",
    "\n",
    "                    X_test_values_list = Tfidf_vect.transform(X_test).toarray()\n",
    "                    X_testing = pd.DataFrame(X_test_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "                    X_testing = np.asarray(X_testing, dtype=float)\n",
    "                    X_testing = torch.from_numpy(X_testing).to(device)\n",
    "\n",
    "                    tokens_to_use = X_training.shape[1]\n",
    "                    print(tokens_to_use)\n",
    "\n",
    "                    #Create model\n",
    "                    model = ClincalNoteTDFNet(tokens = tokens_to_use)\n",
    "                    model = model.to(device)\n",
    "\n",
    "                    ds_train = TDFClinicalNotesDataset(X_training, y_train)\n",
    "                    ds_test = TDFClinicalNotesDataset(X_testing, y_test)\n",
    "\n",
    "                    #Load Data \n",
    "                    train_loader = torch.utils.data.DataLoader(ds_train, batch_size = batch_size, collate_fn=collate_fn)\n",
    "                    val_loader = torch.utils.data.DataLoader(ds_test, batch_size = batch_size,collate_fn=collate_fn)\n",
    "\n",
    "                    model_desc = f\"{disease}_{feature}\"\n",
    "\n",
    "                    trainAndEvaluate(train_loader, val_loader, model, model_desc, batch_name, results_file, disease, lr, dataformat, feature, device, n_epoch, False, use_decay)\n",
    "\n",
    "                    #Save model\n",
    "                    torch.save(model.state_dict(), f'{MODELS_PATH}{model_name}.pkl')\n",
    "\n",
    "                    #Delete model\n",
    "                    del model\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "#Override these if need be\n",
    "disease_list = ['Asthma', 'CAD', 'CHF', 'Depression', 'Diabetes', 'Gallstones', 'GERD', 'Gout', 'Hypercholesterolemia', 'Hypertension', 'Hypertriglyceridemia', 'OA', 'OSA', 'PVD', 'Venous Insufficiency', 'Obesity']\n",
    "#disease_list = ['Asthma']\n",
    "feature_list = ['All','ExtraTreeClassifier','SelectKBest','InfoGainAttributeVal']\n",
    "results_file = f'{RESULTS_PATH}DL_tfidf_results.csv'\n",
    "\n",
    "\n",
    "#0.01 seems to be the most effective with no decay\n",
    "lr_list = [0.1,0.01,0.001]\n",
    "#lr_list = [0.01]\n",
    "epoch_list = [10,25,50,100]\n",
    "\n",
    "#training parameters\n",
    "n_epoch = 50\n",
    "batch_size = 128\n",
    "k = 2\n",
    "\n",
    "#These should not change\n",
    "dataformat = 'text_final'\n",
    "tokens = 600\n",
    "\n",
    "result_time = datetime.datetime.now()\n",
    "result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "batch_name = f'DL_tfidf_results_{result_name}'\n",
    "\n",
    "#commented out because working on Embeddings\n",
    "#iterateTrainAndEvaluateTFIDF(all_df, k, disease_list, feature_list, lr_list, batch_name, batch_size, results_file, dataformat, device, tokens, epoch_list, False, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning - Bag of Words - All Feature Selections - Averaged**\n",
    "\n",
    "![DL BagOfWords AllFeatures Averaged](images\\DL-BagOfWords-ByFeatureSelection.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****DL Model using word embeddings****\n",
    "\n",
    "First we start by creating a dataset.  Note this will have to take the disease as part of the init and filter just for those records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClinicalNoteDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, disease, dataformat):\n",
    "        \"\"\"\n",
    "        TODO: init the Dataset instance.  datafomat is just the column to use from the dataframe 'vector_tokenized' , 'one_hot'\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        self.disease = disease\n",
    "        self.dataformat = dataformat\n",
    "        self.df = dataframe[dataframe['disease'] == disease].copy()\n",
    "        self.df = self.df.reset_index()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        TODO: Denotes the total number of samples\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data\n",
    "            return X, y for the i-th data.\n",
    "        \"\"\"\n",
    "        #Cannot make tensors yet, will need to happen in collate\n",
    "        Y = self.df.iloc[i]['judgment']\n",
    "        X = self.df.iloc[i][self.dataformat]\n",
    "\n",
    "        return X,Y\n",
    "        \n",
    "def vectorize_batch_GloVe(batch):\n",
    "    embedding_size_used = 300\n",
    "    vec = torchtext.vocab.GloVe(name='6B', dim=embedding_size_used)    \n",
    "    Xi, Yi = batch[0]\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    X = torch.zeros(batch_size, len(Xi), embedding_size_used, dtype=torch.float)\n",
    "    Y = torch.zeros((batch_size), dtype=torch.long)\n",
    "    \n",
    "    for i in range(len(batch)):\n",
    "        x, y = batch[i]\n",
    "        #vectors = vec.get_vecs_by_tokens(voc.lookup_tokens(x.tolist()))\n",
    "        vectors = vec.get_vecs_by_tokens(x)\n",
    "\n",
    "        X[i] = vectors.float()\n",
    "        Y[i] = torch.tensor(float(y == True))\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "def vectorize_batch_FastText(batch):\n",
    "    embedding_size_used = 300\n",
    "    vec = torchtext.vocab.FastText()\n",
    "    Xi, Yi = batch[0]\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    X = torch.zeros(batch_size, len(Xi), embedding_size_used, dtype=torch.float)\n",
    "    Y = torch.zeros((batch_size), dtype=torch.long)\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        x, y = batch[i]\n",
    "        #vectors = vec.get_vecs_by_tokens(voc.lookup_tokens(x.tolist()))\n",
    "        vectors = vec.get_vecs_by_tokens(x)\n",
    "\n",
    "        X[i] = vectors.float()\n",
    "        Y[i] = torch.tensor(float(y == True))\n",
    "\n",
    "    return X,Y \n",
    " \n",
    "def vectorize_batch_USE(batch):\n",
    "    embedding_size_used = 512\n",
    "\n",
    "    Xi, Yi = batch[0]\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    X = torch.zeros(batch_size, len(Xi), embedding_size_used, dtype=torch.float)\n",
    "    Y = torch.zeros((batch_size), dtype=torch.long)\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        x, y = batch[i]\n",
    "        \n",
    "        tensor_flow_vectors = embed(x)\n",
    "        array_vectors = tensor_flow_vectors.numpy()\n",
    "\n",
    "        X[i] = torch.tensor(array_vectors).float()\n",
    "        Y[i] = torch.tensor(float(y == True))\n",
    "\n",
    "    return X,Y \n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test DataLoader\n",
    "#batch_size = 128\n",
    "#train_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(train_df, 'Asthma', 'vector_tokenized'), batch_size = batch_size, shuffle=True, collate_fn=vectorize_batch_FastText)\n",
    "#val_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(test_df, 'Asthma', 'vector_tokenized'), batch_size = batch_size, shuffle=False, collate_fn=vectorize_batch_FastText)\n",
    "\n",
    "#print(\"# of train batches:\", len(train_loader))\n",
    "#print(\"# of val batches:\", len(val_loader))\n",
    "\n",
    "#train_iter = iter(train_loader)\n",
    "#x,y = next(train_iter)\n",
    "\n",
    "#print(x.shape)\n",
    "#print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClincalNoteEmbeddingNet(nn.Module):\n",
    "    def __init__(self, embedding_type, max_tokens):\n",
    "        super(ClincalNoteEmbeddingNet, self).__init__()\n",
    "        \n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "        if(embedding_type == 'USE'):\n",
    "            self.embedding_dimension = 512\n",
    "        else:\n",
    "            self.embedding_dimension = 300\n",
    "\n",
    "        self.hidden_dim1 = 128\n",
    "        self.hidden_dim2 = 64\n",
    "        self.num_layers = 1\n",
    "\n",
    "        #Because it is bidirectional, the output from LTSM is coming in twice the size of the hidden states required.\n",
    "        #input is (batch, #of tokens * embedding_dimension)\n",
    "        self.bilstm1 = nn.LSTM(input_size = self.embedding_dimension, hidden_size = int(self.hidden_dim1/2), bidirectional = True, \n",
    "                               batch_first = True, num_layers = self.num_layers) \n",
    "        #self.fcb1 = nn.Linear(self.hidden_dim1 * 2, self.hidden_dim1)\n",
    "        \n",
    "        self.bilstm2 = nn.LSTM(input_size = self.hidden_dim1, hidden_size = int(self.hidden_dim2/2), bidirectional = True, #was dim2 for hidden\n",
    "                               batch_first = True, num_layers=self.num_layers)\n",
    "        #self.fcb2 = nn.Linear(self.hidden_dim2 * 2, self.hidden_dim2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(self.hidden_dim2 * self.max_tokens, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        #2 is for number of directions\n",
    "        #h_0 = torch.rand(2 * self.num_layers, batch_size, self.hidden_dim2)\n",
    "        #c_0 = torch.rand(2 * self.num_layers, batch_size, self.hidden_dim2)\n",
    "\n",
    "        x, states = self.bilstm1(x)\n",
    "        #x = self.fcb1(x)\n",
    "\n",
    "        #want to pass in final states, but pytorch concats the two (note first two parameters of view are layers and directions)\n",
    "\n",
    "        #states/context[0] and [1] are 2, 128, 128 (directions*layers, batch, hidden)\n",
    "        #for next layer, will need to be (directions*layers,batch, hidden) 2, 128, 64\n",
    "\n",
    "        #final_hidden = states[0].view(1, 2, batch_size, self.hidden_dim1)[-1]  #This [-1] just getes rid of the first dimension\n",
    "        #final_context = states[1].view(1, 2, batch_size, self.hidden_dim1)[-1]\n",
    "\n",
    "        #h_1, h_2 = final_hidden[0], final_hidden[1]  #one is forward, one is backward\n",
    "        #h_1 = F.linear(h_1,self.hidden_dim2)\n",
    "        #h_2 = F.linear(h_2,self.hidden_dim2)\n",
    "        #final_hidden = torch.cat((h_1,h_2), 1)\n",
    "\n",
    "        #c_1, c_2 = final_context[0], final_context[1]\n",
    "        #final_context = torch.cat((c_1, c_2), 1)\n",
    "        #print(final_hidden.shape, final_context.shape)\n",
    "\n",
    "\n",
    "        x, states = self.bilstm2(x)\n",
    "        #x = self.fcb2(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x #F.sigmoid(x).squeeze(dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to create a loop to train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateTrainAndEvaluate(df, k, disease_list, embedding_list, lr_list, \n",
    "                            batch_name, results_file, device, epoch_list, cv = False, use_decay = False):\n",
    "\n",
    "    for _,disease in enumerate(disease_list):\n",
    "        for _,embedding in enumerate(embedding_list):\n",
    "            for _,lr in enumerate(lr_list):\n",
    "                for _, n_epoch in enumerate(epoch_list):\n",
    "                    #Create a name for the model\n",
    "                    model_name = f\"{disease}_{embedding}_{batch_name}\"\n",
    "\n",
    "                    #Create model\n",
    "                    if embedding == 'USE':\n",
    "                        model_tokens = max_sentences\n",
    "                    else:\n",
    "                        model_tokens = max_tokens\n",
    "                        \n",
    "                    model = ClincalNoteEmbeddingNet(embedding, max_tokens = model_tokens)\n",
    "                    model = model.to(device)\n",
    "\n",
    "                    if embedding == 'GloVe':\n",
    "                        custom_collate=vectorize_batch_GloVe\n",
    "                        dataformat = 'vector_tokenized'\n",
    "                    if embedding == 'FastText':\n",
    "                        custom_collate=vectorize_batch_FastText\n",
    "                        dataformat = 'vector_tokenized'\n",
    "                    if embedding == 'USE':\n",
    "                        custom_collate=vectorize_batch_USE\n",
    "                        dataformat = 'sentence_tokenized'\n",
    "\n",
    "                    ds = ClinicalNoteDataset(df, disease, dataformat)\n",
    "                    ds_train, ds_test = train_test_split(ds, test_size=0.20, shuffle=True, random_state = seed)\n",
    "\n",
    "                    #Load Data \n",
    "                    train_loader = torch.utils.data.DataLoader(ds_train, batch_size = batch_size, collate_fn=custom_collate)\n",
    "                    val_loader = torch.utils.data.DataLoader(ds_test, batch_size = batch_size, collate_fn=custom_collate)\n",
    "                    \n",
    "                    model_desc = f\"{disease}_{embedding}\"\n",
    "\n",
    "                    trainAndEvaluate(train_loader, val_loader, model, model_desc, batch_name, results_file, disease, lr, dataformat, embedding, device, n_epoch, False, use_decay)\n",
    "\n",
    "                    #Save model\n",
    "                    torch.save(model.state_dict(), f'{MODELS_PATH}{model_name}.pkl')\n",
    "\n",
    "                    #Delete model\n",
    "                    del model\n",
    "\n",
    "                    if cv:\n",
    "                        #note, cross validation is only used to validate the model works consistently\n",
    "                        splits=KFold(n_splits=k,shuffle=True,random_state=seed)\n",
    "\n",
    "                        for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(ds)))):\n",
    "                            #for now, let's keep the results at the fold level\n",
    "                            model = ClincalNoteEmbeddingNet(embedding, max_tokens = max_tokens)\n",
    "                            model = model.to(device)\n",
    "                            \n",
    "                            train_sampler = SubsetRandomSampler(train_idx)\n",
    "                            val_sampler = SubsetRandomSampler(val_idx)\n",
    "                            #Load Data \n",
    "                            train_loader = torch.utils.data.DataLoader(ds, batch_size = batch_size, sampler=train_sampler, collate_fn=custom_collate)\n",
    "                            val_loader = torch.utils.data.DataLoader(ds, batch_size = batch_size, sampler=val_sampler, collate_fn=custom_collate)\n",
    "                            \n",
    "                            model_desc = f\"{disease}_{embedding}_Fold{fold+1}\"\n",
    "\n",
    "                            trainAndEvaluate(train_loader, val_loader, model, model_desc, batch_name, results_file, disease, lr, dataformat, embedding, device, n_epoch, cv, use_decay)\n",
    "\n",
    "                            del model\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Asthma_USE-Lr0.01-Epoch32...: 100%|██████████| 8/8 [00:39<00:00,  4.89s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch33...: 100%|██████████| 8/8 [00:38<00:00,  4.82s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch34...: 100%|██████████| 8/8 [00:39<00:00,  4.95s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch35...: 100%|██████████| 8/8 [00:40<00:00,  5.05s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch36...: 100%|██████████| 8/8 [00:39<00:00,  4.99s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch37...: 100%|██████████| 8/8 [00:44<00:00,  5.55s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch38...: 100%|██████████| 8/8 [00:38<00:00,  4.80s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch39...: 100%|██████████| 8/8 [00:38<00:00,  4.82s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch40...: 100%|██████████| 8/8 [00:38<00:00,  4.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch40: curr_epoch_loss=0.0021118908189237118,execution_time=0:00:38.564297,lr=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Asthma_USE-Lr0.01-Epoch41...: 100%|██████████| 8/8 [00:38<00:00,  4.79s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch42...: 100%|██████████| 8/8 [00:38<00:00,  4.84s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch43...: 100%|██████████| 8/8 [00:38<00:00,  4.79s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch44...: 100%|██████████| 8/8 [00:38<00:00,  4.86s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch45...: 100%|██████████| 8/8 [00:38<00:00,  4.81s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch46...: 100%|██████████| 8/8 [00:38<00:00,  4.81s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch47...: 100%|██████████| 8/8 [00:38<00:00,  4.84s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch48...: 100%|██████████| 8/8 [00:38<00:00,  4.83s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch49...: 100%|██████████| 8/8 [00:38<00:00,  4.81s/it]\n",
      "Training Asthma_USE-Lr0.01-Epoch50...: 100%|██████████| 8/8 [00:38<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch50: curr_epoch_loss=0.00029335595900192857,execution_time=0:00:38.475133,lr=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Asthma_USE...: 100%|██████████| 2/2 [00:08<00:00,  4.21s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch1...: 100%|██████████| 8/8 [00:38<00:00,  4.78s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch2...: 100%|██████████| 8/8 [00:38<00:00,  4.80s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch3...: 100%|██████████| 8/8 [00:38<00:00,  4.83s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch4...: 100%|██████████| 8/8 [00:38<00:00,  4.85s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch5...: 100%|██████████| 8/8 [00:38<00:00,  4.83s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch6...: 100%|██████████| 8/8 [00:38<00:00,  4.83s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch7...: 100%|██████████| 8/8 [00:38<00:00,  4.82s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch8...: 100%|██████████| 8/8 [00:38<00:00,  4.83s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch9...: 100%|██████████| 8/8 [00:38<00:00,  4.80s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch10...: 100%|██████████| 8/8 [00:44<00:00,  5.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch10: curr_epoch_loss=0.32697412371635437,execution_time=0:00:44.134008,lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Asthma_USE...: 100%|██████████| 2/2 [00:08<00:00,  4.27s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch1...: 100%|██████████| 8/8 [00:38<00:00,  4.83s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch2...: 100%|██████████| 8/8 [00:38<00:00,  4.82s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch3...: 100%|██████████| 8/8 [00:38<00:00,  4.83s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch4...: 100%|██████████| 8/8 [00:39<00:00,  4.98s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch5...: 100%|██████████| 8/8 [00:38<00:00,  4.81s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch6...: 100%|██████████| 8/8 [00:38<00:00,  4.78s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch7...: 100%|██████████| 8/8 [00:38<00:00,  4.80s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch8...: 100%|██████████| 8/8 [00:39<00:00,  4.98s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch9...: 100%|██████████| 8/8 [00:40<00:00,  5.09s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch10...: 100%|██████████| 8/8 [00:38<00:00,  4.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch10: curr_epoch_loss=0.3603130578994751,execution_time=0:00:38.529746,lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Asthma_USE-Lr0.001-Epoch11...: 100%|██████████| 8/8 [00:40<00:00,  5.03s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch12...: 100%|██████████| 8/8 [00:38<00:00,  4.87s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch13...: 100%|██████████| 8/8 [00:37<00:00,  4.73s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch14...: 100%|██████████| 8/8 [00:38<00:00,  4.80s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch15...: 100%|██████████| 8/8 [00:39<00:00,  4.88s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch16...: 100%|██████████| 8/8 [00:38<00:00,  4.83s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch17...: 100%|██████████| 8/8 [00:38<00:00,  4.80s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch18...: 100%|██████████| 8/8 [00:39<00:00,  4.88s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch19...: 100%|██████████| 8/8 [00:39<00:00,  4.92s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch20...: 100%|██████████| 8/8 [00:38<00:00,  4.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch20: curr_epoch_loss=0.1376766860485077,execution_time=0:00:38.174751,lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Asthma_USE-Lr0.001-Epoch21...: 100%|██████████| 8/8 [00:38<00:00,  4.75s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch22...: 100%|██████████| 8/8 [00:39<00:00,  4.89s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch23...: 100%|██████████| 8/8 [00:44<00:00,  5.58s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch24...: 100%|██████████| 8/8 [00:38<00:00,  4.83s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch25...: 100%|██████████| 8/8 [00:40<00:00,  5.04s/it]\n",
      "Evaluating Asthma_USE...: 100%|██████████| 2/2 [00:09<00:00,  4.51s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch1...: 100%|██████████| 8/8 [00:41<00:00,  5.14s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch2...: 100%|██████████| 8/8 [00:39<00:00,  4.93s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch3...: 100%|██████████| 8/8 [00:40<00:00,  5.10s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch4...: 100%|██████████| 8/8 [00:38<00:00,  4.82s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch5...: 100%|██████████| 8/8 [00:39<00:00,  4.96s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch6...: 100%|██████████| 8/8 [00:38<00:00,  4.78s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch7...: 100%|██████████| 8/8 [00:38<00:00,  4.82s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch8...: 100%|██████████| 8/8 [00:38<00:00,  4.75s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch9...: 100%|██████████| 8/8 [00:38<00:00,  4.81s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch10...: 100%|██████████| 8/8 [00:38<00:00,  4.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch10: curr_epoch_loss=0.35831835865974426,execution_time=0:00:38.287494,lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Asthma_USE-Lr0.001-Epoch11...: 100%|██████████| 8/8 [00:37<00:00,  4.74s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch12...: 100%|██████████| 8/8 [00:38<00:00,  4.86s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch13...: 100%|██████████| 8/8 [00:40<00:00,  5.09s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch14...: 100%|██████████| 8/8 [00:40<00:00,  5.01s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch15...: 100%|██████████| 8/8 [00:40<00:00,  5.01s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch16...: 100%|██████████| 8/8 [00:40<00:00,  5.01s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch17...: 100%|██████████| 8/8 [00:39<00:00,  4.96s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch18...: 100%|██████████| 8/8 [00:40<00:00,  5.05s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch19...: 100%|██████████| 8/8 [00:40<00:00,  5.02s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch20...: 100%|██████████| 8/8 [00:46<00:00,  5.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch20: curr_epoch_loss=0.15219908952713013,execution_time=0:00:46.062555,lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Asthma_USE-Lr0.001-Epoch21...: 100%|██████████| 8/8 [00:40<00:00,  5.01s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch22...: 100%|██████████| 8/8 [00:40<00:00,  5.05s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch23...: 100%|██████████| 8/8 [00:39<00:00,  4.99s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch24...: 100%|██████████| 8/8 [00:39<00:00,  4.98s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch25...: 100%|██████████| 8/8 [00:40<00:00,  5.03s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch26...: 100%|██████████| 8/8 [00:40<00:00,  5.10s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch27...: 100%|██████████| 8/8 [00:40<00:00,  5.01s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch28...: 100%|██████████| 8/8 [00:40<00:00,  5.11s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch29...: 100%|██████████| 8/8 [00:39<00:00,  5.00s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch30...: 100%|██████████| 8/8 [00:40<00:00,  5.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch30: curr_epoch_loss=0.1205504983663559,execution_time=0:00:40.186593,lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Asthma_USE-Lr0.001-Epoch31...: 100%|██████████| 8/8 [00:40<00:00,  5.00s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch32...: 100%|██████████| 8/8 [00:39<00:00,  4.97s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch33...: 100%|██████████| 8/8 [00:39<00:00,  5.00s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch34...: 100%|██████████| 8/8 [00:40<00:00,  5.02s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch35...: 100%|██████████| 8/8 [00:40<00:00,  5.02s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch36...: 100%|██████████| 8/8 [00:40<00:00,  5.05s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch37...: 100%|██████████| 8/8 [00:39<00:00,  5.00s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch38...: 100%|██████████| 8/8 [00:40<00:00,  5.04s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch39...: 100%|██████████| 8/8 [00:40<00:00,  5.01s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch40...: 100%|██████████| 8/8 [00:40<00:00,  5.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch40: curr_epoch_loss=0.0026083742268383503,execution_time=0:00:40.230381,lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Asthma_USE-Lr0.001-Epoch41...: 100%|██████████| 8/8 [00:39<00:00,  4.99s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch42...: 100%|██████████| 8/8 [00:45<00:00,  5.74s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch43...: 100%|██████████| 8/8 [00:40<00:00,  5.02s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch44...: 100%|██████████| 8/8 [00:40<00:00,  5.01s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch45...: 100%|██████████| 8/8 [00:40<00:00,  5.01s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch46...: 100%|██████████| 8/8 [00:40<00:00,  5.02s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch47...: 100%|██████████| 8/8 [00:40<00:00,  5.01s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch48...: 100%|██████████| 8/8 [00:40<00:00,  5.02s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch49...: 100%|██████████| 8/8 [00:40<00:00,  5.04s/it]\n",
      "Training Asthma_USE-Lr0.001-Epoch50...: 100%|██████████| 8/8 [00:40<00:00,  5.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch50: curr_epoch_loss=0.000439024850493297,execution_time=0:00:40.475061,lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Asthma_USE...: 100%|██████████| 2/2 [00:08<00:00,  4.47s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch1...: 100%|██████████| 8/8 [00:40<00:00,  5.07s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch2...: 100%|██████████| 8/8 [00:40<00:00,  5.04s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch3...: 100%|██████████| 8/8 [00:40<00:00,  5.01s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch4...: 100%|██████████| 8/8 [00:39<00:00,  4.98s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch5...: 100%|██████████| 8/8 [00:39<00:00,  4.98s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch6...: 100%|██████████| 8/8 [00:39<00:00,  4.99s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch7...: 100%|██████████| 8/8 [00:40<00:00,  5.04s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch8...: 100%|██████████| 8/8 [00:39<00:00,  5.00s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch9...: 100%|██████████| 8/8 [00:40<00:00,  5.01s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch10...: 100%|██████████| 8/8 [00:39<00:00,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch10: curr_epoch_loss=0.39848172664642334,execution_time=0:00:39.944971,lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Asthma_USE...: 100%|██████████| 2/2 [00:09<00:00,  4.60s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch1...: 100%|██████████| 8/8 [00:39<00:00,  4.97s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch2...: 100%|██████████| 8/8 [00:40<00:00,  5.00s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch3...: 100%|██████████| 8/8 [00:40<00:00,  5.03s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch4...: 100%|██████████| 8/8 [00:45<00:00,  5.75s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch5...: 100%|██████████| 8/8 [00:40<00:00,  5.02s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch6...: 100%|██████████| 8/8 [00:40<00:00,  5.04s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch7...: 100%|██████████| 8/8 [00:40<00:00,  5.01s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch8...: 100%|██████████| 8/8 [00:40<00:00,  5.02s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch9...: 100%|██████████| 8/8 [00:40<00:00,  5.05s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch10...: 100%|██████████| 8/8 [00:40<00:00,  5.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch10: curr_epoch_loss=0.3985184133052826,execution_time=0:00:40.242660,lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Asthma_USE-Lr0.0001-Epoch11...: 100%|██████████| 8/8 [00:40<00:00,  5.10s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch12...: 100%|██████████| 8/8 [00:39<00:00,  4.98s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch13...: 100%|██████████| 8/8 [00:39<00:00,  4.94s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch14...: 100%|██████████| 8/8 [00:39<00:00,  4.93s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch15...: 100%|██████████| 8/8 [00:40<00:00,  5.04s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch16...: 100%|██████████| 8/8 [00:39<00:00,  4.92s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch17...: 100%|██████████| 8/8 [00:39<00:00,  4.91s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch18...: 100%|██████████| 8/8 [00:39<00:00,  4.89s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch19...: 100%|██████████| 8/8 [00:39<00:00,  4.96s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch20...: 100%|██████████| 8/8 [00:40<00:00,  5.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch20: curr_epoch_loss=0.3935902714729309,execution_time=0:00:40.301080,lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Asthma_USE-Lr0.0001-Epoch21...: 100%|██████████| 8/8 [00:39<00:00,  4.89s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch22...: 100%|██████████| 8/8 [00:38<00:00,  4.83s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch23...: 100%|██████████| 8/8 [00:39<00:00,  4.99s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch24...: 100%|██████████| 8/8 [00:39<00:00,  4.90s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch25...: 100%|██████████| 8/8 [00:38<00:00,  4.86s/it]\n",
      "Evaluating Asthma_USE...: 100%|██████████| 2/2 [00:08<00:00,  4.43s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch1...: 100%|██████████| 8/8 [00:43<00:00,  5.42s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch2...: 100%|██████████| 8/8 [00:41<00:00,  5.22s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch3...: 100%|██████████| 8/8 [00:40<00:00,  5.01s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch4...: 100%|██████████| 8/8 [00:40<00:00,  5.00s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch5...: 100%|██████████| 8/8 [00:39<00:00,  4.92s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch6...: 100%|██████████| 8/8 [00:39<00:00,  4.92s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch7...: 100%|██████████| 8/8 [00:39<00:00,  4.97s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch8...: 100%|██████████| 8/8 [00:39<00:00,  4.94s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch9...: 100%|██████████| 8/8 [00:39<00:00,  4.92s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch10...: 100%|██████████| 8/8 [00:39<00:00,  4.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch10: curr_epoch_loss=0.3981943726539612,execution_time=0:00:39.684304,lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Asthma_USE-Lr0.0001-Epoch11...: 100%|██████████| 8/8 [00:37<00:00,  4.72s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch12...: 100%|██████████| 8/8 [00:37<00:00,  4.73s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch13...: 100%|██████████| 8/8 [00:37<00:00,  4.74s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch14...: 100%|██████████| 8/8 [00:37<00:00,  4.74s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch15...: 100%|██████████| 8/8 [00:37<00:00,  4.71s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch16...: 100%|██████████| 8/8 [00:37<00:00,  4.73s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch17...: 100%|██████████| 8/8 [00:38<00:00,  4.77s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch18...: 100%|██████████| 8/8 [00:37<00:00,  4.74s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch19...: 100%|██████████| 8/8 [00:37<00:00,  4.73s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch20...: 100%|██████████| 8/8 [00:37<00:00,  4.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch20: curr_epoch_loss=0.39294156432151794,execution_time=0:00:37.745359,lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Asthma_USE-Lr0.0001-Epoch21...: 100%|██████████| 8/8 [00:37<00:00,  4.71s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch22...: 100%|██████████| 8/8 [00:37<00:00,  4.70s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch23...: 100%|██████████| 8/8 [00:37<00:00,  4.71s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch24...: 100%|██████████| 8/8 [00:37<00:00,  4.74s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch25...: 100%|██████████| 8/8 [00:43<00:00,  5.42s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch26...: 100%|██████████| 8/8 [00:37<00:00,  4.72s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch27...: 100%|██████████| 8/8 [00:37<00:00,  4.73s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch28...: 100%|██████████| 8/8 [00:39<00:00,  4.95s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch29...: 100%|██████████| 8/8 [00:40<00:00,  5.03s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch30...: 100%|██████████| 8/8 [00:40<00:00,  5.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch30: curr_epoch_loss=0.3772171437740326,execution_time=0:00:40.102844,lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Asthma_USE-Lr0.0001-Epoch31...: 100%|██████████| 8/8 [00:39<00:00,  4.94s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch32...: 100%|██████████| 8/8 [00:38<00:00,  4.77s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch33...: 100%|██████████| 8/8 [00:38<00:00,  4.81s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch34...: 100%|██████████| 8/8 [00:38<00:00,  4.82s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch35...: 100%|██████████| 8/8 [00:38<00:00,  4.76s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch36...: 100%|██████████| 8/8 [00:38<00:00,  4.77s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch37...: 100%|██████████| 8/8 [00:38<00:00,  4.77s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch38...: 100%|██████████| 8/8 [00:38<00:00,  4.79s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch39...: 100%|██████████| 8/8 [00:39<00:00,  4.88s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch40...: 100%|██████████| 8/8 [00:38<00:00,  4.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch40: curr_epoch_loss=0.31914186477661133,execution_time=0:00:38.290399,lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Asthma_USE-Lr0.0001-Epoch41...: 100%|██████████| 8/8 [00:38<00:00,  4.82s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch42...: 100%|██████████| 8/8 [00:38<00:00,  4.79s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch43...: 100%|██████████| 8/8 [00:38<00:00,  4.80s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch44...: 100%|██████████| 8/8 [00:38<00:00,  4.82s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch45...: 100%|██████████| 8/8 [00:37<00:00,  4.74s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch46...: 100%|██████████| 8/8 [00:37<00:00,  4.75s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch47...: 100%|██████████| 8/8 [00:38<00:00,  4.78s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch48...: 100%|██████████| 8/8 [00:44<00:00,  5.59s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch49...: 100%|██████████| 8/8 [00:38<00:00,  4.77s/it]\n",
      "Training Asthma_USE-Lr0.0001-Epoch50...: 100%|██████████| 8/8 [00:39<00:00,  4.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch50: curr_epoch_loss=0.2319374680519104,execution_time=0:00:39.416243,lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Asthma_USE...: 100%|██████████| 2/2 [00:08<00:00,  4.40s/it]\n"
     ]
    }
   ],
   "source": [
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "#Override these if need be\n",
    "#disease_list = ['Asthma', 'CAD', 'CHF', 'Depression', 'Diabetes', 'Gallstones', 'GERD', 'Gout', 'Hypercholesterolemia', 'Hypertension', 'Hypertriglyceridemia', 'OA', 'OSA', 'PVD', 'Venous Insufficiency', 'Obesity']\n",
    "disease_list = ['Asthma']\n",
    "embedding_list = ['GloVe','FastText','USE']\n",
    "#embedding_list = ['GloVe']\n",
    "epoch_list = [10,25,50]\n",
    "#epoch_list = [1]\n",
    "\n",
    "#0.01 seems to be the most effective, added decay logic - starting at 0.1 seems to cause NaNs, if fix those it gets \"stuck\"\n",
    "lr_list = [0.01,0.001,0.0001]\n",
    "\n",
    "results_file = f'{RESULTS_PATH}DL_embedding_results.csv'\n",
    "\n",
    "#training parameters\n",
    "batch_size = 128\n",
    "k = 2\n",
    "\n",
    "#These should not change\n",
    "\n",
    "result_time = datetime.datetime.now()\n",
    "result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "batch_name = f'DL_embedding_results_{result_name}'\n",
    "\n",
    "iterateTrainAndEvaluate(all_df_expanded, k, disease_list, embedding_list, lr_list, batch_name, results_file, device, epoch_list, False, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch</th>\n",
       "      <th>Disease</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1_MACRO</th>\n",
       "      <th>F1_MICRO</th>\n",
       "      <th>Exec Time</th>\n",
       "      <th>Total Run (secs)</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>LR</th>\n",
       "      <th>CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>GloVe</td>\n",
       "      <td>0.635285</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.567221</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>Trn,Eval,Ttl=0:06:54.587015,0:00:04.568539,0:0...</td>\n",
       "      <td>419.155554</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>GloVe</td>\n",
       "      <td>0.640388</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.633357</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>Trn,Eval,Ttl=0:12:48.305454,0:00:04.393396,0:1...</td>\n",
       "      <td>772.698849</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>GloVe</td>\n",
       "      <td>0.638729</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.650038</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>Trn,Eval,Ttl=0:27:31.390557,0:00:04.482974,0:2...</td>\n",
       "      <td>1655.873531</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>GloVe</td>\n",
       "      <td>0.635923</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>0.563658</td>\n",
       "      <td>0.845833</td>\n",
       "      <td>Trn,Eval,Ttl=0:04:50.214639,0:00:04.589322,0:0...</td>\n",
       "      <td>294.803961</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>GloVe</td>\n",
       "      <td>0.709784</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.590223</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>Trn,Eval,Ttl=0:12:37.010035,0:00:04.426170,0:1...</td>\n",
       "      <td>761.436204</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>GloVe</td>\n",
       "      <td>0.550963</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.516486</td>\n",
       "      <td>0.829167</td>\n",
       "      <td>Trn,Eval,Ttl=0:24:55.656996,0:00:04.554401,0:2...</td>\n",
       "      <td>1500.211397</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>GloVe</td>\n",
       "      <td>0.645490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455782</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>Trn,Eval,Ttl=0:04:55.498826,0:00:04.467984,0:0...</td>\n",
       "      <td>299.966810</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>GloVe</td>\n",
       "      <td>0.682357</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.501889</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>Trn,Eval,Ttl=0:12:29.489329,0:00:04.187737,0:1...</td>\n",
       "      <td>753.677066</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>GloVe</td>\n",
       "      <td>0.716673</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.478147</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>Trn,Eval,Ttl=0:25:08.262258,0:00:04.359152,0:2...</td>\n",
       "      <td>1512.621409</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>FastText</td>\n",
       "      <td>0.581707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455782</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>Trn,Eval,Ttl=0:09:08.658882,0:00:11.394890,0:0...</td>\n",
       "      <td>560.053772</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>FastText</td>\n",
       "      <td>0.504911</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.506376</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>Trn,Eval,Ttl=0:21:52.236733,0:00:10.124243,0:2...</td>\n",
       "      <td>1322.360977</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>FastText</td>\n",
       "      <td>0.507845</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Trn,Eval,Ttl=0:43:46.415269,0:00:09.588437,0:4...</td>\n",
       "      <td>2636.003706</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>FastText</td>\n",
       "      <td>0.565761</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.513889</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>Trn,Eval,Ttl=0:07:44.745163,0:00:09.569689,0:0...</td>\n",
       "      <td>474.314852</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>FastText</td>\n",
       "      <td>0.637709</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.633788</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>Trn,Eval,Ttl=0:19:55.180235,0:00:10.011191,0:2...</td>\n",
       "      <td>1205.191427</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>FastText</td>\n",
       "      <td>0.529915</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.543988</td>\n",
       "      <td>0.820833</td>\n",
       "      <td>Trn,Eval,Ttl=0:43:11.056850,0:00:09.761891,0:4...</td>\n",
       "      <td>2600.818740</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>FastText</td>\n",
       "      <td>0.596887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455782</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>Trn,Eval,Ttl=0:08:01.229912,0:00:09.686429,0:0...</td>\n",
       "      <td>490.916341</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>FastText</td>\n",
       "      <td>0.650338</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.508837</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>Trn,Eval,Ttl=0:20:23.365684,0:00:09.888698,0:2...</td>\n",
       "      <td>1233.254382</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>FastText</td>\n",
       "      <td>0.685419</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.572193</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>Trn,Eval,Ttl=0:42:41.192175,0:00:09.712347,0:4...</td>\n",
       "      <td>2570.904522</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>USE</td>\n",
       "      <td>0.707233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455782</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>Trn,Eval,Ttl=0:06:28.656416,0:00:08.563054,0:0...</td>\n",
       "      <td>397.219469</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>USE</td>\n",
       "      <td>0.846026</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.687011</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>Trn,Eval,Ttl=0:16:07.732962,0:00:08.752299,0:1...</td>\n",
       "      <td>976.485261</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>USE</td>\n",
       "      <td>0.810690</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.771501</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>Trn,Eval,Ttl=0:32:21.009896,0:00:08.464279,0:3...</td>\n",
       "      <td>1949.474175</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>USE</td>\n",
       "      <td>0.721903</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.481818</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>Trn,Eval,Ttl=0:06:31.082780,0:00:08.607351,0:0...</td>\n",
       "      <td>399.690131</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>USE</td>\n",
       "      <td>0.773951</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.716798</td>\n",
       "      <td>0.870833</td>\n",
       "      <td>Trn,Eval,Ttl=0:16:18.982617,0:00:09.076009,0:1...</td>\n",
       "      <td>988.058626</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>USE</td>\n",
       "      <td>0.800995</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>Trn,Eval,Ttl=0:33:26.423054,0:00:08.996030,0:3...</td>\n",
       "      <td>2015.419084</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>USE</td>\n",
       "      <td>0.578518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455782</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>Trn,Eval,Ttl=0:06:40.867209,0:00:09.262751,0:0...</td>\n",
       "      <td>410.129959</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>USE</td>\n",
       "      <td>0.574180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455782</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>Trn,Eval,Ttl=0:16:40.930958,0:00:08.924216,0:1...</td>\n",
       "      <td>1009.855174</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DL_embedding_results_2023-04-12-08-26-58</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>USE</td>\n",
       "      <td>0.792576</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.607618</td>\n",
       "      <td>0.845833</td>\n",
       "      <td>Trn,Eval,Ttl=0:32:25.909197,0:00:08.849197,0:3...</td>\n",
       "      <td>1954.758394</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Batch Disease Embedding     AUROC  \\\n",
       "0   DL_embedding_results_2023-04-12-08-26-58  Asthma     GloVe  0.635285   \n",
       "1   DL_embedding_results_2023-04-12-08-26-58  Asthma     GloVe  0.640388   \n",
       "2   DL_embedding_results_2023-04-12-08-26-58  Asthma     GloVe  0.638729   \n",
       "3   DL_embedding_results_2023-04-12-08-26-58  Asthma     GloVe  0.635923   \n",
       "4   DL_embedding_results_2023-04-12-08-26-58  Asthma     GloVe  0.709784   \n",
       "5   DL_embedding_results_2023-04-12-08-26-58  Asthma     GloVe  0.550963   \n",
       "6   DL_embedding_results_2023-04-12-08-26-58  Asthma     GloVe  0.645490   \n",
       "7   DL_embedding_results_2023-04-12-08-26-58  Asthma     GloVe  0.682357   \n",
       "8   DL_embedding_results_2023-04-12-08-26-58  Asthma     GloVe  0.716673   \n",
       "9   DL_embedding_results_2023-04-12-08-26-58  Asthma  FastText  0.581707   \n",
       "10  DL_embedding_results_2023-04-12-08-26-58  Asthma  FastText  0.504911   \n",
       "11  DL_embedding_results_2023-04-12-08-26-58  Asthma  FastText  0.507845   \n",
       "12  DL_embedding_results_2023-04-12-08-26-58  Asthma  FastText  0.565761   \n",
       "13  DL_embedding_results_2023-04-12-08-26-58  Asthma  FastText  0.637709   \n",
       "14  DL_embedding_results_2023-04-12-08-26-58  Asthma  FastText  0.529915   \n",
       "15  DL_embedding_results_2023-04-12-08-26-58  Asthma  FastText  0.596887   \n",
       "16  DL_embedding_results_2023-04-12-08-26-58  Asthma  FastText  0.650338   \n",
       "17  DL_embedding_results_2023-04-12-08-26-58  Asthma  FastText  0.685419   \n",
       "18  DL_embedding_results_2023-04-12-08-26-58  Asthma       USE  0.707233   \n",
       "19  DL_embedding_results_2023-04-12-08-26-58  Asthma       USE  0.846026   \n",
       "20  DL_embedding_results_2023-04-12-08-26-58  Asthma       USE  0.810690   \n",
       "21  DL_embedding_results_2023-04-12-08-26-58  Asthma       USE  0.721903   \n",
       "22  DL_embedding_results_2023-04-12-08-26-58  Asthma       USE  0.773951   \n",
       "23  DL_embedding_results_2023-04-12-08-26-58  Asthma       USE  0.800995   \n",
       "24  DL_embedding_results_2023-04-12-08-26-58  Asthma       USE  0.578518   \n",
       "25  DL_embedding_results_2023-04-12-08-26-58  Asthma       USE  0.574180   \n",
       "26  DL_embedding_results_2023-04-12-08-26-58  Asthma       USE  0.792576   \n",
       "\n",
       "          F1  F1_MACRO  F1_MICRO  \\\n",
       "0   0.217391  0.567221  0.850000   \n",
       "1   0.346154  0.633357  0.858333   \n",
       "2   0.377358  0.650038  0.862500   \n",
       "3   0.212766  0.563658  0.845833   \n",
       "4   0.269231  0.590223  0.841667   \n",
       "5   0.127660  0.516486  0.829167   \n",
       "6   0.000000  0.455782  0.837500   \n",
       "7   0.093023  0.501889  0.837500   \n",
       "8   0.047619  0.478147  0.833333   \n",
       "9   0.000000  0.455782  0.837500   \n",
       "10  0.117647  0.506376  0.812500   \n",
       "11  0.200000  0.542857  0.800000   \n",
       "12  0.125000  0.513889  0.825000   \n",
       "13  0.360656  0.633788  0.837500   \n",
       "14  0.188679  0.543988  0.820833   \n",
       "15  0.000000  0.455782  0.837500   \n",
       "16  0.120000  0.508837  0.816667   \n",
       "17  0.235294  0.572193  0.837500   \n",
       "18  0.000000  0.455782  0.837500   \n",
       "19  0.444444  0.687011  0.875000   \n",
       "20  0.608696  0.771501  0.887500   \n",
       "21  0.050000  0.481818  0.841667   \n",
       "22  0.507937  0.716798  0.870833   \n",
       "23  0.500000  0.711538  0.866667   \n",
       "24  0.000000  0.455782  0.837500   \n",
       "25  0.000000  0.455782  0.837500   \n",
       "26  0.301887  0.607618  0.845833   \n",
       "\n",
       "                                            Exec Time  Total Run (secs)  \\\n",
       "0   Trn,Eval,Ttl=0:06:54.587015,0:00:04.568539,0:0...        419.155554   \n",
       "1   Trn,Eval,Ttl=0:12:48.305454,0:00:04.393396,0:1...        772.698849   \n",
       "2   Trn,Eval,Ttl=0:27:31.390557,0:00:04.482974,0:2...       1655.873531   \n",
       "3   Trn,Eval,Ttl=0:04:50.214639,0:00:04.589322,0:0...        294.803961   \n",
       "4   Trn,Eval,Ttl=0:12:37.010035,0:00:04.426170,0:1...        761.436204   \n",
       "5   Trn,Eval,Ttl=0:24:55.656996,0:00:04.554401,0:2...       1500.211397   \n",
       "6   Trn,Eval,Ttl=0:04:55.498826,0:00:04.467984,0:0...        299.966810   \n",
       "7   Trn,Eval,Ttl=0:12:29.489329,0:00:04.187737,0:1...        753.677066   \n",
       "8   Trn,Eval,Ttl=0:25:08.262258,0:00:04.359152,0:2...       1512.621409   \n",
       "9   Trn,Eval,Ttl=0:09:08.658882,0:00:11.394890,0:0...        560.053772   \n",
       "10  Trn,Eval,Ttl=0:21:52.236733,0:00:10.124243,0:2...       1322.360977   \n",
       "11  Trn,Eval,Ttl=0:43:46.415269,0:00:09.588437,0:4...       2636.003706   \n",
       "12  Trn,Eval,Ttl=0:07:44.745163,0:00:09.569689,0:0...        474.314852   \n",
       "13  Trn,Eval,Ttl=0:19:55.180235,0:00:10.011191,0:2...       1205.191427   \n",
       "14  Trn,Eval,Ttl=0:43:11.056850,0:00:09.761891,0:4...       2600.818740   \n",
       "15  Trn,Eval,Ttl=0:08:01.229912,0:00:09.686429,0:0...        490.916341   \n",
       "16  Trn,Eval,Ttl=0:20:23.365684,0:00:09.888698,0:2...       1233.254382   \n",
       "17  Trn,Eval,Ttl=0:42:41.192175,0:00:09.712347,0:4...       2570.904522   \n",
       "18  Trn,Eval,Ttl=0:06:28.656416,0:00:08.563054,0:0...        397.219469   \n",
       "19  Trn,Eval,Ttl=0:16:07.732962,0:00:08.752299,0:1...        976.485261   \n",
       "20  Trn,Eval,Ttl=0:32:21.009896,0:00:08.464279,0:3...       1949.474175   \n",
       "21  Trn,Eval,Ttl=0:06:31.082780,0:00:08.607351,0:0...        399.690131   \n",
       "22  Trn,Eval,Ttl=0:16:18.982617,0:00:09.076009,0:1...        988.058626   \n",
       "23  Trn,Eval,Ttl=0:33:26.423054,0:00:08.996030,0:3...       2015.419084   \n",
       "24  Trn,Eval,Ttl=0:06:40.867209,0:00:09.262751,0:0...        410.129959   \n",
       "25  Trn,Eval,Ttl=0:16:40.930958,0:00:08.924216,0:1...       1009.855174   \n",
       "26  Trn,Eval,Ttl=0:32:25.909197,0:00:08.849197,0:3...       1954.758394   \n",
       "\n",
       "    Epochs      LR     CV  \n",
       "0       10  0.0100  False  \n",
       "1       25  0.0100  False  \n",
       "2       50  0.0100  False  \n",
       "3       10  0.0010  False  \n",
       "4       25  0.0010  False  \n",
       "5       50  0.0010  False  \n",
       "6       10  0.0001  False  \n",
       "7       25  0.0001  False  \n",
       "8       50  0.0001  False  \n",
       "9       10  0.0100  False  \n",
       "10      25  0.0100  False  \n",
       "11      50  0.0100  False  \n",
       "12      10  0.0010  False  \n",
       "13      25  0.0010  False  \n",
       "14      50  0.0010  False  \n",
       "15      10  0.0001  False  \n",
       "16      25  0.0001  False  \n",
       "17      50  0.0001  False  \n",
       "18      10  0.0100  False  \n",
       "19      25  0.0100  False  \n",
       "20      50  0.0100  False  \n",
       "21      10  0.0010  False  \n",
       "22      25  0.0010  False  \n",
       "23      50  0.0010  False  \n",
       "24      10  0.0001  False  \n",
       "25      25  0.0001  False  \n",
       "26      50  0.0001  False  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(results_file)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning - Word Embeddings - All Features - With Stop Words**\n",
    "\n",
    "![DL BagOfWords AllFeatures Averaged](images\\dl-we-swyes.gif)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10pK5od01jfTHJyLN94dJxEube3sJszFm",
     "timestamp": 1678482671183
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
