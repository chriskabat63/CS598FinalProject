{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"3d1ehXFWM5aE"},"source":["This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n","***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n","\n","\n","\n"," "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gv360l2IkNfO"},"source":["The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1250,"status":"ok","timestamp":1679769727418,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"zyXrAo2dsJqf","outputId":"0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","import datetime\n","from datetime import timedelta\n","from tqdm import tqdm\n","import torchtext\n","\n","# set seed\n","seed = 24\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","# define data path\n","DATA_PATH = './obesity_data/'\n","RESULTS_PATH = './results/'\n","MODELS_PATH = './models/'\n","\n","if os.path.exists(RESULTS_PATH) == False:\n","    os.mkdir(RESULTS_PATH)\n","if os.path.exists(MODELS_PATH) == False:\n","    os.mkdir(MODELS_PATH)\n","\n","\n","test_df = pd.read_pickle(DATA_PATH + '/test.pkl') \n","train_df = pd.read_pickle(DATA_PATH + '/train.pkl') \n","#corpus = pd.read_pickle(DATA_PATH + '/corpus.pkl')\n","disease_list = test_df['disease'].unique().tolist()\n","embedding_list = ['GloVe', \"FastText\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#rebuild the vocabulary\n","#import torchtext, torch, torch.nn.functional as F\n","#from torchtext.data.utils import get_tokenizer\n","#from torchtext.vocab import build_vocab_from_iterator\n","\n","#tokenizer = get_tokenizer(\"basic_english\")\n","#tokens = [tokenizer(doc) for doc in corpus]\n","\n","#voc = build_vocab_from_iterator(tokens, specials = ['<pad>'])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Deep Learning - Bag of Words - All Feature Selections - Averaged**\n","\n","![DL BagOfWords AllFeatures Averaged](images\\DL-BagOfWords-ByFeatureSelection.gif)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["****DL Model using word embeddings****\n","\n","First we start by creating a dataset.  Note this will have to take the disease as part of the init and filter just for those records."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","class ClinicalNoteDataset(Dataset):\n","\n","    def __init__(self, dataframe, disease, dataformat):\n","        \"\"\"\n","        TODO: init the Dataset instance.  datafomat is just the column to use from the dataframe 'vector_tokenized' , 'one_hot'\n","        \"\"\"\n","        # your code here\n","        self.disease = disease\n","        self.dataformat = dataformat\n","        self.df = dataframe[dataframe['disease'] == disease].copy()\n","        self.df = self.df.reset_index()\n","\n","    def __len__(self):\n","        \"\"\"\n","        TODO: Denotes the total number of samples\n","        \"\"\"\n","        return len(self.df)\n","\n","    def __getitem__(self, i):\n","        \"\"\"\n","        TODO: Generates one sample of data\n","            return X, y for the i-th data.\n","        \"\"\"\n","        #Cannot make tensors yet, will need to happen in collate\n","        Y = self.df.iloc[i]['judgment']\n","        X = self.df.iloc[i][self.dataformat]\n","\n","        return X,Y\n","        \n","def vectorize_batch_GloVe(batch):\n","    embedding_size_used = 300\n","    vec = torchtext.vocab.GloVe(name='6B', dim=embedding_size_used)    \n","    Xi, Yi = batch[0]\n","    batch_size = len(batch)\n","\n","    X = torch.zeros(batch_size, len(Xi), embedding_size_used, dtype=torch.float)\n","    Y = torch.zeros((batch_size), dtype=torch.long)\n","    \n","    for i in range(len(batch)):\n","        x, y = batch[i]\n","        #vectors = vec.get_vecs_by_tokens(voc.lookup_tokens(x.tolist()))\n","        vectors = vec.get_vecs_by_tokens(x)\n","\n","        X[i] = vectors.float()\n","        Y[i] = torch.tensor(float(y == True))\n","\n","    return X,Y\n","\n","def vectorize_batch_FastText(batch):\n","    embedding_size_used = 300\n","    vec = torchtext.vocab.FastText()\n","    Xi, Yi = batch[0]\n","    batch_size = len(batch)\n","\n","    X = torch.zeros(batch_size, len(Xi), embedding_size_used, dtype=torch.float)\n","    Y = torch.zeros((batch_size), dtype=torch.long)\n","\n","    for i in range(len(batch)):\n","        x, y = batch[i]\n","        #vectors = vec.get_vecs_by_tokens(voc.lookup_tokens(x.tolist()))\n","        vectors = vec.get_vecs_by_tokens(x)\n","\n","        X[i] = vectors.float()\n","        Y[i] = torch.tensor(float(y == True))\n","\n","    return X,Y \n"," \n","          \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["##Test DataLoader\n","batch_size = 128\n","train_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(train_df, 'Asthma', 'vector_tokenized'), batch_size = batch_size, shuffle=True, collate_fn=vectorize_batch_FastText)\n","val_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(test_df, 'Asthma', 'vector_tokenized'), batch_size = batch_size, shuffle=False, collate_fn=vectorize_batch_FastText)\n","\n","print(\"# of train batches:\", len(train_loader))\n","print(\"# of val batches:\", len(val_loader))\n","\n","train_iter = iter(train_loader)\n","x,y = next(train_iter)\n","\n","print(x.shape)\n","print(y.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ClincalNoteEmbeddingNet(nn.Module):\n","    def __init__(self, embedding_type, max_tokens):\n","        super(ClincalNoteEmbeddingNet, self).__init__()\n","        \n","        self.embedding_type = embedding_type\n","        self.max_tokens = max_tokens\n","\n","        if(embedding_type == 'USE'):\n","            self.embedding_dimension = 512\n","        else:\n","            self.embedding_dimension = 300\n","\n","        self.hidden_dim1 = 128\n","        self.hidden_dim2 = 64\n","        self.num_layers = 1\n","\n","        #Because it is bidirectional, the output from LTSM is coming in twice the size of the hidden states required.\n","        #input is (batch, #of tokens * embedding_dimension)\n","        self.bilstm1 = nn.LSTM(input_size = self.embedding_dimension, hidden_size = self.hidden_dim1, bidirectional = True, batch_first = True, num_layers = self.num_layers, bias = False) \n","        self.bilstm2 = nn.LSTM(input_size = self.hidden_dim1 * 2, hidden_size = self.hidden_dim2, bidirectional = True, batch_first = True, num_layers=self.num_layers, bias = False)\n"," \n","        self.fc1 = nn.Linear(self.hidden_dim2 * self.max_tokens * 2, 2)\n","        #self.fc2 = nn.Linear(self.hidden_dim2 , 2)\n"," \n","    def forward(self, x):\n","        x, states = self.bilstm1(x)\n","        x, states = self.bilstm2(x)\n","        \n","        batch_size = x.shape[0]\n","        x = x.reshape(batch_size, -1)\n","        x = self.fc1(x)\n","        ##x = self.fc2(x)\n","        #x = F.softmax(x, dim=1) #Used with NLLLLoss()\n","\n","        return x #F.sigmoid(x).squeeze(dim=-1)\n","        #return torch.nan_to_num(x) #F.sigmoid(x).squeeze(dim=-1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["eps=1e-10\n","\n","def train_model(tmodel, train_dataloader, n_epoch=5, lr=0.003, device=None, model_name='unk'):\n","    import torch.optim as optim\n","    \n","    device = device or torch.device('cpu')\n","    tmodel.train()\n","\n","    loss_history = []\n","\n","    # your code here\n","    optimizer = optim.Adam(tmodel.parameters(), lr=lr)\n","    # want to decay the learning rate as teh number of epochs get larger\n","    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma = 0.1)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n","        factor=0.1, patience=10, threshold=0.0001, threshold_mode='abs')\n","\n","    #loss_func = nn.BCELoss()\n","    loss_func = nn.CrossEntropyLoss()\n","    #loss_func = nn.NLLLoss()\n","\n","    for epoch in range(n_epoch):\n","        epoch = epoch+1\n","        curr_epoch_loss = []\n","        start = time.time()\n","        for X, Y in tqdm(train_dataloader,desc=f\"Training {model_name}-Lr{str(lr)}-Epoch{epoch}...\"):\n","            # your code here\n","            optimizer.zero_grad()\n","\n","            y_hat = tmodel(X)\n","\n","            loss = loss_func(y_hat, Y)\n","            #loss = loss_func(torch.log(y_hat+ eps), Y)\n","            \n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step(loss)\n","            \n","            curr_epoch_loss.append(loss.cpu().data.numpy())\n","\n","\n","        end = time.time()\n","        #if epoch % 10 == 0:\n","        print(f\"epoch{epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)},execution_time={str(datetime.timedelta(seconds = (end-start)))},lr={optimizer.param_groups[0]['lr']}\")\n","\n","        #scheduler.step()\n","        loss_history += curr_epoch_loss\n","    return tmodel, loss_history\n","\n","def eval_model(emodel, dataloader, device=None, model_name='unk'):\n","    \"\"\"\n","    :return:\n","        pred_all: prediction of model on the dataloder.\n","        Y_test: truth labels. Should be an numpy array of ints\n","    TODO:\n","        evaluate the model using on the data in the dataloder.\n","        Add all the prediction and truth to the corresponding list\n","        Convert pred_all and Y_test to numpy arrays \n","    \"\"\"\n","    device = device or torch.device('cpu')\n","    emodel.eval()\n","    pred_all = []\n","    Y_test = []\n","    for X, Y in tqdm(dataloader, desc=f\"Evaluating {model_name}...\"):\n","        # your code here\n","        y_hat = emodel(X)\n","        \n","        pred_all.append(y_hat.detach().to('cpu'))\n","        Y_test.append(Y.detach().to('cpu'))\n","        \n","    pred_all = np.concatenate(pred_all, axis=0)\n","    Y_test = np.concatenate(Y_test, axis=0)\n","\n","    return pred_all, Y_test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def evaluate_predictions(truth, pred):\n","    \"\"\"\n","    TODO: Evaluate the performance of the predictoin via AUROC, and F1 score\n","    each prediction in pred is a vector representing [p_0, p_1].\n","    When defining the scores we are interesed in detecting class 1 only\n","    (Hint: use roc_auc_score and f1_score from sklearn.metrics, be sure to read their documentation)\n","    return: auroc, f1\n","    \"\"\"\n","    from sklearn.metrics import roc_auc_score, f1_score\n","\n","    # your code here\n","    auroc = roc_auc_score(truth, pred[:,1])\n","    f1 = f1_score(truth, np.argmax(pred,axis=1))\n","    f1_macro = f1_score(truth, np.argmax(pred,axis=1),average='macro')\n","    f1_micro = f1_score(truth, np.argmax(pred,axis=1),average='micro')\n","\n","    return auroc, f1, f1_macro, f1_micro"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Need to create a loop to train and evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Results will be stored as rows:diseases, cols:embedding,results(4 metrics), runtime\n","result_cols = ['Batch','Disease','Embedding','AUROC','F1','F1_MACRO', 'F1_MICRO', 'Exec Time', 'Total Run (secs)','Epochs', 'LR']\n","result_time = datetime.datetime.now()\n","#result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","results_file = f'{RESULTS_PATH}DL_embedding_results.csv'\n","#results_file = f'{RESULTS_PATH}DL_embedding_results_{result_name}.csv'\n","\n","def trainAndEvaluate(batch_name, disease, lr,  dataformat, embedding, device, max_tokens, n_epoch):\n","            \n","    return_val = False\n","\n","    #Create a name for the model\n","    model_name = f\"{disease}_{embedding}_{batch_name}\"\n","    model_desc = f\"{disease}_{embedding}\"\n","\n","    #Load Data \n","    if embedding == 'GloVe':\n","        train_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(train_df, disease, dataformat), batch_size = batch_size, shuffle=True, collate_fn=vectorize_batch_GloVe)\n","        val_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(test_df, disease, dataformat), batch_size = batch_size, shuffle=False, collate_fn=vectorize_batch_GloVe)\n","    if embedding == 'FastText':\n","        train_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(train_df, disease, dataformat), batch_size = batch_size, shuffle=True, collate_fn=vectorize_batch_FastText)\n","        val_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(test_df, disease, dataformat), batch_size = batch_size, shuffle=False, collate_fn=vectorize_batch_FastText)\n","\n","    #Train model\n","    model = ClincalNoteEmbeddingNet(embedding, max_tokens = max_tokens)\n","    model = model.to(device)\n","\n","    start_train = time.time()\n","    model, loss_history = train_model(model, train_loader, n_epoch=n_epoch, lr = lr, device=device, model_name=model_desc)\n","    end_train = time.time()\n","\n","    try:\n","        #Evaluate model\n","        start_eval = time.time()\n","        pred, truth = eval_model(model, val_loader, device=device, model_name=model_desc)\n","        end_eval = time.time()\n","\n","        auroc, f1, f1_macro, f1_micro = evaluate_predictions(truth, pred)\n","        runtime = f\"Trn,Eval,Ttl={str(datetime.timedelta(seconds = (end_train-start_train)))},{str(datetime.timedelta(seconds = (end_eval-start_eval)))},{str(datetime.timedelta(seconds = (end_eval-start_train)))}\"\n","        runtime_sec = end_eval-start_train\n","\n","        #Save model\n","        torch.save(model.state_dict(), f'{MODELS_PATH}{model_name}.pkl')\n","\n","        return_val = True\n","\n","    except:\n","        auroc = -1\n","        f1=-1\n","        f1_macro = -1\n","        f1_micro = -1\n","        runtime_sec = end_train-start_train\n","        runtime = 'Failure'\n","        print(\"Failure!\")\n","\n","\n","    #Delete model\n","    del model\n","\n","    #Append to results\n","    if os.path.exists(results_file):\n","        results = pd.read_csv(results_file)\n","    else:\n","        results = pd.DataFrame(columns=result_cols)\n","\n","    result = pd.DataFrame(columns=result_cols,data=[[batch_name, disease,embedding,auroc,f1,f1_macro,f1_micro,runtime,runtime_sec,n_epoch,lr]])\n","    results = pd.concat([results,result])\n","\n","    #Save results - overwrite so we can see progress\n","    results.to_csv(results_file, index=False)\n","\n","    return return_val\n","\n","def iterateTrainAndEvaluate(disease_list, embedding_list, lr_list, \n","                            batch_name, dataformat, device, max_tokens, n_epoch):\n","\n","    for _,disease in enumerate(disease_list):\n","        for _,embedding in enumerate(embedding_list):\n","            for _,lr in enumerate(lr_list):\n","                #test with one Epic to allow learning rate failure to be tested early - don't need this anymore as I've set NaN \n","                #if trainAndEvaluate(batch_name, disease, lr, dataformat, embedding, device, max_tokens, 1):\n","                trainAndEvaluate(batch_name, disease, lr, dataformat, embedding, device, max_tokens, n_epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device = 'cpu'\n","print(f'Using device: {device}')\n","\n","#Override these if need be\n","disease_list = ['Asthma', 'CAD', 'CHF', 'Depression', 'Diabetes', 'Gallstones', 'GERD', 'Gout', 'Hypercholesterolemia', 'Hypertension', 'Hypertriglyceridemia', 'OA', 'OSA', 'PVD', 'Venous Insufficiency', 'Obesity']\n","#disease_list = ['Asthma']\n","embedding_list = ['GloVe','FastText']\n","#0.01 seems to be the most effective, added decay logic - starting at 0.1 seems to cause NaNs, if fix those it gets \"stuck\"\n","lr_list = [0.01]\n","\n","#training parameters\n","n_epoch = 20\n","batch_size = 128\n","\n","#These should not change\n","dataformat = 'vector_tokenized'\n","max_tokens = 1430\n","\n","result_time = datetime.datetime.now()\n","result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","batch_name = f'DL_embedding_results_{result_name}'\n","\n","iterateTrainAndEvaluate(disease_list, embedding_list, lr_list, batch_name, dataformat, device, max_tokens, n_epoch)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results = pd.read_csv(results_file)\n","results"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Deep Learning - Word Embeddings - All Features - With Stop Words**\n","\n","![DL BagOfWords AllFeatures Averaged](images\\dl-we-swyes.gif)"]}],"metadata":{"colab":{"provenance":[{"file_id":"10pK5od01jfTHJyLN94dJxEube3sJszFm","timestamp":1678482671183}]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"}}},"nbformat":4,"nbformat_minor":0}
