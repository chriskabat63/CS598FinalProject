{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"3d1ehXFWM5aE"},"source":["This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n","***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n","\n","\n","\n"," "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gv360l2IkNfO"},"source":["The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1250,"status":"ok","timestamp":1679769727418,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"zyXrAo2dsJqf","outputId":"0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# set seed\n","seed = 24\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","# define data path\n","DATA_PATH = './obesity_data/'\n","\n","test_df = pd.read_pickle(DATA_PATH + '/test.pkl') \n","train_df = pd.read_pickle(DATA_PATH + '/train.pkl') \n","corpus = pd.read_pickle(DATA_PATH + '/corpus.pkl')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#rebuild the vocabulary\n","import torchtext, torch, torch.nn.functional as F\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","tokenizer = get_tokenizer(\"basic_english\")\n","tokens = [tokenizer(doc) for doc in corpus]\n","\n","voc = build_vocab_from_iterator(tokens, specials = ['<pad>'])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Deep Learning - Bag of Words - All Feature Selections - Averaged**\n","\n","![DL BagOfWords AllFeatures Averaged](images\\DL-BagOfWords-ByFeatureSelection.gif)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["****DL Model using word embeddings****\n","\n","First we start by creating a dataset.  Note this will have to take the disease as part of the init and filter just for those records."]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","class ClinicalNoteDataset(Dataset):\n","\n","    def __init__(self, dataframe, disease, embedding):\n","        \"\"\"\n","        TODO: init the Dataset instance.\n","        \"\"\"\n","        # your code here\n","        self.disease = disease\n","        self.embedding = embedding\n","        self.df = dataframe[dataframe['disease'] == disease]\n","\n","    def __len__(self):\n","        \"\"\"\n","        TODO: Denotes the total number of samples\n","        \"\"\"\n","        return len(self.df)\n","\n","    def __getitem__(self, i):\n","        \"\"\"\n","        TODO: Generates one sample of data\n","            return X, y for the i-th data.\n","        \"\"\"\n","        Y = torch.tensor(self.df.iloc[i]['judgment']).long()\n","\n","        X = self.df.iloc[i]['one_hot']\n","\n","        if self.embedding == 'GloVe':\n","            vec = torchtext.vocab.GloVe(name='6B', dim=300)\n","            X = vec.get_vecs_by_tokens(voc.lookup_tokens(X))\n","        elif self.embedding == 'FastText':\n","            vec = torchtext.vocab.FastText()\n","            X = vec.get_vecs_by_tokens(voc.lookup_tokens(X))\n","        else:\n","            X = torch.tensor(X)\n","            \n","        return X,Y\n","        \n","\n"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["# of train batches: 6\n","# of val batches: 5\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\chkabat\\AppData\\Local\\Temp/ipykernel_16952/2859277782.py:26: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n","  Y = torch.tensor(self.df.iloc[i]['judgment']).int()\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([128, 1430, 300])\n","torch.Size([128])\n"]}],"source":["##Test DataLoader\n","batch_size = 128\n","train_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(train_df, 'Asthma', 'GloVe'), batch_size = batch_size, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(test_df, 'Asthma', 'GloVe'), batch_size = batch_size, shuffle=False)\n","\n","print(\"# of train batches:\", len(train_loader))\n","print(\"# of val batches:\", len(val_loader))\n","\n","train_iter = iter(train_loader)\n","x,y = next(train_iter)\n","\n","print(x.shape)\n","print(y.shape)\n"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["class ClincalNoteEmbeddingNet(nn.Module):\n","    def __init__(self, embedding_type, max_tokens):\n","        super(ClincalNoteEmbeddingNet, self).__init__()\n","        \n","        self.embedding_type = embedding_type\n","        self.max_tokens = max_tokens\n","\n","        if(embedding_type == 'USE'):\n","            self.embedding_dimension = 512\n","        else:\n","            self.embedding_dimension = 300\n","\n","        #Because it is bidirectional, the output from LTSM is coming in twice the size of the hidden states required.\n","        #input is (batch, #of tokens * embedding_dimension)\n","        self.bilstm1 = nn.LSTM(self.embedding_dimension * self.max_tokens,64, bidirectional = True, batch_first = True) \n","        self.bilstm2 = nn.LSTM(64*2, 64, bidirectional = True, batch_first = True)\n","        self.fc1 = nn.Linear(64*2,2)\n","        \n","\n","    def forward(self, x):\n","        batch_size = x.shape[0]\n","\n","        #reshape as it is a 2 dimensional embedding\n","        x = x.reshape(batch_size, -1)\n","\n","        (output,states) = self.bilstm1(x)\n","        (output,states) = self.bilstm2(output)\n","        output = self.fc1(output)\n","        output = F.softmax(output, dim=1)\n","\n","        return output\n","\n"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["def train_model(model, train_dataloader, n_epoch=5, lr=0.003, device=None):\n","    import torch.optim as optim\n","    \n","    device = device or torch.device('cpu')\n","    model.train()\n","\n","    loss_history = []\n","\n","    # your code here\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    for epoch in range(n_epoch):\n","        curr_epoch_loss = []\n","        for X, Y in train_dataloader:\n","            # your code here\n","            optimizer.zero_grad()\n","\n","            y_hat = model(X)\n","\n","            loss = loss_func(y_hat, Y)\n","            \n","            loss.backward()\n","            optimizer.step()\n","            \n","            curr_epoch_loss.append(loss.cpu().data.numpy())\n","        print(f\"epoch{epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n","        loss_history += curr_epoch_loss\n","    return model, loss_history\n","\n","def eval_model(model, dataloader, device=None):\n","    \"\"\"\n","    :return:\n","        pred_all: prediction of model on the dataloder.\n","        Y_test: truth labels. Should be an numpy array of ints\n","    TODO:\n","        evaluate the model using on the data in the dataloder.\n","        Add all the prediction and truth to the corresponding list\n","        Convert pred_all and Y_test to numpy arrays \n","    \"\"\"\n","    device = device or torch.device('cpu')\n","    model.eval()\n","    pred_all = []\n","    Y_test = []\n","    for X, Y in dataloader:\n","        # your code here\n","        y_hat = model(X)\n","        \n","        pred_all.append(y_hat.detach().to('cpu'))\n","        Y_test.append(Y.detach().to('cpu'))\n","        \n","    pred_all = np.concatenate(pred_all, axis=0)\n","    Y_test = np.concatenate(Y_test, axis=0)\n","\n","    return pred_all, Y_test"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Need to create a loop to train and evaluate"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\chkabat\\AppData\\Local\\Temp/ipykernel_16952/3000934431.py:26: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n","  Y = torch.tensor(self.df.iloc[i]['judgment']).long()\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([32, 2])\n","torch.Size([32])\n","torch.Size([32, 2])\n","torch.Size([32])\n"]}],"source":["device = torch.device('cpu')\n","n_epoch = 20\n","lr = 0.003\n","batch_size = 32\n","disease_input = 'Asthma'\n","embedding_input = 'GloVe'\n","max_tokens = 1430\n","\n","train_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(train_df, disease_input, embedding_input), batch_size = batch_size, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(ClinicalNoteDataset(test_df, disease_input, embedding_input), batch_size = batch_size, shuffle=False)\n","\n","\n","model =ClincalNoteEmbeddingNet('GloVe', max_tokens = max_tokens)\n","model = model.to(device)\n","\n","\n","model, loss_history = train_model(model, train_loader, n_epoch=n_epoch, lr=lr, device=device)\n","pred, truth = eval_model(model, val_loader, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def evaluate_predictions(truth, pred):\n","    \"\"\"\n","    TODO: Evaluate the performance of the predictoin via AUROC, and F1 score\n","    each prediction in pred is a vector representing [p_0, p_1].\n","    When defining the scores we are interesed in detecting class 1 only\n","    (Hint: use roc_auc_score and f1_score from sklearn.metrics, be sure to read their documentation)\n","    return: auroc, f1\n","    \"\"\"\n","    from sklearn.metrics import roc_auc_score, f1_score\n","\n","    # your code here\n","    auroc = roc_auc_score(truth, pred[:,1])\n","    f1 = f1_score(truth, np.argmax(pred,axis=1))\n","    f1_macro = f1_score(truth, np.argmax(pred,axis=1),average='macro')\n","    f1_micro = f1_score(truth, np.argmax(pred,axis=1),average='micro')\n","\n","    return auroc, f1, f1_macro, f1_micro"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\chkabat\\AppData\\Local\\Temp/ipykernel_16952/3000934431.py:26: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n","  Y = torch.tensor(self.df.iloc[i]['judgment']).long()\n"]},{"name":"stdout","output_type":"stream","text":["AUROC=0.5616450683945284 and F1=0.0 and F1_macro=0.45841584158415843 and F1_micro=0.8464351005484461\n"]}],"source":["pred, truth = eval_model(model, val_loader, device=device)\n","auroc, f1, f1_macro, f1_micro = evaluate_predictions(truth, pred)\n","print(f\"AUROC={auroc} and F1={f1} and F1_macro={f1_macro} and F1_micro={f1_micro}\")\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Deep Learning - Word Embeddings - All Features - Averaged with Stop Words**\n","\n","![DL BagOfWords AllFeatures Averaged](images\\dl-we-swyes.gif)"]}],"metadata":{"colab":{"provenance":[{"file_id":"10pK5od01jfTHJyLN94dJxEube3sJszFm","timestamp":1678482671183}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
