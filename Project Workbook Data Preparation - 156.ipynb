{"cells":[{"cell_type":"markdown","metadata":{"id":"3d1ehXFWM5aE"},"source":["This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n","***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n","\n","Abstract:  The main goal of the paper is to extract Morbidity from clinical notes.  The idea was to use a combination of classical and deep learning methods to determine the best approach for classifying these notes in one or more of 16 morbidity conditions.  These models used a combination of NLP techniques including embeddings and bag of words implementations.  It also measured the effect including of stop words.  Lastly, it used ensemble techniques to tie together a number of the classical and deep learning models to provide the most accurate results.\n","\n","Dataset was retrieved from the DBMI Data Portal, Department of Biomedical Informatics (DBMI) in the Blavatnik Institute at Harvard Medical School.  This dataset was originally created for the i2b2 Obesity Challenge conducted in 2008.\n","This data was provided in XML format with a test and training set.  Along with the test and training set, labeled data of two forms were included. They were called Intuitive and Textual.  Textual judgements were derived by looking at the notes by multiple experts.  When the experts didn’t agree, a resident doctor annotated it with a Intuitive judgement.\n","\n","In this workbook, we are taking the following steps:\n","\n","\n","* Loading test and train data along with annotations\n","* Exploring the best annotation data sets to use\n","* Preprocessing the data using NLP techniques described below.\n","* Saving the data as pkl files for use in additional notebooks.\n","\n","\n","\n","\n","\n"," "]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4130,"status":"ok","timestamp":1679769726172,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"abcc0TyDkN4r","outputId":"12bdbdd7-6ffe-48fd-9800-bb5895f0b1e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: xmltodict in c:\\programdata\\anaconda3\\lib\\site-packages (0.12.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install xmltodict\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gv360l2IkNfO"},"source":["The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1250,"status":"ok","timestamp":1679769727418,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"zyXrAo2dsJqf","outputId":"0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"},"outputs":[],"source":["DATA_PATH = './obesity_data/'"]},{"cell_type":"markdown","metadata":{"id":"cw_odcoTOTS-"},"source":["Next we create a function to load the data from XML files and convert to a more usable dataframe structure."]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1679769727418,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"AKAxFxOCMadc"},"outputs":[],"source":["import pandas as pd\n","import xmltodict\n","\n","def load_dataset(filepath, xpath):    \n","    return pd.read_xml(filepath, xpath=xpath)\n","\n","def load_annotations(filepath):\n","\n","  with open(filepath,\"r\") as f:\n","      data = f.read()\n","\n","  df = pd.DataFrame(columns=['source','disease','id','judgment'])\n","\n","  data = xmltodict.parse(data)['diseaseset']['diseases']\n","\n","  for key,val in enumerate(data):\n","    if(isinstance(val,str)):\n","      source = data['@source']\n","      disease = data['disease']\n","    else:\n","      source = val['@source']\n","      disease = val['disease']\n","\n","    for key,val in enumerate(disease):\n","      if(isinstance(val,str)):\n","        disease_name = disease['@name']\n","        doc = disease['doc']\n","      else:\n","        disease_name = val['@name']\n","        doc = val['doc']\n","      \n","      for key,val in enumerate(doc):\n","        if(isinstance(val,str)):\n","          doc_id = doc['@id']\n","          judgment = doc['@judgment']\n","        else:\n","          doc_id = val['@id']\n","          judgment = val['@judgment']\n","        df_temp = pd.DataFrame([{\"source\":source,\"disease\":disease_name,\"id\":doc_id,\"judgment\":judgment}])\n","        #df = df.append(df_temp)  \n","        df = pd.concat([df,df_temp])\n","\n","  #The xml acts really strange if there are single nodes.  Dropping duplicates solves it.\n","  return df.drop_duplicates()"]},{"cell_type":"markdown","metadata":{"id":"rLs0aX04OgNn"},"source":["Now we load the test and train datasets and examine the notes. Note, we are loading the training file with 2 as a seperate data frame as it relates to all the addendums which we believe was not used by the paper."]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":154,"status":"ok","timestamp":1679769727568,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"yq2MgXu1naLN","outputId":"3cfb3f54-e246-4b7a-9799-15701ee92f28"},"outputs":[{"name":"stdout","output_type":"stream","text":["   id                                               text\n","0   3  470971328 | AECH | 09071283 | | 6159055 | 5/26...\n","1   5  508283935 | KFM | 67491508 | | 9707967 | 9/25/...\n","2   7  248652055 | CM | 07563073 | | 5027467 | 8/29/2...\n","3   8  052907410 | FTH | 50999409 | | 7815179 | 10/6/...\n","4   9  628477951 | MBCH | 30737210 | | 5713924 | 12/1...\n","507\n","   id                                               text\n","0   1  490646815 | WMC | 31530471 | | 9629480 | 11/23...\n","1   2  159644670 | VH | 60656526 | | 6334749 | 11/29/...\n","2   4  368346277 | EMH | 64927307 | | 815098 | 3/29/1...\n","3   6  018858680 | AOH | 80239131 | | 9725704 | 11/4/...\n","4  13  908761918 | MMC | 45427009 | | 0927689 | 5/26/...\n","611\n","730\n","470971328 | AECH | 09071283 | | 6159055 | 5/26/2006 12:00:00 AM | PNUEMONIA | Signed | DIS | Admission Date: 4/22/2006 Report Status: Signed\n","\n","Discharge Date: 7/27/2006\n","ATTENDING: CARINE , WALTER MD\n","SERVICE:\n","Medicine Service.\n","ADMISSION INFORMATION AND CHIEF COMPLAINT:\n","Hypoxemic respiratory failure.\n","HISTORY OF PRESENT ILLNESS:\n","The patient is a 57-year-old woman with a past medical history of\n","OSA , asthma , CAD status post CABG. On 8/19/06 , she underwent a\n","right total knee replacement at Dola Elan Hospital .  On\n","8/9/06 , she was discharged to rehabilitation. There , she\n","experienced fever , cough and dyspnea. She was started on\n","vancomycin , ceftazidime , and Flagyl for presumed pneumonia. In\n","the L ED , the patient was afebrile with a temperature of 97.6 ,\n","pulse of 88 , blood pressure 117/70 , oxygen saturation 97% on 6\n","liters nasal cannula. Her exam was notable for crackles in the\n","left base and 1+ lower extremity edema.\n","ADMISSION LABS:\n","Notable for white blood cell count of 20 , hematocrit 3of 5 ,\n","platelets of 442 , 000 , creatinine of 0.6 , and INR of 1.2. Her\n","admission EKG revealed sinus tachycardia of 119 beats per minute ,\n","normal axis , QRS 104 milliseconds , QTC 461 milliseconds , no\n","evidence of atrial enlargement or ventricular hypertrophy , poor\n","R-wave progression , 2 mm ST depressions and T-wave inversions in\n","leads 1 , aVL , V5 , V6 , 1 mm J-point elevation in V3 ( prior EKG\n","showed T-wave inversions in 1 , and aVL with no ST depressions ).\n","Her admission chest x-ray revealed bilateral diffuse patchy\n","opacities.\n","The patient was presumed to have pneumonia versus CHF. She was\n","treated with vancomycin , cefotaxime , levofloxacin , and\n","azithromycin , and was admitted to the Medicine Service for\n","further evaluation and management.\n","PAST MEDICAL HISTORY:\n","1. Left carotid artery stenosis status post CEA.\n","2. Right carotid artery stenosis , status post angioplasty.\n","3. OSA.\n","4. Asthma.\n","5. CAD status post three-vessel CABG in 2004 and subsequent PCI\n","to the ramus in 2005.\n","6. 70-80% RCA stenosis not bypassed during CABG.\n","7. Hypertension.\n","8. CHF , ejection fraction 45-50%.\n","9. AS status post aortic valve replacement.\n","10. Pericarditis removal.\n","11. Diabetes.\n","12. Peripheral vascular disease.\n","MEDICATIONS AT REHAB:\n","1. Vancomycin 1 gram IV q. 12h. , ( first dose 27 of March ).\n","2. Ceftazidime 1 g IV q. 8h. , ( first dose 7/17/06 )\n","3. Flagyl 500 mg IV q. 8h. , ( first dose 7/17/06 .\n","4. Advair 100/50 inhaled b.i.d.\n","5. Aspirin 325 mg p.o. daily.\n","6. Lipitor 80 mg p.o. at bedtime.\n","7. Zetia 10 mg p.o. daily.\n","8. Lopressor 75 mg p.o. q. 6h.\n","9. Lasix 1 tablet p.o. daily.\n","10. Colace 100 mg p.o. b.i.d.\n","11. Multivitamin 1 tab p.o. daily.\n","12. CaCO3 500 mg p.o. daily.\n","13. Cholecalciferol 400 units p.o. daily.\n","14. Ferrous sulfate 300 mg p.o. t.i.d.\n","15. Folic acid 1 mg p.o. daily.\n","16. Avapro 225 mg p.o. daily.\n","17. Lantus 100 units subq daily.\n","18. Lispro sliding scale.\n","19. Coumadin.\n","20. P.r.n. oxycodone , Tylenol , Benadryl , and Metamucil.\n","ALLERGIES:\n","Lisinopril leads to cough and metformin leads to GI distress.\n","SOCIAL HISTORY:\n","The patient was formerly employed as a cashier. She has two\n","children. She is a former cigarette smoker. She does not use\n","alcohol.\n","FAMILY HISTORY:\n","The patient has a positive family history of coronary disease ,\n","hypertension and diabetes.\n","HOSPITAL COURSE BY SYSTEM/ PROBLEM:\n","Persistent pulmonary\n","1. Hypoxemic respiratory failure. On 1/7/06 , shortly after\n","her admission to the medical floor , the patient was noted to be\n","in respiratory distress with tachypnea , accessory muscle use and\n","oxygen saturation of 68% on 6 liters nasal cannula. She was\n","placed on a nonrebreather. Her oxygen saturation increased to\n","93%; however , she continued to be in respiratory distress with\n","tachypnea and accessory muscle use. She was intubated and\n","transferred to the Medical Intensive Care Unit for further\n","evaluation and management. Her respiratory failure was thought\n","to be secondary to pneumonia with a component of superimposed\n","volume overload. She was treated with a 10-day course of\n","vancomycin , levofloxacin and ceftazidime as well as with IV\n","Lasix. She underwent a code green on 7/30/06 during an ETT tube\n","change , wherein a patent airway was transiently loss. The\n","patient was slow to wean from the ventilator. Her chest imaging\n","revealed persistent bilateral opacifications. It was thought\n","that after an initial infectious insult , the patient developed\n","ARDS. On 9/14/06 , the patient underwent bronchoscopy and BAL\n","revealing MRSA and HSV. The patient was treated with a 10-day\n","course of acyclovir for presumed HSV tracheobronchitis. Given\n","her inability to be weaned from the vent , the patient underwent a\n","tracheostomy on 4/17/06 .  Post-tracheostomy , the patient\n","alternated between pressure support ventilation with low driving\n","pressure and PEEP with a trach collar..\n","Infectious disease:\n","1. Fevers. From 1/7/06 to 7/28/06 , the patient was treated\n","with vancomycin , levofloxacin and ceftazidime for hospital\n","acquired pneumonia. After discontinuation of her antibiotics ,\n","the patient continued to spike fevers and evidence of\n","leukocytosis. In verification into source of her fevers included\n","serial blood cultures , urine cultures and C-dif. Positive data\n","included: 7/28/06 , urine culture with yeast , 7/28/06 , blood\n","culture with coag-negative staph , 9/14/06 BAL washings with HSV ,\n","9/14/06 , blood culture with coag-negative staph , 7/11/06 , blood\n","culture with coag-negative staph , 2/29/06 , urine culture with\n","yeast , 2/29/06 BAL washings with MRSA , 10/28/06 urine with yeast\n","urine with yeast , 10/28/06 , 6/23/06 , 4/17/06 , and 10/11/06\n","sputum with MRSA. 7/28/06 , chest CT with bilateral\n","opacification in the lung parenchyma. 2/29/06 facial CT with\n","left sphenoid maxillary thickening. 2/29/06 chest CT with\n","bilateral opacification in the lung parenchyma. Of note , a\n","2/29/06 abdominal CT showed no evidence of abdominal infection ,\n","2/29/06 TTE showed no obvious vegetations , and 10/11/06 tap of\n","the right knee grew no organisms. In light of the data above ,\n","the patient's indwelling catheters were changed. She underwent\n","treatment with linezolid x7 days for MRSA line infection. She\n","also underwent treatment with acyclovir x10 days for HSV\n","tracheobronchitis. The aforementioned antibiosis was mostly\n","prophylactic. It was thought that the patient's intermittent\n","fevers were not infectious , but rather reflected a drug allergy ,\n","most likely to vancomycin. This hypothesis was supported by a\n","robust eosinophilia coinciding with vancomycin administration.\n","Shortly after vancomycin discontinuation , the patient's fevers\n","resolved. The patient was afebrile for greater than 48 hours off\n","all antibiotics prior to transfer to rehabilitation.\n","Cardiovascular:\n","1. Volume status. The patient's admission weight was 106.2 kg.\n","It is unclear what her dry weight was. Given that pulmonary\n","edema was thought to be contributing to the patient's slow\n","ventilator wean , she was diuresed with a combination of Lasix and\n","Diuril followed by combinations of torsemide and Diuril. Her\n","discharge weight was 100.7 kilograms. Her diuretic regimen on\n","discharge with torsemide 100 mg IV t.i.d. and Diuril 500 mg IV\n","t.i.d. The patient's diuretic regimen will need be adjusted as\n","her intake is adjusted. Her creatinine will need to be monitored\n","very closely. Her weight will need to be checked daily.\n","2. Pump: The patient underwent echocardiogram on 11/19/06 ,\n","7/17/06 , and 2/29/06 .  On the whole , these studies revealed an\n","ejection fraction of 45-50% , concentric LVH , global hypokinesis\n","with regional variation , mild left atrial enlargement , mild\n","tricuspid regurgitation , a question of mild atrial stenosis , and\n","pulmonary artery pressures in the 40s. For her heart failure ,\n","the patient was treated with Lopressor and diuretics as above.\n","She was not started on an ACE inhibitor given her allergy\n","( cough ). She was started on low-dose ARB.\n","3. Ischemia: The patient has a history of coronary artery\n","disease status post three-vessel CABG and subsequent\n","single-vessel PPI. She has an RCA stenosis , 70-80% that has not\n","intervened upon. On admission , in the setting of respiratory\n","distress , the patient was in sinus tachycardia with rate related\n","to lateral ST depressions. Her cardiac biomarkers were positive\n","consistent with NSTEMI. It was thought that the patient\n","experienced demand ischemia rather than an acute plaque rupture.\n","On 10/8/06 , her troponin peaked at 8.53 , her CK at 275 , and her\n","MB of 16.3. The patient was treated with aspirin , Lopressor , and\n","Zocor. She was not started on an ACE inhibitor as detailed\n","above. She will likely warrant Cardiology followup with possible\n","RCA revascularization.\n","Neuro:\n","1. Sedation: While intubated , the patient was treated with IV\n","Versed and fentanyl titrated to light sedation. After her\n","tracheostomy , the patient's Versed and fentanyl drips were\n","discontinued. She was treated with Seroquel at bedtime to\n","preclude nighttime agitation. Her QTC should be monitored while\n","on Seroquel.\n","GI:\n","1. FEN: The patient initially received tube feeds via feeding\n","tube. She underwent a PEG placement on 4/17/06 .  She continued\n","on tube feeds. She also passed speech and Swallow and was thus\n","started on p.o. feeds with aspiration precautions. She also\n","received supplemental multivitamins , calcium carbonate , and\n","cholecalciferol. The patient will require speech and swallow\n","evaluation at rehabilitation. Now that she is awake , she may be\n","able to tolerate oral feeds with aspiration precautions.\n","2. Bowel regimen. The patient was treated with Colace , senna ,\n","and Dulcolax.\n","Heme:\n","1. Anemia: The patient has known iron deficiency anemia. She\n","was continued on iron and folate. She may benefit from an\n","outpatient colonoscopy if she has not had one recently.\n","2. Bleeding from tracheostomy site: On 4/24/06 , the patient\n","was noted to have bleeding from her tracheostomy site. She had\n","no hematocrit drop. She remained hemodynamically stable. She\n","underwent a bronchoscopy , which showed no active bleeding. Her\n","mild bleeding was thought to relate to suction trauma. Her\n","prophylactic heparin was held x1 day. It was thought that the\n","patient should be discharged on prophylactic heparin because her\n","DVT risk is so high. Should the patient have intense bleeding\n","from the tracheostomy site , a hematocrit drop or hemodynamic\n","changes. Her heparin subcu should be discontinued , her\n","hematocrit should be monitored closely , and she should be\n","transfused as needed. She should also at that point probably\n","undergo reevaluation by Pulmonary or Thoracics.\n","Endocrine:\n","1. Diabetes: The patient was treated with Lantus plus regular\n","insulin q. 6h. plus sliding scale insulin while she was on tube\n","feeds. Her insulin was changed on the night prior to discharge.\n","She was started on Lantus 100 subq b.i.d. She got her first dose\n","of 100 units subq on the evening prior to discharge , her morning\n","sugars were in the mid 100s. Her blood sugars should be followed\n","closely on the first one or two days at rehab. The blood sugars\n","should be monitored every 2-3 hours and her insulin should be\n","adjusted accordingly. Her insulin dose should be adjusted if her\n","tube feeds are cycled rather than given continuously or if she is\n","NPO.\n","Ortho:\n","1. Total knee replacement: The patient is status post right\n","total knee replacement on 8/19/06 .  Her right knee has a\n","well-healed incision and is not erythematous or tender. The\n","patient was initially on low-dose anticoagulation with Coumadin.\n","Her Coumadin was discontinued given her acute illness and her\n","need for procedures. She was treated with heparin subq\n","prophylactic doses 5000 units t.i.d. as described above. She\n","will need rehabilitation for her knee.\n","2. Prophylaxis: The patient was treated with subcutaneous\n","heparin and Nexium.\n","3. Access: The patient has peripheral IVs.\n","DISCHARGE STATUS:\n","On the day of discharge 3/8/06 , the patient was afebrile.\n","Heart rate was in the 80s , blood pressure was in the\n","100s-130s/60s-80s. Discharge weight was 100.7 kilograms. The\n","patient was drowsy , but arouseable. She was breathing on a trach\n","collar. She had decreased breath sounds at the bases. She had\n","an S1 and S2 with a 2/6 systolic murmur at the lower sternal\n","border. Her abdomen was soft and nontender with positive bowel\n","sounds. Her trach and PEG site were intact without any\n","surrounding erythema. Her extremities were warm without edema.\n","She was on trach collar with 50% FiO2 and oxygen saturation of\n","97%. Her discharge labs included a white blood cell count of 21 ,\n","stable , hematocrit of 28 , and platelets 436 , 000. Her chem-7\n","included a sodium of 131 , potassium 3 , chloride 87 , CO2 32 , BUN\n","67 , creatinine 0.8 , and glucose 154. Her LFTs were normal. Her\n","INR was 1.1. Her most recent microdata showed sputum with few\n","polys and a few gram-positive cocci in clusters. Her chest x-ray\n","showed a trach in place and bilateral hazy infiltrate consistent\n","with resolving ARDS.\n","An addendum will be given with discharge medications and doses.\n","At rehab , the patient's weight should be monitored daily. Her\n","blood sugars initially be monitored every 2-3 hours. After that ,\n","her blood sugar monitoring can be spaced out to t.i.d. or\n","whenever deemed appropriate by the physician at the\n","rehabilitation facility. Her hematocrit should be checked\n","particularly if she has any bleeding from her tracheostomy site;\n","otherwise , the hematocrit can be checked every day. The white\n","blood cell count should be monitored if the patient has any\n","fever. The creatinine should be monitored daily in light of the\n","patient's changing diuretic regimen. The patient in's and out's\n","should be monitored closely.\n","CONTACTS AT THE HOSPITAL:\n","Liri Hospital Of regarding the patient's inpatient course or\n","Dr. Julio Golding . Contacts regarding follow up issues , the\n","patient's primary care physician , Dr. Adrian Mckinley Rickenbaker at Mawahutche Rehabilitation Hospital Of 080-524-3286. The patient's healthcare proxy is her\n","sister , Kyle Lusk who is a registered nurse , her telephone\n","number 507-122-9398 at home and 581-981-9887 cell phone.\n","eScription document: 9-3627813 EMSSten Tel\n","CC: Geraldo Lanny Bogda MD\n","Los Di Wi\n","CC: Raleigh Semmens MD\n","Dictated By: DELIBERTIS , BRADLY\n","Attending: BEAGLEY , ORLANDO\n","Dictation ID 4434301\n","D: 3/8/06\n","T: 3/8/06\n"]}],"source":["test_df = load_dataset(DATA_PATH + 'obesity_patient_records_test.xml', xpath='/root/docs/*')\n","test_df['id'] = pd.to_numeric(test_df['id'])\n","print(test_df.head())\n","print(len(test_df))\n","\n","train_df = load_dataset(DATA_PATH + 'obesity_patient_records_training.xml', xpath='/root/docs/*')\n","train_df_with2 = train_df.append(load_dataset(DATA_PATH + '/obesity_patient_records_training2.xml', xpath='/root/docs/*'))\n","train_df['id'] = pd.to_numeric(train_df['id'])\n","train_df_with2['id'] = pd.to_numeric(train_df_with2['id'])\n","print(train_df.head())\n","print(len(train_df))\n","print(len(train_df_with2))\n","\n","print(test_df['text'][0])"]},{"cell_type":"markdown","metadata":{"id":"ACBjTAgsOmgT"},"source":["The annotation data came in two forms: textual and intuitive.  It also came with files with the forms in seperate files and with the forms all together in one file.  We do some exploration to determine which set of data is the closest to the study."]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27805,"status":"ok","timestamp":1679769755372,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"Mlf84b2lhh9x","outputId":"cc2b826a-e08a-43a6-9269-9ee9549d590c"},"outputs":[{"name":"stdout","output_type":"stream","text":["      source disease  id judgment\n","0  intuitive  Asthma   3        Y\n","0  intuitive  Asthma   5        N\n","0  intuitive  Asthma   7        N\n","0  intuitive  Asthma   9        Y\n","0  intuitive  Asthma  10        N\n","7399\n","    source disease  id judgment\n","0  textual  Asthma   3        Y\n","0  textual  Asthma   5        U\n","0  textual  Asthma   7        U\n","0  textual  Asthma   8        U\n","0  textual  Asthma   9        Y\n","8044\n"]}],"source":["test_annot_intuitive_df = load_annotations(DATA_PATH + \"obesity_standoff_annotations_test_intuitive.xml\")\n","test_annot_intuitive_df['id'] = pd.to_numeric(test_annot_intuitive_df['id'])\n","\n","test_annot_textual_df = load_annotations(DATA_PATH + \"obesity_standoff_annotations_test_textual.xml\")\n","test_annot_textual_df['id'] = pd.to_numeric(test_annot_textual_df['id'])\n","\n","print(test_annot_intuitive_df.head())\n","print(len(test_annot_intuitive_df))\n","\n","print(test_annot_textual_df.head())\n","print(len(test_annot_textual_df))\n"]},{"cell_type":"markdown","metadata":{"id":"vlDvN-LnPLwl"},"source":["The test file with all forms is explored and the record count seems the same as combining the seperate files."]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11952,"status":"ok","timestamp":1679769767321,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"u8Y_KX0h1Jsn","outputId":"c37da12d-1e5e-4ca9-ad59-f90251686ecd"},"outputs":[{"name":"stdout","output_type":"stream","text":["      source disease  id judgment\n","0  intuitive  Asthma   3        Y\n","0  intuitive  Asthma   5        N\n","0  intuitive  Asthma   7        N\n","0  intuitive  Asthma   9        Y\n","0  intuitive  Asthma  10        N\n","15443\n"]}],"source":["#trying to verify the same number of records in the one with both intuitive and textual\n","test_annot_all_df = load_annotations(DATA_PATH + \"obesity_standoff_annotations_test.xml\")\n","test_annot_all_df['id'] = pd.to_numeric(test_annot_all_df['id'])\n","\n","print(test_annot_all_df.head())\n","print(len(test_annot_all_df))"]},{"cell_type":"markdown","metadata":{"id":"CR_EMmuoPSf0"},"source":["We then do the same analysis with the training annotations."]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29125,"status":"ok","timestamp":1679769796436,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"1nUpoSeuxMDG","outputId":"0ed999c6-111d-4aa8-b988-2e1bf6f6e468"},"outputs":[{"name":"stdout","output_type":"stream","text":["      source disease  id judgment\n","0  intuitive  Asthma   1        N\n","0  intuitive  Asthma   2        Y\n","0  intuitive  Asthma   4        N\n","0  intuitive  Asthma   6        N\n","0  intuitive  Asthma  15        N\n","8621\n","    source disease  id judgment\n","0  textual  Asthma   1        U\n","0  textual  Asthma   2        Y\n","0  textual  Asthma   4        U\n","0  textual  Asthma   6        U\n","0  textual  Asthma  13        U\n","9655\n"]}],"source":["train_annot_intuitive_df = load_annotations(DATA_PATH + \"obesity_standoff_intuitive_annotations_training.xml\")\n","train_annot_intuitive_df['id'] = pd.to_numeric(train_annot_intuitive_df['id'])\n","train_annot_textual_df = load_annotations(DATA_PATH + \"obesity_standoff_textual_annotations_training.xml\")\n","train_annot_textual_df['id'] = pd.to_numeric(train_annot_textual_df['id'])\n","\n","print(train_annot_intuitive_df.head())\n","print(len(train_annot_intuitive_df))\n","\n","print(train_annot_textual_df.head())\n","print(len(train_annot_textual_df))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QM3B1QR-PWsq"},"source":["When we look at the full file with addendums, we see there is a lot more data in the full file than the seperate file."]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18423,"status":"ok","timestamp":1679769814849,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"Qrh6CrnzC1P8","outputId":"bda08ef8-49ef-4b81-ccfa-bb9fb8d8ead2"},"outputs":[{"name":"stdout","output_type":"stream","text":["      source disease  id judgment\n","0  intuitive  Asthma   1        N\n","0  intuitive  Asthma   2        Y\n","0  intuitive  Asthma   4        N\n","0  intuitive  Asthma   6        N\n","0  intuitive  Asthma  15        N\n","18276\n","22285\n"]}],"source":["#trying to verify the same number of records in the one with both intuitive and textual (It isn't according to tally.pdf it should be 22285 with the annotations and addendums)\n","train_annot_all_df = load_annotations(DATA_PATH + \"obesity_standoff_annotations_training.xml\")\n","train_annot_all_df_with2 = pd.concat([train_annot_all_df,load_annotations(DATA_PATH + \"obesity_standoff_annotations_training_addendum.xml\")])\n","train_annot_all_df_with2 = pd.concat([train_annot_all_df_with2,load_annotations(DATA_PATH + \"obesity_standoff_annotations_training_addendum2.xml\")])\n","train_annot_all_df_with2 = pd.concat([train_annot_all_df_with2,load_annotations(DATA_PATH + \"obesity_standoff_annotations_training_addendum3.xml\")])\n","\n","train_annot_all_df['id'] = pd.to_numeric(train_annot_all_df['id'])\n","train_annot_all_df_with2['id'] = pd.to_numeric(train_annot_all_df_with2['id'])\n","\n","print(train_annot_all_df.head())\n","print(len(train_annot_all_df))\n","print(len(train_annot_all_df_with2))\n"]},{"cell_type":"markdown","metadata":{"id":"TFn3q_Fom9T6"},"source":["We are going to start with the annotations in one file (test_annot_all_df, train_annot_all_df).  The paper only used annotations that were clearly marked 'Y' or 'N' (It excluded the 'Q' and 'U')."]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1679769814849,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"QfHtYhyrm8zP","outputId":"459aaf55-69d9-4eae-e8d0-0627249da20a"},"outputs":[{"name":"stdout","output_type":"stream","text":["15443 18276\n","9642 11274\n"]}],"source":["print(len(test_annot_all_df),len(train_annot_all_df))\n","test_annot_all_df_clean = test_annot_all_df[(test_annot_all_df['judgment']  == 'Y') | (test_annot_all_df['judgment']  == 'N')]\n","train_annot_all_df_clean = train_annot_all_df[(train_annot_all_df['judgment']  == 'Y') | (train_annot_all_df['judgment']  == 'N')]\n","print(len(test_annot_all_df_clean),len(train_annot_all_df_clean))\n"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1679769814849,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"PHKozf15u2Km","outputId":"b5719f17-543c-4fbd-f325-cda401b40043"},"outputs":[{"name":"stdout","output_type":"stream","text":["disease\n","Asthma                  541\n","CAD                     756\n","CHF                     650\n","Depression              549\n","Diabetes                829\n","GERD                    494\n","Gallstones              580\n","Gout                    552\n","Hypercholesterolemia    650\n","Hypertension            826\n","Hypertriglyceridemia    496\n","OA                      544\n","OSA                     562\n","Obesity                 648\n","PVD                     528\n","Venous Insufficiency    437\n","dtype: int64\n"]}],"source":["print(test_annot_all_df_clean.groupby('disease').size())\n"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1679769814850,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"WUVhK8bkvbXV","outputId":"5d03131a-d4af-49a4-f452-9e62c6cfd92b"},"outputs":[{"name":"stdout","output_type":"stream","text":["disease\n","Asthma                  648\n","CAD                     897\n","CHF                     489\n","Depression              668\n","Diabetes                978\n","GERD                    586\n","Gallstones              687\n","Gout                    667\n","Hypercholesterolemia    757\n","Hypertension            983\n","Hypertriglyceridemia    602\n","OA                      654\n","OSA                     678\n","Obesity                 801\n","PVD                     639\n","Venous Insufficiency    540\n","dtype: int64\n"]}],"source":["\n","print(train_annot_all_df_clean.groupby('disease').size())\n"]},{"cell_type":"markdown","metadata":{"id":"mfezONE45GEd"},"source":["The paper specifically calls out 6 files and does not mention the addendums, so we will stick with the seperately labeled files for our study. There seems to be only one record in each of the test and training set where the textual and intuitive disagree."]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1679769814850,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"EkckqW7G0Fqh","outputId":"5dca85d7-49c8-46a3-fed8-031958afe016"},"outputs":[{"name":"stdout","output_type":"stream","text":["7385\n","2257\n","       source_x disease  id judgment_x source_y judgment_y\n","1712  intuitive      OA   8          N  textual          Y\n","8598\n","2676\n","      source_x disease    id judgment_x source_y judgment_y\n","571  intuitive     CHF  1072          Y  textual          N\n"]}],"source":["test_annot_intuitive_df_clean = test_annot_intuitive_df[(test_annot_intuitive_df['judgment']  == 'Y') | (test_annot_intuitive_df['judgment']  == 'N')]\n","test_annot_textual_df_clean = test_annot_textual_df[(test_annot_textual_df['judgment']  == 'Y') | (test_annot_textual_df['judgment']  == 'N')]\n","train_annot_intuitive_df_clean = train_annot_intuitive_df[(train_annot_intuitive_df['judgment']  == 'Y') | (train_annot_intuitive_df['judgment']  == 'N')]\n","train_annot_textual_df_clean = train_annot_textual_df[(train_annot_textual_df['judgment']  == 'Y') | (train_annot_textual_df['judgment']  == 'N')]\n","\n","print(len(test_annot_intuitive_df_clean))\n","print(len(test_annot_textual_df_clean))\n","\n","\n","df = test_annot_intuitive_df_clean.merge(test_annot_textual_df_clean, on=['id','disease'])\n","print(df[df['judgment_x'] != df['judgment_y']])\n","\n","print(len(train_annot_intuitive_df_clean))\n","print(len(train_annot_textual_df_clean))\n","\n","df = train_annot_intuitive_df_clean.merge(train_annot_textual_df_clean, on=['id','disease'])\n","print(df[df['judgment_x'] != df['judgment_y']])\n"]},{"cell_type":"markdown","metadata":{"id":"2Z9joC8e-aFl"},"source":["Let's remove those two records from the textual table and recheck."]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1679769814851,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"YfspC1q0-lUU","outputId":"301372e0-0415-4a60-b5fd-91b5d8fd8490"},"outputs":[{"name":"stdout","output_type":"stream","text":["2256\n","Empty DataFrame\n","Columns: [source_x, disease, id, judgment_x, index, source_y, judgment_y]\n","Index: []\n","2675\n","Empty DataFrame\n","Columns: [source_x, disease, id, judgment_x, index, source_y, judgment_y]\n","Index: []\n"]}],"source":["df = test_annot_textual_df_clean\n","df = df.reset_index()\n","index_names = df[(df['disease'] == 'OA') & (df['id'] == 8)].index\n","test_annot_textual_df_clean = df.drop(index_names)\n","\n","df = train_annot_textual_df_clean\n","df = df.reset_index()\n","index_names = df[(df['disease'] == 'CHF') & (df['id'] == 1072)].index\n","train_annot_textual_df_clean = df.drop(index_names)\n","\n","print(len(test_annot_textual_df_clean))\n","df = test_annot_intuitive_df_clean.merge(test_annot_textual_df_clean, on=['id','disease'])\n","print(df[df['judgment_x'] != df['judgment_y']])\n","\n","print(len(train_annot_textual_df_clean))\n","df = train_annot_intuitive_df_clean.merge(train_annot_textual_df_clean, on=['id','disease'])\n","print(df[df['judgment_x'] != df['judgment_y']])\n"]},{"cell_type":"markdown","metadata":{"id":"rDoDyp5u9uSA"},"source":["The paper does a classification model for each disase seperately.  Need to be able to loop through each. Using the seperate files  seems to come closer to the disease counts the paper discusses before pre-processing, so we will use this data. The study must have done some additional processing that is not evident from the paper, so our results may be a little different."]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1679769815355,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"l0WT-GUr5o7o","outputId":"1eee01bd-aa4b-4f0b-cda3-63ede6c3a392"},"outputs":[{"name":"stdout","output_type":"stream","text":["Disease: Asthma\n","Train: 572 76 648\n","Test: 471 70 541\n","All: 1189\n","Disease: CAD\n","Train: 548 349 897\n","Test: 457 299 756\n","All: 1653\n","Disease: CHF\n","Train: 243 245 488\n","Test: 434 216 650\n","All: 1138\n","Disease: Depression\n","Train: 582 86 668\n","Test: 477 72 549\n","All: 1217\n","Disease: Diabetes\n","Train: 567 411 978\n","Test: 479 350 829\n","All: 1807\n","Disease: Gallstones\n","Train: 593 94 687\n","Test: 491 89 580\n","All: 1267\n","Disease: GERD\n","Train: 487 99 586\n","Test: 424 70 494\n","All: 1080\n","Disease: Gout\n","Train: 596 71 667\n","Test: 500 52 552\n","All: 1219\n","Disease: Hypercholesterolemia\n","Train: 502 255 757\n","Test: 431 219 650\n","All: 1407\n","Disease: Hypertension\n","Train: 531 452 983\n","Test: 446 380 826\n","All: 1809\n","Disease: Hypertriglyceridemia\n","Train: 587 15 602\n","Test: 486 10 496\n","All: 1098\n","Disease: OA\n","Train: 565 89 654\n","Test: 458 85 543\n","All: 1197\n","Disease: Obesity\n","Train: 553 248 801\n","Test: 447 201 648\n","All: 1449\n","Disease: OSA\n","Train: 590 88 678\n","Test: 493 69 562\n","All: 1240\n","Disease: PVD\n","Train: 556 83 639\n","Test: 464 64 528\n","All: 1167\n","Disease: Venous Insufficiency\n","Train: 526 14 540\n","Test: 427 10 437\n","All: 977\n","Samples: 20914\n"]}],"source":["disease_list = train_annot_intuitive_df_clean['disease'].unique().tolist()\n","\n","train_annot_all_df_clean = pd.concat([train_annot_intuitive_df_clean,train_annot_textual_df_clean])\n","train_annot_all_df_clean = train_annot_all_df_clean.drop(['source'], axis=1)\n","train_annot_all_df_clean = train_annot_all_df_clean.drop_duplicates()\n","\n","test_annot_all_df_clean = pd.concat([test_annot_intuitive_df_clean,test_annot_textual_df_clean])\n","test_annot_all_df_clean = test_annot_all_df_clean.drop(['source'], axis=1)\n","test_annot_all_df_clean = test_annot_all_df_clean.drop_duplicates()\n","\n","annot_all_df_clean = pd.concat([train_annot_all_df_clean,test_annot_all_df_clean])\n","annot_all_df_clean = annot_all_df_clean.drop_duplicates()\n","samples = 0\n","\n","for disease in disease_list:\n","  print('Disease:',disease)\n","  print('Train:',sum(train_annot_intuitive_df_clean['disease'] == disease),\n","        sum(train_annot_textual_df_clean['disease'] == disease),\n","        sum(train_annot_all_df_clean['disease'] == disease))\n","  print('Test:',sum(test_annot_intuitive_df_clean['disease'] == disease),\n","        sum(test_annot_textual_df_clean['disease'] == disease),\n","        sum(test_annot_all_df_clean['disease'] == disease))  \n","  print('All:',sum(annot_all_df_clean['disease'] == disease))\n","  samples = sum(annot_all_df_clean['disease'] == disease) + samples\n","\n","print(\"Samples:\",samples)\n"]},{"cell_type":"markdown","metadata":{"id":"_VmV5x9WQfZ3"},"source":["Datasets to use for rest of the study:\n","* test_df [id,text] (document, clinical notes)\n","* train_df [id,text] (document, clinical notes)\n","* test_annot_all_df_clean [disease,id,judment,index] (disease, document, judgment)\n","* train_annot_all_df_clean [disease,id,judment,index] (disease, document, judgment)\n","\n","For the annotations, we should convert judgement to a numeric label.\n","\n","Our next step is to continue the preprocessing of the data.  We want to do this seperately from the annotations, they can be joined when doing classification by each disase. This includes:\n","\n","* Lower-casing of the text\n","* Removing punctuation and numeric values from the text\n","* Tokenization of text \n","* Lemmatizattion of the tokens\n","* TF-IDF matrix generation (TF-IDF Vectorizer4 from the scikit-learn library)\n","\n","We have an optional parameter to remove stop words as the paper discusses the fact that stop words should be included for deep learning models."]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125,"status":"ok","timestamp":1679769978772,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"Oqv8oDCxfRAL","outputId":"7a4cd7ff-8a1a-4bd3-d396-cf07b49a607d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>disease</th>\n","      <th>id</th>\n","      <th>judgment</th>\n","      <th>index</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Asthma</td>\n","      <td>1</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Asthma</td>\n","      <td>2</td>\n","      <td>Y</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Asthma</td>\n","      <td>4</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Asthma</td>\n","      <td>6</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Asthma</td>\n","      <td>15</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2671</th>\n","      <td>Venous Insufficiency</td>\n","      <td>879</td>\n","      <td>Y</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2672</th>\n","      <td>Venous Insufficiency</td>\n","      <td>989</td>\n","      <td>Y</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2673</th>\n","      <td>Venous Insufficiency</td>\n","      <td>1055</td>\n","      <td>Y</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2674</th>\n","      <td>Venous Insufficiency</td>\n","      <td>1149</td>\n","      <td>Y</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2675</th>\n","      <td>Venous Insufficiency</td>\n","      <td>1216</td>\n","      <td>Y</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11273 rows × 4 columns</p>\n","</div>"],"text/plain":["                   disease    id judgment  index\n","0                   Asthma     1        N    NaN\n","0                   Asthma     2        Y    NaN\n","0                   Asthma     4        N    NaN\n","0                   Asthma     6        N    NaN\n","0                   Asthma    15        N    NaN\n","...                    ...   ...      ...    ...\n","2671  Venous Insufficiency   879        Y    0.0\n","2672  Venous Insufficiency   989        Y    0.0\n","2673  Venous Insufficiency  1055        Y    0.0\n","2674  Venous Insufficiency  1149        Y    0.0\n","2675  Venous Insufficiency  1216        Y    0.0\n","\n","[11273 rows x 4 columns]"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["\n","test_df\n","train_df\n","test_annot_all_df_clean\n","train_annot_all_df_clean"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":283,"status":"ok","timestamp":1679770827175,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"zkGLF2yPfdnJ","outputId":"c37191e4-cae7-4573-f6f0-8328570c055b"},"outputs":[],"source":["from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","import re\n","import string\n","import nltk\n","#nltk.download('wordnet')"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":106,"status":"ok","timestamp":1679771081128,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"q7KPCWF4B6hE"},"outputs":[],"source":["#wn = WordNetLemmatizer()\n","\n","#def black_txt(token):\n","  #return token not in list(string.punctuation) and len(token) > 2\n","\n","#def clean_txt(text):\n","  #clean_text = []\n","  \n","  #text = re.sub(re.escape(\"'\"), \"\", text)\n","  #text = re.sub(re.escape(\"\\\\d|\\\\W)+\"), \" \", text)\n","  #clean_text = [wn.lemmatize(word, pos = \"v\") for word in word_tokenize(text.lower()) if black_txt(word)]\n","\n","  #return \" \".join(clean_text)\n"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4057,"status":"ok","timestamp":1679771153404,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"Gw14Xn7zDL_e","outputId":"ea3aafa5-c429-4456-aeeb-1ddd58c42f52"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\chkabat\\AppData\\Local\\Temp/ipykernel_25716/3043505989.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n","  train_df[\"no_punc_text\"] = train_df['text'].str.replace('[^\\w\\s]', '')\n","C:\\Users\\chkabat\\AppData\\Local\\Temp/ipykernel_25716/3043505989.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n","  train_df[\"no_numerics_text\"] = train_df['no_punc_text'].str.replace('\\d+', '')\n"]}],"source":["#train_df['Clean_Description'] = train_df['text'].map(str).apply(clean_txt)\n","wn = WordNetLemmatizer()\n","\n","train_df[\"no_punc_text\"] = train_df['text'].str.replace('[^\\w\\s]', '')\n","train_df[\"no_numerics_text\"] = train_df['no_punc_text'].str.replace('\\d+', '')\n","train_df[\"lower_text\"] = train_df['no_numerics_text'].apply(str.lower)\n","\n","train_df[\"tokenized_text\"] = train_df.apply(lambda row: word_tokenize(row['lower_text']), axis=1)\n","\n","train_df[\"tok_lem_text\"] = train_df['tokenized_text'].apply(\n","    lambda lst:[wn.lemmatize(word) for word in lst])"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1679771153404,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"QQbaVExOGyoP","outputId":"763849da-e152-4b63-9625-607f50c63981"},"outputs":[{"name":"stdout","output_type":"stream","text":["['wmc', 'am', 'anemia', 'signed', 'dis', 'admission', 'date', 'report', 'status', 'signed', 'discharge', 'date', 'attending', 'truka', 'deon', 'xavier', 'md', 'service', 'bh', 'principal', 'diagnosis', 'anemia', 'and', 'gi', 'bleed', 'secondary', 'diagnosis', 'diabetes', 'mitral', 'valve', 'replacement', 'atrial', 'fibrillation', 'and', 'chronic', 'kidney', 'disease', 'history', 'of', 'present', 'illness', 'the', 'patient', 'is', 'an', 'yearold', 'woman', 'with', 'a', 'history', 'of', 'diabetes', 'chronic', 'kidney', 'disease', 'congestive', 'heart', 'failure', 'with', 'ejection', 'fraction', 'of', 'to', 'who', 'present', 'from', 'clinic', 'with', 'a', 'chief', 'complaint', 'of', 'fatigue', 'and', 'weakness', 'for', 'one', 'week', 'she', 'had', 'had', 'worsening', 'right', 'groin', 'and', 'hip', 'pain', 'status', 'post', 'a', 'total', 'hip', 'replacement', 'approximately', 'year', 'ago', 'which', 'had', 'been', 'worsening', 'for', 'two', 'week', 'and', 'she', 'ha', 'also', 'recently', 'completed', 'a', 'course', 'of', 'levaquin', 'for', 'urinary', 'tract', 'infection', 'she', 'presented', 'to', 'dr', 'parrent', 'office', 'complaining', 'of', 'fatigue', 'and', 'weakness', 'for', 'one', 'week', 'she', 'ha', 'had', 'some', 'abdominal', 'pain', 'in', 'a', 'bandlike', 'distribution', 'around', 'her', 'right', 'side', 'she', 'wa', 'found', 'to', 'have', 'a', 'hematocrit', 'of', 'down', 'from', 'eight', 'day', 'ago', 'and', 'wa', 'sent', 'to', 'the', 'emergency', 'department', 'for', 'transfusion', 'and', 'workup', 'of', 'her', 'anemia', 'preadmission', 'medication', 'caltrate', 'plus', 'd', 'one', 'tab', 'po', 'bid', 'lantus', 'unit', 'sc', 'qpm', 'novolog', 'unit', 'unit', 'unit', 'sc', 'tid', 'imdur', 'mg', 'bid', 'amlodipine', 'mg', 'bid', 'furosemide', 'mg', 'daily', 'valsartan', 'mg', 'daily', 'warfarin', 'mg', 'daily', 'iron', 'sulfate', 'mg', 'po', 'daily', 'and', 'multivitamin', 'daily', 'past', 'medical', 'history', 'chronic', 'kidney', 'disease', 'presumed', 'due', 'to', 'congestive', 'heart', 'failurediuresisrenal', 'artery', 'diseaseearly', 'diabetic', 'nephropathy', 'type', 'diabetes', 'previous', 'stroke', 'congestive', 'heart', 'failure', 'with', 'ejection', 'fraction', 'of', 'to', 'rheumatic', 'valvular', 'disease', 'with', 'mitral', 'valve', 'replacement', 'and', 'tricuspid', 'valve', 'repair', 'atrial', 'fibrillation', 'history', 'of', 'small', 'bowel', 'obstruction', 'status', 'post', 'right', 'total', 'hip', 'replacement', 'approximately', 'year', 'ago', 'family', 'history', 'no', 'family', 'history', 'of', 'kidney', 'disease', 'or', 'heart', 'disease', 'social', 'history', 'she', 'ha', 'child', 'life', 'alone', 'with', 'home', 'care', 'in', 'me', 'but', 'ha', 'moved', 'in', 'to', 'live', 'with', 'her', 'daughter', 'in', 'news', 'irv', 'in', 'she', 'denies', 'tobacco', 'use', 'and', 'drink', 'alcohol', 'rarely', 'allergy', 'codeine', 'and', 'benadryl', 'admission', 'physical', 'examination', 'vital', 'sign', 'were', 'temperature', 'heart', 'rate', 'blood', 'pressure', 'respiration', 'and', 'sao', 'on', 'room', 'air', 'the', 'patient', 'is', 'a', 'frail', 'elderly', 'woman', 'in', 'no', 'acute', 'distress', 'she', 'ha', 'poor', 'dentition', 'jvp', 'is', 'difficult', 'to', 'ass', 'secondary', 'to', 'tricuspid', 'regurgitation', 'lung', 'were', 'clear', 'to', 'auscultation', 'bilaterally', 'cardiovascular', 'exam', 'showed', 'bradycardia', 'with', 'heart', 'rate', 'in', 'the', 's', 'that', 'wa', 'irregular', 's', 'plus', 's', 'with', 'systolic', 'murmur', 'heard', 'throughout', 'with', 'mechanical', 'sounding', 's', 'abdomen', 'wa', 'mildly', 'tender', 'to', 'palpation', 'in', 'the', 'mid', 'epigastrium', 'with', 'no', 'rebound', 'or', 'guarding', 'extremity', 'showed', 'venous', 'stasis', 'change', 'in', 'her', 'lower', 'extremity', 'bilaterally', 'foot', 'were', 'cool', 'with', 'diminished', 'dp', 'and', 'pt', 'pulse', 'on', 'neurological', 'exam', 'she', 'wa', 'alert', 'and', 'oriented', 'x', 'and', 'cranial', 'nerve', 'ii', 'through', 'xii', 'were', 'intact', 'study', 'ekg', 'showed', 'atrial', 'fibrillation', 'with', 'slow', 'ventricular', 'response', 'with', 'heart', 'rate', 'of', 'widened', 'qrs', 'a', 'q', 'wave', 'in', 'avl', 'and', 'u', 'wave', 'in', 'the', 'lateral', 'lead', 'chest', 'xray', 'showed', 'improved', 'pleural', 'effusion', 'and', 'pulmonary', 'edema', 'stable', 'marked', 'cardiomegaly', 'xray', 'of', 'the', 'right', 'hip', 'showed', 'status', 'post', 'right', 'total', 'hip', 'arthroplasty', 'with', 'stable', 'periprosthetic', 'lucency', 'and', 'cortical', 'remodeling', 'severe', 'left', 'hip', 'osteoarthritis', 'diffuse', 'atherosclerosis', 'egd', 'on', 'showed', 'hiatal', 'hernia', 'fundic', 'polyp', 'with', 'pathology', 'showing', 'hypoplastic', 'and', 'inflammatory', 'lesion', 'mild', 'antral', 'erosion', 'with', 'pathology', 'demonstrating', 'mild', 'regeneration', 'that', 'wa', 'nonspecific', 'and', 'no', 'h', 'pylorus', 'and', 'duodenitis', 'with', 'pathology', 'showing', 'normal', 'mucosa', 'but', 'no', 'active', 'bleeding', 'or', 'sign', 'of', 'recent', 'bleeding', 'capsule', 'endoscopy', 'on', 'showed', 'a', 'healing', 'gastric', 'ulcer', 'likely', 'in', 'the', 'antrum', 'small', 'bowel', 'lymphangiectasia', 'and', 'angioectasia', 'in', 'the', 'distal', 'small', 'bowel', 'colonoscopy', 'on', 'demonstrated', 'cecal', 'diverticulum', 'approximately', 'mm', 'ascending', 'sessile', 'polyp', 'and', 'several', 'small', 'sessile', 'polyp', 'in', 'the', 'rectosigmoid', 'procedure', 'right', 'basilic', 'vein', 'transposition', 'on', 'by', 'dr', 'jacinto', 'goonez', 'hospital', 'course', 'by', 'problem', 'gi', 'bleed', 'in', 'the', 'emergency', 'department', 'the', 'patient', 'vital', 'sign', 'were', 'temperature', 'heart', 'rate', 'respiration', 'blood', 'pressure', 'with', 'sao', 'on', 'room', 'air', 'she', 'wa', 'found', 'to', 'have', 'black', 'guaiac', 'positive', 'stool', 'and', 'gi', 'wa', 'consulted', 'she', 'wa', 'started', 'on', 'iv', 'nexium', 'mg', 'bid', 'and', 'wa', 'given', 'vitamin', 'k', 'mg', 'subcutaneously', 'and', 'two', 'unit', 'of', 'ffp', 'and', 'she', 'wa', 'transfused', 'three', 'unit', 'of', 'packed', 'red', 'blood', 'cell', 'with', 'lasix', 'mg', 'iv', 'for', 'each', 'bag', 'of', 'note', 'the', 'patient', 'had', 'a', 'colonoscopy', 'in', 'wa', 'on', 'which', 'showed', 'bleeding', 'rectal', 'ulcer', 'with', 'biopsy', 'consistent', 'with', 'ischemic', 'colitis', 'an', 'egd', 'on', 'showed', 'a', 'hiatal', 'hernia', 'fundic', 'polyp', 'path', 'hypoplasticinflammatory', 'mild', 'antral', 'erosion', 'path', 'mild', 'regeneration', 'nonspecific', 'no', 'h', 'pylorus', 'duodenitis', 'path', 'normal', 'mucosa', 'but', 'no', 'active', 'bleeding', 'or', 'sign', 'of', 'recent', 'bleeding', 'capsule', 'endoscopy', 'on', 'showed', 'healing', 'gastric', 'ulcer', 'likely', 'in', 'the', 'antrum', 'small', 'bowel', 'lymphangiectasia', 'and', 'angioectasia', 'in', 'the', 'distal', 'small', 'bowel', 'which', 'were', 'considered', 'the', 'likely', 'source', 'of', 'bleeding', 'colonoscopy', 'wa', 'performed', 'on', 'to', 'search', 'for', 'angioectasias', 'for', 'which', 'intervention', 'would', 'be', 'possible', 'but', 'demonstrated', 'only', 'a', 'cecal', 'diverticulum', 'and', 'approximately', 'mm', 'ascending', 'sessile', 'polyp', 'and', 'several', 'small', 'sessile', 'polyp', 'in', 'the', 'rectosigmoid', 'the', 'patient', 'wa', 'started', 'on', 'aranesp', 'but', 'had', 'a', 'point', 'hematocrit', 'drop', 'from', 'to', 'on', 'for', 'which', 'she', 'required', 'another', 'two', 'unit', 'of', 'blood', 'along', 'with', 'lasix', 'the', 'patient', 'hematocrit', 'remained', 'stable', 'at', 'approximately', 'to', 'and', 'she', 'will', 'be', 'restarted', 'on', 'anticoagulation', 'on', 'thursday', 'one', 'week', 'after', 'her', 'av', 'fistula', 'surgery', 'renal', 'the', 'patient', 'ha', 'chronic', 'kidney', 'disease', 'and', 'is', 'being', 'considered', 'for', 'a', 'possible', 'hemodialysis', 'in', 'the', 'future', 'she', 'wa', 'continued', 'on', 'her', 'caltrate', 'plus', 'd', 'multivitamin', 'and', 'iron', 'supplementation', 'and', 'she', 'wa', 'started', 'on', 'aranesp', 'mcg', 'weekly', 'and', 'wa', 'given', 'sevelamer', 'mg', 'qac', 'for', 'elevated', 'phosphate', 'level', 'vein', 'mapping', 'study', 'wa', 'performed', 'on', 'and', 'a', 'right', 'basilic', 'vein', 'transposition', 'wa', 'performed', 'on', 'by', 'dr', 'landrie', 'the', 'patient', 'had', 'postoperative', 'right', 'hand', 'coolness', 'numbness', 'and', 'weakness', 'always', 'with', 'dopplerable', 'radial', 'pulse', 'from', 'steal', 'versus', 'neurapraxia', 'which', 'wa', 'improved', 'by', 'the', 'time', 'of', 'discharge', 'she', 'will', 'follow', 'up', 'with', 'dr', 'chanthasene', 'a', 'an', 'outpatient', 'the', 'patient', 'creatinine', 'wa', 'on', 'admission', 'improved', 'to', 'by', 'which', 'wa', 'the', 'day', 'of', 'her', 'surgery', 'and', 'increased', 'again', 'to', 'on', 'she', 'wa', 'given', 'iv', 'lasix', 'bolus', 'a', 'she', 'had', 'evidence', 'of', 'volume', 'overload', 'with', 'over', 'eightpound', 'weight', 'gain', 'during', 'her', 'hospitalization', 'and', 'creatinine', 'improved', 'to', 'by', 'the', 'day', 'of', 'discharge', 'cardiovascular', 'pump', 'the', 'patient', 'ejection', 'fraction', 'is', 'to', 'and', 'she', 'wa', 'given', 'mg', 'of', 'iv', 'lasix', 'for', 'each', 'unit', 'of', 'packed', 'red', 'blood', 'cell', 'she', 'received', 'along', 'with', 'mg', 'po', 'daily', 'with', 'iv', 'bolus', 'of', 'mg', 'a', 'needed', 'for', 'volume', 'overload', 'a', 'judged', 'by', 'an', 'increase', 'in', 'her', 'weight', 'antihypertensive', 'medication', 'were', 'originally', 'held', 'for', 'her', 'gi', 'bleed', 'and', 'they', 'were', 'restarted', 'on', 'with', 'systolic', 'blood', 'pressure', 'remaining', 'in', 'the', 's', 'to', 's', 'rhythm', 'the', 'patient', 'ha', 'atrial', 'fibrillation', 'with', 'slow', 'ventricular', 'response', 'with', 'heart', 'rate', 'a', 'low', 'a', 'the', 'upper', 's', 'occasionally', 'her', 'heart', 'rate', 'appeared', 'regular', 'and', 'wa', 'thought', 'to', 'be', 'junctional', 'escape', 'rhythm', 'she', 'wa', 'asymptomatic', 'throughout', 'her', 'hospitalization', 'she', 'wa', 'discussed', 'with', 'her', 'cardiologist', 'dr', 'fritz', 'who', 'will', 'consider', 'a', 'pacemaker', 'a', 'an', 'outpatient', 'endocrine', 'she', 'is', 'euthyroid', 'with', 'tsh', 'of', 'for', 'her', 'diabetes', 'she', 'received', 'nightly', 'lantus', 'with', 'aspart', 'qac', 'and', 'sliding', 'scale', 'when', 'eating', 'and', 'regular', 'insulin', 'sliding', 'scale', 'qh', 'when', 'npo', 'fingersticks', 'were', 'elevated', 'to', 's', 'early', 'during', 'this', 'admission', 'but', 'improved', 'to', 'the', 's', 'upon', 'increasing', 'her', 'insulin', 'dose', 'musculoskeletal', 'the', 'patient', 'is', 'status', 'post', 'right', 'total', 'hip', 'replacement', 'approximately', 'year', 'ago', 'and', 'complained', 'of', 'right', 'hip', 'pain', 'upon', 'admission', 'an', 'xray', 'showed', 'stable', 'arthroplasty', 'and', 'the', 'pain', 'wa', 'improved', 'by', 'the', 'morning', 'of', 'she', 'will', 'follow', 'with', 'physiatrist', 'dr', 'allan', 'kofoed', 'complication', 'right', 'hand', 'weakness', 'numbness', 'and', 'coolness', 'status', 'post', 'av', 'fistula', 'surgery', 'possibly', 'secondary', 'to', 'steal', 'or', 'neurapraxia', 'significantly', 'improved', 'by', 'the', 'time', 'of', 'discharge', 'consultant', 'dr', 'garfield', 'kiehne', 'from', 'vascular', 'surgery', 'dr', 'ambrose', 'moldrem', 'from', 'gastroenterology', 'physical', 'examination', 'on', 'discharge', 'stable', 'vital', 'sign', 'lung', 'with', 'bibasilar', 'crackle', 'systolic', 'murmur', 'heard', 'throughout', 'plus', 'a', 'mechanical', 's', 'abdomen', 'benign', 'lower', 'extremity', 'with', 'chronic', 'venous', 'stasis', 'change', 'right', 'upper', 'extremity', 'av', 'fistula', 'with', 'thrill', 'decreased', 'right', 'radial', 'pulse', 'with', 'warm', 'hand', 'distally', 'and', 'strength', 'in', 'hand', 'grip', 'of', 'the', 'right', 'hand', 'discharge', 'medication', 'norvasc', 'mg', 'daily', 'caltrate', 'plus', 'd', 'one', 'tablet', 'po', 'bid', 'aranesp', 'mcg', 'subcu', 'weekly', 'lovenox', 'mg', 'subcu', 'daily', 'starting', 'on', 'thursday', 'to', 'be', 'discontinued', 'when', 'patient', 'inr', 'is', 'therapeutic', 'on', 'coumadin', 'nexium', 'mg', 'po', 'bid', 'ferrous', 'sulfate', 'mg', 'po', 'bid', 'lasix', 'mg', 'po', 'daily', 'insulin', 'aspart', 'unit', 'subcu', 'every', 'meal', 'insulin', 'lantus', 'unit', 'subcu', 'qpm', 'imdur', 'mg', 'po', 'bid', 'sevelamer', 'mg', 'po', 'qac', 'multivitamin', 'one', 'tablet', 'daily', 'valsartan', 'mg', 'po', 'daily', 'and', 'coumadin', 'mg', 'po', 'qpm', 'starting', 'on', 'thursday', 'disposition', 'to', 'home', 'with', 'service', 'followup', 'appointment', 'the', 'patient', 'will', 'follow', 'up', 'with', 'dr', 'brendan', 'b', 'tordsen', 'in', 'one', 'to', 'two', 'week', 'with', 'dr', 'schaetzle', 'from', 'vascular', 'surgery', 'on', 'at', 'am', 'with', 'dr', 'salvatore', 'sherrod', 'from', 'physiatry', 'on', 'at', 'am', 'and', 'dr', 'margarito', 'nolting', 'on', 'at', 'am', 'code', 'status', 'the', 'patient', 'is', 'full', 'code', 'and', 'her', 'healthcare', 'proxy', 'is', 'her', 'daughter', 'shane', 'lutao', 'primary', 'care', 'physician', 'rufus', 'mannheimer', 'md', 'escription', 'document', 'cssten', 'tel', 'dictated', 'by', 'beier', 'julio', 'attending', 'hambric', 'margarito', 'kurt', 'dictation', 'id', 'd', 't']\n"]}],"source":["print(train_df['tok_lem_text'][0])"]}],"metadata":{"colab":{"provenance":[{"file_id":"10pK5od01jfTHJyLN94dJxEube3sJszFm","timestamp":1678482671183}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
