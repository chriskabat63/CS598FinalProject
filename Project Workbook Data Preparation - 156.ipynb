{"cells":[{"cell_type":"markdown","metadata":{"id":"3d1ehXFWM5aE"},"source":["This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n","***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n","\n","Abstract:  The main goal of the paper is to extract Morbidity from clinical notes.  The idea was to use a combination of classical and deep learning methods to determine the best approach for classifying these notes in one or more of 16 morbidity conditions.  These models used a combination of NLP techniques including embeddings and bag of words implementations.  It also measured the effect including of stop words.  Lastly, it used ensemble techniques to tie together a number of the classical and deep learning models to provide the most accurate results.\n","\n","Dataset was retrieved from the DBMI Data Portal, Department of Biomedical Informatics (DBMI) in the Blavatnik Institute at Harvard Medical School.  This dataset was originally created for the i2b2 Obesity Challenge conducted in 2008.\n","This data was provided in XML format with a test and training set.  Along with the test and training set, labeled data of two forms were included. They were called Intuitive and Textual.  Textual judgements were derived by looking at the notes by multiple experts.  When the experts didn’t agree, a resident doctor annotated it with a Intuitive judgement.\n","\n","In this workbook, we are taking the following steps:\n","\n","\n","* Loading test and train data along with annotations\n","* Exploring the best annotation data sets to use\n","* Preprocessing the data using NLP techniques described below.\n","* Saving the data as pkl files for use in additional notebooks.\n","\n","\n","\n","\n","\n"," "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4130,"status":"ok","timestamp":1679769726172,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"abcc0TyDkN4r","outputId":"12bdbdd7-6ffe-48fd-9800-bb5895f0b1e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: xmltodict in c:\\programdata\\anaconda3\\lib\\site-packages (0.12.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install xmltodict\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gv360l2IkNfO"},"source":["The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1250,"status":"ok","timestamp":1679769727418,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"zyXrAo2dsJqf","outputId":"0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"},"outputs":[],"source":["DATA_PATH = './obesity_data/'"]},{"cell_type":"markdown","metadata":{"id":"cw_odcoTOTS-"},"source":["Next we create a function to load the data from XML files and convert to a more usable dataframe structure."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1679769727418,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"AKAxFxOCMadc"},"outputs":[],"source":["import pandas as pd\n","import xmltodict\n","\n","def load_dataset(filepath, xpath):    \n","    return pd.read_xml(filepath, xpath=xpath)\n","\n","def load_annotations(filepath):\n","\n","  with open(filepath,\"r\") as f:\n","      data = f.read()\n","\n","  df = pd.DataFrame(columns=['source','disease','id','judgment'])\n","\n","  data = xmltodict.parse(data)['diseaseset']['diseases']\n","\n","  for key,val in enumerate(data):\n","    if(isinstance(val,str)):\n","      source = data['@source']\n","      disease = data['disease']\n","    else:\n","      source = val['@source']\n","      disease = val['disease']\n","\n","    for key,val in enumerate(disease):\n","      if(isinstance(val,str)):\n","        disease_name = disease['@name']\n","        doc = disease['doc']\n","      else:\n","        disease_name = val['@name']\n","        doc = val['doc']\n","      \n","      for key,val in enumerate(doc):\n","        if(isinstance(val,str)):\n","          doc_id = doc['@id']\n","          judgment = doc['@judgment']\n","        else:\n","          doc_id = val['@id']\n","          judgment = val['@judgment']\n","        df_temp = pd.DataFrame([{\"source\":source,\"disease\":disease_name,\"id\":doc_id,\"judgment\":judgment}])\n","        #df = df.append(df_temp)  \n","        df = pd.concat([df,df_temp])\n","\n","  #The xml acts really strange if there are single nodes.  Dropping duplicates solves it.\n","  return df.drop_duplicates()"]},{"cell_type":"markdown","metadata":{"id":"rLs0aX04OgNn"},"source":["Now we load the test and train datasets and examine the notes. Note, we are loading the training file with 2 as a seperate data frame as it relates to all the addendums which we believe was not used by the paper."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":154,"status":"ok","timestamp":1679769727568,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"yq2MgXu1naLN","outputId":"3cfb3f54-e246-4b7a-9799-15701ee92f28"},"outputs":[{"name":"stdout","output_type":"stream","text":["   id                                               text\n","0   3  470971328 | AECH | 09071283 | | 6159055 | 5/26...\n","1   5  508283935 | KFM | 67491508 | | 9707967 | 9/25/...\n","2   7  248652055 | CM | 07563073 | | 5027467 | 8/29/2...\n","3   8  052907410 | FTH | 50999409 | | 7815179 | 10/6/...\n","4   9  628477951 | MBCH | 30737210 | | 5713924 | 12/1...\n","507\n"]},{"ename":"NameError","evalue":"name 'DATA' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18280/16118029.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'obesity_patient_records_training.xml'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'/root/docs/*'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_df_with2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/obesity_patient_records_training2.xml'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'/root/docs/*'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtrain_df_with2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df_with2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mNameError\u001b[0m: name 'DATA' is not defined"]}],"source":["test_df = load_dataset(DATA_PATH + 'obesity_patient_records_test.xml', xpath='/root/docs/*')\n","test_df['id'] = pd.to_numeric(test_df['id'])\n","print(test_df.head())\n","print(len(test_df))\n","\n","train_df = load_dataset(DATA_PATH + 'obesity_patient_records_training.xml', xpath='/root/docs/*')\n","train_df_with2 = train_df.append(load_dataset(DATA + '/obesity_patient_records_training2.xml', xpath='/root/docs/*'))\n","train_df['id'] = pd.to_numeric(train_df['id'])\n","train_df_with2['id'] = pd.to_numeric(train_df_with2['id'])\n","print(train_df.head())\n","print(len(train_df))\n","print(len(train_df_with2))\n","\n","print(test_df['text'][0])"]},{"cell_type":"markdown","metadata":{"id":"ACBjTAgsOmgT"},"source":["The annotation data came in two forms: textual and intuitive.  It also came with files with the forms in seperate files and with the forms all together in one file.  We do some exploration to determine which set of data is the closest to the study."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27805,"status":"ok","timestamp":1679769755372,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"Mlf84b2lhh9x","outputId":"cc2b826a-e08a-43a6-9269-9ee9549d590c"},"outputs":[{"name":"stdout","output_type":"stream","text":["      source disease  id judgment\n","0  intuitive  Asthma   3        Y\n","0  intuitive  Asthma   5        N\n","0  intuitive  Asthma   7        N\n","0  intuitive  Asthma   9        Y\n","0  intuitive  Asthma  10        N\n","7399\n","    source disease  id judgment\n","0  textual  Asthma   3        Y\n","0  textual  Asthma   5        U\n","0  textual  Asthma   7        U\n","0  textual  Asthma   8        U\n","0  textual  Asthma   9        Y\n","8044\n"]}],"source":["test_annot_intuitive_df = load_annotations(DATA_PATH + \"obesity_standoff_annotations_test_intuitive.xml\")\n","test_annot_intuitive_df['id'] = pd.to_numeric(test_annot_intuitive_df['id'])\n","\n","test_annot_textual_df = load_annotations(DATA_PATH + \"obesity_standoff_annotations_test_textual.xml\")\n","test_annot_textual_df['id'] = pd.to_numeric(test_annot_textual_df['id'])\n","\n","print(test_annot_intuitive_df.head())\n","print(len(test_annot_intuitive_df))\n","\n","print(test_annot_textual_df.head())\n","print(len(test_annot_textual_df))\n"]},{"cell_type":"markdown","metadata":{"id":"vlDvN-LnPLwl"},"source":["The test file with all forms is explored and the record count seems the same as combining the seperate files."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11952,"status":"ok","timestamp":1679769767321,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"u8Y_KX0h1Jsn","outputId":"c37da12d-1e5e-4ca9-ad59-f90251686ecd"},"outputs":[{"name":"stdout","output_type":"stream","text":["      source disease  id judgment\n","0  intuitive  Asthma   3        Y\n","0  intuitive  Asthma   5        N\n","0  intuitive  Asthma   7        N\n","0  intuitive  Asthma   9        Y\n","0  intuitive  Asthma  10        N\n","15443\n"]}],"source":["#trying to verify the same number of records in the one with both intuitive and textual\n","test_annot_all_df = load_annotations(DATA_PATH + \"obesity_standoff_annotations_test.xml\")\n","test_annot_all_df['id'] = pd.to_numeric(test_annot_all_df['id'])\n","\n","print(test_annot_all_df.head())\n","print(len(test_annot_all_df))"]},{"cell_type":"markdown","metadata":{"id":"CR_EMmuoPSf0"},"source":["We then do the same analysis with the training annotations."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29125,"status":"ok","timestamp":1679769796436,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"1nUpoSeuxMDG","outputId":"0ed999c6-111d-4aa8-b988-2e1bf6f6e468"},"outputs":[{"name":"stdout","output_type":"stream","text":["      source disease  id judgment\n","0  intuitive  Asthma   1        N\n","0  intuitive  Asthma   2        Y\n","0  intuitive  Asthma   4        N\n","0  intuitive  Asthma   6        N\n","0  intuitive  Asthma  15        N\n","8621\n","    source disease  id judgment\n","0  textual  Asthma   1        U\n","0  textual  Asthma   2        Y\n","0  textual  Asthma   4        U\n","0  textual  Asthma   6        U\n","0  textual  Asthma  13        U\n","9655\n"]}],"source":["train_annot_intuitive_df = load_annotations(DATA_PATH + \"obesity_standoff_intuitive_annotations_training.xml\")\n","train_annot_intuitive_df['id'] = pd.to_numeric(train_annot_intuitive_df['id'])\n","train_annot_textual_df = load_annotations(DATA_PATH + \"obesity_standoff_textual_annotations_training.xml\")\n","train_annot_textual_df['id'] = pd.to_numeric(train_annot_textual_df['id'])\n","\n","print(train_annot_intuitive_df.head())\n","print(len(train_annot_intuitive_df))\n","\n","print(train_annot_textual_df.head())\n","print(len(train_annot_textual_df))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QM3B1QR-PWsq"},"source":["When we look at the full file with addendums, we see there is a lot more data in the full file than the seperate file."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18423,"status":"ok","timestamp":1679769814849,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"Qrh6CrnzC1P8","outputId":"bda08ef8-49ef-4b81-ccfa-bb9fb8d8ead2"},"outputs":[{"name":"stdout","output_type":"stream","text":["      source disease  id judgment\n","0  intuitive  Asthma   1        N\n","0  intuitive  Asthma   2        Y\n","0  intuitive  Asthma   4        N\n","0  intuitive  Asthma   6        N\n","0  intuitive  Asthma  15        N\n","18276\n","22285\n"]}],"source":["#trying to verify the same number of records in the one with both intuitive and textual (It isn't according to tally.pdf it should be 22285 with the annotations and addendums)\n","train_annot_all_df = load_annotations(DATA_PATH + \"obesity_standoff_annotations_training.xml\")\n","train_annot_all_df_with2 = pd.concat([train_annot_all_df,load_annotations(DATA_PATH + \"obesity_standoff_annotations_training_addendum.xml\")])\n","train_annot_all_df_with2 = pd.concat([train_annot_all_df_with2,load_annotations(DATA_PATH + \"obesity_standoff_annotations_training_addendum2.xml\")])\n","train_annot_all_df_with2 = pd.concat([train_annot_all_df_with2,load_annotations(DATA_PATH + \"obesity_standoff_annotations_training_addendum3.xml\")])\n","\n","train_annot_all_df['id'] = pd.to_numeric(train_annot_all_df['id'])\n","train_annot_all_df_with2['id'] = pd.to_numeric(train_annot_all_df_with2['id'])\n","\n","print(train_annot_all_df.head())\n","print(len(train_annot_all_df))\n","print(len(train_annot_all_df_with2))\n"]},{"cell_type":"markdown","metadata":{"id":"TFn3q_Fom9T6"},"source":["We are going to start with the annotations in one file (test_annot_all_df, train_annot_all_df).  The paper only used annotations that were clearly marked 'Y' or 'N' (It excluded the 'Q' and 'U')."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1679769814849,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"QfHtYhyrm8zP","outputId":"459aaf55-69d9-4eae-e8d0-0627249da20a"},"outputs":[{"name":"stdout","output_type":"stream","text":["15443 18276\n","9642 11274\n"]}],"source":["print(len(test_annot_all_df),len(train_annot_all_df))\n","test_annot_all_df_clean = test_annot_all_df[(test_annot_all_df['judgment']  == 'Y') | (test_annot_all_df['judgment']  == 'N')]\n","train_annot_all_df_clean = train_annot_all_df[(train_annot_all_df['judgment']  == 'Y') | (train_annot_all_df['judgment']  == 'N')]\n","print(len(test_annot_all_df_clean),len(train_annot_all_df_clean))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1679769814849,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"PHKozf15u2Km","outputId":"b5719f17-543c-4fbd-f325-cda401b40043"},"outputs":[{"name":"stdout","output_type":"stream","text":["disease\n","Asthma                  541\n","CAD                     756\n","CHF                     650\n","Depression              549\n","Diabetes                829\n","GERD                    494\n","Gallstones              580\n","Gout                    552\n","Hypercholesterolemia    650\n","Hypertension            826\n","Hypertriglyceridemia    496\n","OA                      544\n","OSA                     562\n","Obesity                 648\n","PVD                     528\n","Venous Insufficiency    437\n","dtype: int64\n"]}],"source":["print(test_annot_all_df_clean.groupby('disease').size())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1679769814850,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"WUVhK8bkvbXV","outputId":"5d03131a-d4af-49a4-f452-9e62c6cfd92b"},"outputs":[{"name":"stdout","output_type":"stream","text":["disease\n","Asthma                  648\n","CAD                     897\n","CHF                     489\n","Depression              668\n","Diabetes                978\n","GERD                    586\n","Gallstones              687\n","Gout                    667\n","Hypercholesterolemia    757\n","Hypertension            983\n","Hypertriglyceridemia    602\n","OA                      654\n","OSA                     678\n","Obesity                 801\n","PVD                     639\n","Venous Insufficiency    540\n","dtype: int64\n"]}],"source":["\n","print(train_annot_all_df_clean.groupby('disease').size())\n"]},{"cell_type":"markdown","metadata":{"id":"mfezONE45GEd"},"source":["The paper specifically calls out 6 files and does not mention the addendums, so we will stick with the seperately labeled files for our study. There seems to be only one record in each of the test and training set where the textual and intuitive disagree."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1679769814850,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"EkckqW7G0Fqh","outputId":"5dca85d7-49c8-46a3-fed8-031958afe016"},"outputs":[{"name":"stdout","output_type":"stream","text":["7385\n","2257\n","       source_x disease  id judgment_x source_y judgment_y\n","1712  intuitive      OA   8          N  textual          Y\n","8598\n","2676\n","      source_x disease    id judgment_x source_y judgment_y\n","571  intuitive     CHF  1072          Y  textual          N\n"]}],"source":["test_annot_intuitive_df_clean = test_annot_intuitive_df[(test_annot_intuitive_df['judgment']  == 'Y') | (test_annot_intuitive_df['judgment']  == 'N')]\n","test_annot_textual_df_clean = test_annot_textual_df[(test_annot_textual_df['judgment']  == 'Y') | (test_annot_textual_df['judgment']  == 'N')]\n","train_annot_intuitive_df_clean = train_annot_intuitive_df[(train_annot_intuitive_df['judgment']  == 'Y') | (train_annot_intuitive_df['judgment']  == 'N')]\n","train_annot_textual_df_clean = train_annot_textual_df[(train_annot_textual_df['judgment']  == 'Y') | (train_annot_textual_df['judgment']  == 'N')]\n","\n","print(len(test_annot_intuitive_df_clean))\n","print(len(test_annot_textual_df_clean))\n","\n","\n","df = test_annot_intuitive_df_clean.merge(test_annot_textual_df_clean, on=['id','disease'])\n","print(df[df['judgment_x'] != df['judgment_y']])\n","\n","print(len(train_annot_intuitive_df_clean))\n","print(len(train_annot_textual_df_clean))\n","\n","df = train_annot_intuitive_df_clean.merge(train_annot_textual_df_clean, on=['id','disease'])\n","print(df[df['judgment_x'] != df['judgment_y']])\n"]},{"cell_type":"markdown","metadata":{"id":"2Z9joC8e-aFl"},"source":["Let's remove those two records from the textual table and recheck."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1679769814851,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"YfspC1q0-lUU","outputId":"301372e0-0415-4a60-b5fd-91b5d8fd8490"},"outputs":[{"name":"stdout","output_type":"stream","text":["2256\n","Empty DataFrame\n","Columns: [source_x, disease, id, judgment_x, index, source_y, judgment_y]\n","Index: []\n","2675\n","Empty DataFrame\n","Columns: [source_x, disease, id, judgment_x, index, source_y, judgment_y]\n","Index: []\n"]}],"source":["df = test_annot_textual_df_clean\n","df = df.reset_index()\n","index_names = df[(df['disease'] == 'OA') & (df['id'] == 8)].index\n","test_annot_textual_df_clean = df.drop(index_names)\n","\n","df = train_annot_textual_df_clean\n","df = df.reset_index()\n","index_names = df[(df['disease'] == 'CHF') & (df['id'] == 1072)].index\n","train_annot_textual_df_clean = df.drop(index_names)\n","\n","print(len(test_annot_textual_df_clean))\n","df = test_annot_intuitive_df_clean.merge(test_annot_textual_df_clean, on=['id','disease'])\n","print(df[df['judgment_x'] != df['judgment_y']])\n","\n","print(len(train_annot_textual_df_clean))\n","df = train_annot_intuitive_df_clean.merge(train_annot_textual_df_clean, on=['id','disease'])\n","print(df[df['judgment_x'] != df['judgment_y']])\n"]},{"cell_type":"markdown","metadata":{"id":"rDoDyp5u9uSA"},"source":["The paper does a classification model for each disase seperately.  Need to be able to loop through each. Using the seperate files  seems to come closer to the disease counts the paper discusses before pre-processing, so we will use this data. The study must have done some additional processing that is not evident from the paper, so our results may be a little different."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1679769815355,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"l0WT-GUr5o7o","outputId":"1eee01bd-aa4b-4f0b-cda3-63ede6c3a392"},"outputs":[{"name":"stdout","output_type":"stream","text":["Disease: Asthma\n","Train: 572 76 648\n","Test: 471 70 541\n","All: 1189\n","Disease: CAD\n","Train: 548 349 897\n","Test: 457 299 756\n","All: 1653\n","Disease: CHF\n","Train: 243 245 488\n","Test: 434 216 650\n","All: 1138\n","Disease: Depression\n","Train: 582 86 668\n","Test: 477 72 549\n","All: 1217\n","Disease: Diabetes\n","Train: 567 411 978\n","Test: 479 350 829\n","All: 1807\n","Disease: Gallstones\n","Train: 593 94 687\n","Test: 491 89 580\n","All: 1267\n","Disease: GERD\n","Train: 487 99 586\n","Test: 424 70 494\n","All: 1080\n","Disease: Gout\n","Train: 596 71 667\n","Test: 500 52 552\n","All: 1219\n","Disease: Hypercholesterolemia\n","Train: 502 255 757\n","Test: 431 219 650\n","All: 1407\n","Disease: Hypertension\n","Train: 531 452 983\n","Test: 446 380 826\n","All: 1809\n","Disease: Hypertriglyceridemia\n","Train: 587 15 602\n","Test: 486 10 496\n","All: 1098\n","Disease: OA\n","Train: 565 89 654\n","Test: 458 85 543\n","All: 1197\n","Disease: Obesity\n","Train: 553 248 801\n","Test: 447 201 648\n","All: 1449\n","Disease: OSA\n","Train: 590 88 678\n","Test: 493 69 562\n","All: 1240\n","Disease: PVD\n","Train: 556 83 639\n","Test: 464 64 528\n","All: 1167\n","Disease: Venous Insufficiency\n","Train: 526 14 540\n","Test: 427 10 437\n","All: 977\n","Samples: 20914\n"]}],"source":["disease_list = train_annot_intuitive_df_clean['disease'].unique().tolist()\n","\n","train_annot_all_df_clean = pd.concat([train_annot_intuitive_df_clean,train_annot_textual_df_clean])\n","train_annot_all_df_clean = train_annot_all_df_clean.drop(['source'], axis=1)\n","train_annot_all_df_clean = train_annot_all_df_clean.drop_duplicates()\n","\n","test_annot_all_df_clean = pd.concat([test_annot_intuitive_df_clean,test_annot_textual_df_clean])\n","test_annot_all_df_clean = test_annot_all_df_clean.drop(['source'], axis=1)\n","test_annot_all_df_clean = test_annot_all_df_clean.drop_duplicates()\n","\n","annot_all_df_clean = pd.concat([train_annot_all_df_clean,test_annot_all_df_clean])\n","annot_all_df_clean = annot_all_df_clean.drop_duplicates()\n","samples = 0\n","\n","for disease in disease_list:\n","  print('Disease:',disease)\n","  print('Train:',sum(train_annot_intuitive_df_clean['disease'] == disease),\n","        sum(train_annot_textual_df_clean['disease'] == disease),\n","        sum(train_annot_all_df_clean['disease'] == disease))\n","  print('Test:',sum(test_annot_intuitive_df_clean['disease'] == disease),\n","        sum(test_annot_textual_df_clean['disease'] == disease),\n","        sum(test_annot_all_df_clean['disease'] == disease))  \n","  print('All:',sum(annot_all_df_clean['disease'] == disease))\n","  samples = sum(annot_all_df_clean['disease'] == disease) + samples\n","\n","print(\"Samples:\",samples)\n"]},{"cell_type":"markdown","metadata":{"id":"_VmV5x9WQfZ3"},"source":["Datasets to use for rest of the study:\n","* test_df [id,text] (document, clinical notes)\n","* train_df [id,text] (document, clinical notes)\n","* test_annot_all_df_clean [disease,id,judment,index] (disease, document, judgment)\n","* train_annot_all_df_clean [disease,id,judment,index] (disease, document, judgment)\n","\n","For the annotations, we should convert judgement to a numeric label.\n","\n","Our next step is to continue the preprocessing of the data.  We want to do this seperately from the annotations, they can be joined when doing classification by each disase. This includes:\n","\n","* Lower-casing of the text\n","* Removing punctuation and numeric values from the text\n","* Tokenization of text \n","* Lemmatizattion of the tokens\n","* TF-IDF matrix generation (TF-IDF Vectorizer4 from the scikit-learn library)\n","\n","We have an optional parameter to remove stop words as the paper discusses the fact that stop words should be included for deep learning models."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125,"status":"ok","timestamp":1679769978772,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"Oqv8oDCxfRAL","outputId":"7a4cd7ff-8a1a-4bd3-d396-cf07b49a607d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>disease</th>\n","      <th>id</th>\n","      <th>judgment</th>\n","      <th>index</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Asthma</td>\n","      <td>1</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Asthma</td>\n","      <td>2</td>\n","      <td>Y</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Asthma</td>\n","      <td>4</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Asthma</td>\n","      <td>6</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Asthma</td>\n","      <td>15</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2671</th>\n","      <td>Venous Insufficiency</td>\n","      <td>879</td>\n","      <td>Y</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2672</th>\n","      <td>Venous Insufficiency</td>\n","      <td>989</td>\n","      <td>Y</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2673</th>\n","      <td>Venous Insufficiency</td>\n","      <td>1055</td>\n","      <td>Y</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2674</th>\n","      <td>Venous Insufficiency</td>\n","      <td>1149</td>\n","      <td>Y</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2675</th>\n","      <td>Venous Insufficiency</td>\n","      <td>1216</td>\n","      <td>Y</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11273 rows × 4 columns</p>\n","</div>"],"text/plain":["                   disease    id judgment  index\n","0                   Asthma     1        N    NaN\n","0                   Asthma     2        Y    NaN\n","0                   Asthma     4        N    NaN\n","0                   Asthma     6        N    NaN\n","0                   Asthma    15        N    NaN\n","...                    ...   ...      ...    ...\n","2671  Venous Insufficiency   879        Y    0.0\n","2672  Venous Insufficiency   989        Y    0.0\n","2673  Venous Insufficiency  1055        Y    0.0\n","2674  Venous Insufficiency  1149        Y    0.0\n","2675  Venous Insufficiency  1216        Y    0.0\n","\n","[11273 rows x 4 columns]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["\n","test_df\n","train_df\n","test_annot_all_df_clean\n","train_annot_all_df_clean"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":283,"status":"ok","timestamp":1679770827175,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"zkGLF2yPfdnJ","outputId":"c37191e4-cae7-4573-f6f0-8328570c055b"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\chkabat\\AppData\\Roaming\\nltk_data...\n"]},{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","import re\n","import string\n","import nltk\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":106,"status":"ok","timestamp":1679771081128,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"q7KPCWF4B6hE"},"outputs":[],"source":["#wn = WordNetLemmatizer()\n","\n","#def black_txt(token):\n","  #return token not in list(string.punctuation) and len(token) > 2\n","\n","#def clean_txt(text):\n","  #clean_text = []\n","  \n","  #text = re.sub(re.escape(\"'\"), \"\", text)\n","  #text = re.sub(re.escape(\"\\\\d|\\\\W)+\"), \" \", text)\n","  #clean_text = [wn.lemmatize(word, pos = \"v\") for word in word_tokenize(text.lower()) if black_txt(word)]\n","\n","  #return \" \".join(clean_text)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4057,"status":"ok","timestamp":1679771153404,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"Gw14Xn7zDL_e","outputId":"ea3aafa5-c429-4456-aeeb-1ddd58c42f52"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\chkabat\\AppData\\Local\\Temp/ipykernel_18280/3043505989.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n","  train_df[\"no_punc_text\"] = train_df['text'].str.replace('[^\\w\\s]', '')\n","C:\\Users\\chkabat\\AppData\\Local\\Temp/ipykernel_18280/3043505989.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n","  train_df[\"no_numerics_text\"] = train_df['no_punc_text'].str.replace('\\d+', '')\n"]}],"source":["#train_df['Clean_Description'] = train_df['text'].map(str).apply(clean_txt)\n","wn = WordNetLemmatizer()\n","\n","train_df[\"no_punc_text\"] = train_df['text'].str.replace('[^\\w\\s]', '')\n","train_df[\"no_numerics_text\"] = train_df['no_punc_text'].str.replace('\\d+', '')\n","train_df[\"lower_text\"] = train_df['no_numerics_text'].apply(str.lower)\n","\n","train_df[\"tokenized_text\"] = train_df.apply(lambda row: word_tokenize(row['lower_text']), axis=1)\n","\n","train_df[\"tok_lem_text\"] = train_df['tokenized_text'].apply(\n","    lambda lst:[wn.lemmatize(word) for word in lst])"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1679771153404,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"QQbaVExOGyoP","outputId":"763849da-e152-4b63-9625-607f50c63981"},"outputs":[{"name":"stdout","output_type":"stream","text":["['wmc', 'am', 'anemia', 'signed', 'dis', 'admission', 'date', 'report', 'status', 'signed', 'discharge', 'date', 'attending', 'truka', 'deon', 'xavier', 'md', 'service', 'bh', 'principal', 'diagnosis', 'anemia', 'and', 'gi', 'bleed', 'secondary', 'diagnosis', 'diabetes', 'mitral', 'valve', 'replacement', 'atrial', 'fibrillation', 'and', 'chronic', 'kidney', 'disease', 'history', 'of', 'present', 'illness', 'the', 'patient', 'is', 'an', 'yearold', 'woman', 'with', 'a', 'history', 'of', 'diabetes', 'chronic', 'kidney', 'disease', 'congestive', 'heart', 'failure', 'with', 'ejection', 'fraction', 'of', 'to', 'who', 'present', 'from', 'clinic', 'with', 'a', 'chief', 'complaint', 'of', 'fatigue', 'and', 'weakness', 'for', 'one', 'week', 'she', 'had', 'had', 'worsening', 'right', 'groin', 'and', 'hip', 'pain', 'status', 'post', 'a', 'total', 'hip', 'replacement', 'approximately', 'year', 'ago', 'which', 'had', 'been', 'worsening', 'for', 'two', 'week', 'and', 'she', 'ha', 'also', 'recently', 'completed', 'a', 'course', 'of', 'levaquin', 'for', 'urinary', 'tract', 'infection', 'she', 'presented', 'to', 'dr', 'parrent', 'office', 'complaining', 'of', 'fatigue', 'and', 'weakness', 'for', 'one', 'week', 'she', 'ha', 'had', 'some', 'abdominal', 'pain', 'in', 'a', 'bandlike', 'distribution', 'around', 'her', 'right', 'side', 'she', 'wa', 'found', 'to', 'have', 'a', 'hematocrit', 'of', 'down', 'from', 'eight', 'day', 'ago', 'and', 'wa', 'sent', 'to', 'the', 'emergency', 'department', 'for', 'transfusion', 'and', 'workup', 'of', 'her', 'anemia', 'preadmission', 'medication', 'caltrate', 'plus', 'd', 'one', 'tab', 'po', 'bid', 'lantus', 'unit', 'sc', 'qpm', 'novolog', 'unit', 'unit', 'unit', 'sc', 'tid', 'imdur', 'mg', 'bid', 'amlodipine', 'mg', 'bid', 'furosemide', 'mg', 'daily', 'valsartan', 'mg', 'daily', 'warfarin', 'mg', 'daily', 'iron', 'sulfate', 'mg', 'po', 'daily', 'and', 'multivitamin', 'daily', 'past', 'medical', 'history', 'chronic', 'kidney', 'disease', 'presumed', 'due', 'to', 'congestive', 'heart', 'failurediuresisrenal', 'artery', 'diseaseearly', 'diabetic', 'nephropathy', 'type', 'diabetes', 'previous', 'stroke', 'congestive', 'heart', 'failure', 'with', 'ejection', 'fraction', 'of', 'to', 'rheumatic', 'valvular', 'disease', 'with', 'mitral', 'valve', 'replacement', 'and', 'tricuspid', 'valve', 'repair', 'atrial', 'fibrillation', 'history', 'of', 'small', 'bowel', 'obstruction', 'status', 'post', 'right', 'total', 'hip', 'replacement', 'approximately', 'year', 'ago', 'family', 'history', 'no', 'family', 'history', 'of', 'kidney', 'disease', 'or', 'heart', 'disease', 'social', 'history', 'she', 'ha', 'child', 'life', 'alone', 'with', 'home', 'care', 'in', 'me', 'but', 'ha', 'moved', 'in', 'to', 'live', 'with', 'her', 'daughter', 'in', 'news', 'irv', 'in', 'she', 'denies', 'tobacco', 'use', 'and', 'drink', 'alcohol', 'rarely', 'allergy', 'codeine', 'and', 'benadryl', 'admission', 'physical', 'examination', 'vital', 'sign', 'were', 'temperature', 'heart', 'rate', 'blood', 'pressure', 'respiration', 'and', 'sao', 'on', 'room', 'air', 'the', 'patient', 'is', 'a', 'frail', 'elderly', 'woman', 'in', 'no', 'acute', 'distress', 'she', 'ha', 'poor', 'dentition', 'jvp', 'is', 'difficult', 'to', 'ass', 'secondary', 'to', 'tricuspid', 'regurgitation', 'lung', 'were', 'clear', 'to', 'auscultation', 'bilaterally', 'cardiovascular', 'exam', 'showed', 'bradycardia', 'with', 'heart', 'rate', 'in', 'the', 's', 'that', 'wa', 'irregular', 's', 'plus', 's', 'with', 'systolic', 'murmur', 'heard', 'throughout', 'with', 'mechanical', 'sounding', 's', 'abdomen', 'wa', 'mildly', 'tender', 'to', 'palpation', 'in', 'the', 'mid', 'epigastrium', 'with', 'no', 'rebound', 'or', 'guarding', 'extremity', 'showed', 'venous', 'stasis', 'change', 'in', 'her', 'lower', 'extremity', 'bilaterally', 'foot', 'were', 'cool', 'with', 'diminished', 'dp', 'and', 'pt', 'pulse', 'on', 'neurological', 'exam', 'she', 'wa', 'alert', 'and', 'oriented', 'x', 'and', 'cranial', 'nerve', 'ii', 'through', 'xii', 'were', 'intact', 'study', 'ekg', 'showed', 'atrial', 'fibrillation', 'with', 'slow', 'ventricular', 'response', 'with', 'heart', 'rate', 'of', 'widened', 'qrs', 'a', 'q', 'wave', 'in', 'avl', 'and', 'u', 'wave', 'in', 'the', 'lateral', 'lead', 'chest', 'xray', 'showed', 'improved', 'pleural', 'effusion', 'and', 'pulmonary', 'edema', 'stable', 'marked', 'cardiomegaly', 'xray', 'of', 'the', 'right', 'hip', 'showed', 'status', 'post', 'right', 'total', 'hip', 'arthroplasty', 'with', 'stable', 'periprosthetic', 'lucency', 'and', 'cortical', 'remodeling', 'severe', 'left', 'hip', 'osteoarthritis', 'diffuse', 'atherosclerosis', 'egd', 'on', 'showed', 'hiatal', 'hernia', 'fundic', 'polyp', 'with', 'pathology', 'showing', 'hypoplastic', 'and', 'inflammatory', 'lesion', 'mild', 'antral', 'erosion', 'with', 'pathology', 'demonstrating', 'mild', 'regeneration', 'that', 'wa', 'nonspecific', 'and', 'no', 'h', 'pylorus', 'and', 'duodenitis', 'with', 'pathology', 'showing', 'normal', 'mucosa', 'but', 'no', 'active', 'bleeding', 'or', 'sign', 'of', 'recent', 'bleeding', 'capsule', 'endoscopy', 'on', 'showed', 'a', 'healing', 'gastric', 'ulcer', 'likely', 'in', 'the', 'antrum', 'small', 'bowel', 'lymphangiectasia', 'and', 'angioectasia', 'in', 'the', 'distal', 'small', 'bowel', 'colonoscopy', 'on', 'demonstrated', 'cecal', 'diverticulum', 'approximately', 'mm', 'ascending', 'sessile', 'polyp', 'and', 'several', 'small', 'sessile', 'polyp', 'in', 'the', 'rectosigmoid', 'procedure', 'right', 'basilic', 'vein', 'transposition', 'on', 'by', 'dr', 'jacinto', 'goonez', 'hospital', 'course', 'by', 'problem', 'gi', 'bleed', 'in', 'the', 'emergency', 'department', 'the', 'patient', 'vital', 'sign', 'were', 'temperature', 'heart', 'rate', 'respiration', 'blood', 'pressure', 'with', 'sao', 'on', 'room', 'air', 'she', 'wa', 'found', 'to', 'have', 'black', 'guaiac', 'positive', 'stool', 'and', 'gi', 'wa', 'consulted', 'she', 'wa', 'started', 'on', 'iv', 'nexium', 'mg', 'bid', 'and', 'wa', 'given', 'vitamin', 'k', 'mg', 'subcutaneously', 'and', 'two', 'unit', 'of', 'ffp', 'and', 'she', 'wa', 'transfused', 'three', 'unit', 'of', 'packed', 'red', 'blood', 'cell', 'with', 'lasix', 'mg', 'iv', 'for', 'each', 'bag', 'of', 'note', 'the', 'patient', 'had', 'a', 'colonoscopy', 'in', 'wa', 'on', 'which', 'showed', 'bleeding', 'rectal', 'ulcer', 'with', 'biopsy', 'consistent', 'with', 'ischemic', 'colitis', 'an', 'egd', 'on', 'showed', 'a', 'hiatal', 'hernia', 'fundic', 'polyp', 'path', 'hypoplasticinflammatory', 'mild', 'antral', 'erosion', 'path', 'mild', 'regeneration', 'nonspecific', 'no', 'h', 'pylorus', 'duodenitis', 'path', 'normal', 'mucosa', 'but', 'no', 'active', 'bleeding', 'or', 'sign', 'of', 'recent', 'bleeding', 'capsule', 'endoscopy', 'on', 'showed', 'healing', 'gastric', 'ulcer', 'likely', 'in', 'the', 'antrum', 'small', 'bowel', 'lymphangiectasia', 'and', 'angioectasia', 'in', 'the', 'distal', 'small', 'bowel', 'which', 'were', 'considered', 'the', 'likely', 'source', 'of', 'bleeding', 'colonoscopy', 'wa', 'performed', 'on', 'to', 'search', 'for', 'angioectasias', 'for', 'which', 'intervention', 'would', 'be', 'possible', 'but', 'demonstrated', 'only', 'a', 'cecal', 'diverticulum', 'and', 'approximately', 'mm', 'ascending', 'sessile', 'polyp', 'and', 'several', 'small', 'sessile', 'polyp', 'in', 'the', 'rectosigmoid', 'the', 'patient', 'wa', 'started', 'on', 'aranesp', 'but', 'had', 'a', 'point', 'hematocrit', 'drop', 'from', 'to', 'on', 'for', 'which', 'she', 'required', 'another', 'two', 'unit', 'of', 'blood', 'along', 'with', 'lasix', 'the', 'patient', 'hematocrit', 'remained', 'stable', 'at', 'approximately', 'to', 'and', 'she', 'will', 'be', 'restarted', 'on', 'anticoagulation', 'on', 'thursday', 'one', 'week', 'after', 'her', 'av', 'fistula', 'surgery', 'renal', 'the', 'patient', 'ha', 'chronic', 'kidney', 'disease', 'and', 'is', 'being', 'considered', 'for', 'a', 'possible', 'hemodialysis', 'in', 'the', 'future', 'she', 'wa', 'continued', 'on', 'her', 'caltrate', 'plus', 'd', 'multivitamin', 'and', 'iron', 'supplementation', 'and', 'she', 'wa', 'started', 'on', 'aranesp', 'mcg', 'weekly', 'and', 'wa', 'given', 'sevelamer', 'mg', 'qac', 'for', 'elevated', 'phosphate', 'level', 'vein', 'mapping', 'study', 'wa', 'performed', 'on', 'and', 'a', 'right', 'basilic', 'vein', 'transposition', 'wa', 'performed', 'on', 'by', 'dr', 'landrie', 'the', 'patient', 'had', 'postoperative', 'right', 'hand', 'coolness', 'numbness', 'and', 'weakness', 'always', 'with', 'dopplerable', 'radial', 'pulse', 'from', 'steal', 'versus', 'neurapraxia', 'which', 'wa', 'improved', 'by', 'the', 'time', 'of', 'discharge', 'she', 'will', 'follow', 'up', 'with', 'dr', 'chanthasene', 'a', 'an', 'outpatient', 'the', 'patient', 'creatinine', 'wa', 'on', 'admission', 'improved', 'to', 'by', 'which', 'wa', 'the', 'day', 'of', 'her', 'surgery', 'and', 'increased', 'again', 'to', 'on', 'she', 'wa', 'given', 'iv', 'lasix', 'bolus', 'a', 'she', 'had', 'evidence', 'of', 'volume', 'overload', 'with', 'over', 'eightpound', 'weight', 'gain', 'during', 'her', 'hospitalization', 'and', 'creatinine', 'improved', 'to', 'by', 'the', 'day', 'of', 'discharge', 'cardiovascular', 'pump', 'the', 'patient', 'ejection', 'fraction', 'is', 'to', 'and', 'she', 'wa', 'given', 'mg', 'of', 'iv', 'lasix', 'for', 'each', 'unit', 'of', 'packed', 'red', 'blood', 'cell', 'she', 'received', 'along', 'with', 'mg', 'po', 'daily', 'with', 'iv', 'bolus', 'of', 'mg', 'a', 'needed', 'for', 'volume', 'overload', 'a', 'judged', 'by', 'an', 'increase', 'in', 'her', 'weight', 'antihypertensive', 'medication', 'were', 'originally', 'held', 'for', 'her', 'gi', 'bleed', 'and', 'they', 'were', 'restarted', 'on', 'with', 'systolic', 'blood', 'pressure', 'remaining', 'in', 'the', 's', 'to', 's', 'rhythm', 'the', 'patient', 'ha', 'atrial', 'fibrillation', 'with', 'slow', 'ventricular', 'response', 'with', 'heart', 'rate', 'a', 'low', 'a', 'the', 'upper', 's', 'occasionally', 'her', 'heart', 'rate', 'appeared', 'regular', 'and', 'wa', 'thought', 'to', 'be', 'junctional', 'escape', 'rhythm', 'she', 'wa', 'asymptomatic', 'throughout', 'her', 'hospitalization', 'she', 'wa', 'discussed', 'with', 'her', 'cardiologist', 'dr', 'fritz', 'who', 'will', 'consider', 'a', 'pacemaker', 'a', 'an', 'outpatient', 'endocrine', 'she', 'is', 'euthyroid', 'with', 'tsh', 'of', 'for', 'her', 'diabetes', 'she', 'received', 'nightly', 'lantus', 'with', 'aspart', 'qac', 'and', 'sliding', 'scale', 'when', 'eating', 'and', 'regular', 'insulin', 'sliding', 'scale', 'qh', 'when', 'npo', 'fingersticks', 'were', 'elevated', 'to', 's', 'early', 'during', 'this', 'admission', 'but', 'improved', 'to', 'the', 's', 'upon', 'increasing', 'her', 'insulin', 'dose', 'musculoskeletal', 'the', 'patient', 'is', 'status', 'post', 'right', 'total', 'hip', 'replacement', 'approximately', 'year', 'ago', 'and', 'complained', 'of', 'right', 'hip', 'pain', 'upon', 'admission', 'an', 'xray', 'showed', 'stable', 'arthroplasty', 'and', 'the', 'pain', 'wa', 'improved', 'by', 'the', 'morning', 'of', 'she', 'will', 'follow', 'with', 'physiatrist', 'dr', 'allan', 'kofoed', 'complication', 'right', 'hand', 'weakness', 'numbness', 'and', 'coolness', 'status', 'post', 'av', 'fistula', 'surgery', 'possibly', 'secondary', 'to', 'steal', 'or', 'neurapraxia', 'significantly', 'improved', 'by', 'the', 'time', 'of', 'discharge', 'consultant', 'dr', 'garfield', 'kiehne', 'from', 'vascular', 'surgery', 'dr', 'ambrose', 'moldrem', 'from', 'gastroenterology', 'physical', 'examination', 'on', 'discharge', 'stable', 'vital', 'sign', 'lung', 'with', 'bibasilar', 'crackle', 'systolic', 'murmur', 'heard', 'throughout', 'plus', 'a', 'mechanical', 's', 'abdomen', 'benign', 'lower', 'extremity', 'with', 'chronic', 'venous', 'stasis', 'change', 'right', 'upper', 'extremity', 'av', 'fistula', 'with', 'thrill', 'decreased', 'right', 'radial', 'pulse', 'with', 'warm', 'hand', 'distally', 'and', 'strength', 'in', 'hand', 'grip', 'of', 'the', 'right', 'hand', 'discharge', 'medication', 'norvasc', 'mg', 'daily', 'caltrate', 'plus', 'd', 'one', 'tablet', 'po', 'bid', 'aranesp', 'mcg', 'subcu', 'weekly', 'lovenox', 'mg', 'subcu', 'daily', 'starting', 'on', 'thursday', 'to', 'be', 'discontinued', 'when', 'patient', 'inr', 'is', 'therapeutic', 'on', 'coumadin', 'nexium', 'mg', 'po', 'bid', 'ferrous', 'sulfate', 'mg', 'po', 'bid', 'lasix', 'mg', 'po', 'daily', 'insulin', 'aspart', 'unit', 'subcu', 'every', 'meal', 'insulin', 'lantus', 'unit', 'subcu', 'qpm', 'imdur', 'mg', 'po', 'bid', 'sevelamer', 'mg', 'po', 'qac', 'multivitamin', 'one', 'tablet', 'daily', 'valsartan', 'mg', 'po', 'daily', 'and', 'coumadin', 'mg', 'po', 'qpm', 'starting', 'on', 'thursday', 'disposition', 'to', 'home', 'with', 'service', 'followup', 'appointment', 'the', 'patient', 'will', 'follow', 'up', 'with', 'dr', 'brendan', 'b', 'tordsen', 'in', 'one', 'to', 'two', 'week', 'with', 'dr', 'schaetzle', 'from', 'vascular', 'surgery', 'on', 'at', 'am', 'with', 'dr', 'salvatore', 'sherrod', 'from', 'physiatry', 'on', 'at', 'am', 'and', 'dr', 'margarito', 'nolting', 'on', 'at', 'am', 'code', 'status', 'the', 'patient', 'is', 'full', 'code', 'and', 'her', 'healthcare', 'proxy', 'is', 'her', 'daughter', 'shane', 'lutao', 'primary', 'care', 'physician', 'rufus', 'mannheimer', 'md', 'escription', 'document', 'cssten', 'tel', 'dictated', 'by', 'beier', 'julio', 'attending', 'hambric', 'margarito', 'kurt', 'dictation', 'id', 'd', 't']\n"]}],"source":["print(train_df['tok_lem_text'][0])"]}],"metadata":{"colab":{"provenance":[{"file_id":"10pK5od01jfTHJyLN94dJxEube3sJszFm","timestamp":1678482671183}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
