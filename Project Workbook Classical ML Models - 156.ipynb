{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d1ehXFWM5aE"
   },
   "source": [
    "This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n",
    "***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv360l2IkNfO"
   },
   "source": [
    "The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1250,
     "status": "ok",
     "timestamp": 1679769727418,
     "user": {
      "displayName": "Matthew Lopes",
      "userId": "01980291092524472313"
     },
     "user_tz": 240
    },
    "id": "zyXrAo2dsJqf",
    "outputId": "0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = './obesity_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - All Features**\n",
    "\n",
    "![CML TFIDF All](images\\cml-tfidf-all.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - ExtraTreesClassifier Features**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-tfidf-extra.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - InfoGain Features**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-tfidf-infogain.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - SelectKBest Features**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-tfidf-selectkbest.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - Word Embeddings - No Stopwords**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-we-swno.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - Word Embeddings - Stopwords**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-we-swyes.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import torchtext\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection, svm, naive_bayes\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# define data path\n",
    "DATA_PATH = './obesity_data/'\n",
    "RESULTS_PATH = './results/'\n",
    "MODELS_PATH = './models/'\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "#Download info for USE\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "all_docs_df = pd.read_pickle(DATA_PATH + '/alldocs_df.pkl')\n",
    "all_docs_df_ns = pd.read_pickle(DATA_PATH + '/alldocs_df_ns.pkl')\n",
    "all_annot_df = pd.read_pickle(DATA_PATH + '/allannot_df.pkl')\n",
    "\n",
    "#all_df  = pd.read_pickle(DATA_PATH + '/all_df.pkl') \n",
    "all_df_expanded = pd.read_pickle(DATA_PATH + '/all_df_expanded.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.merge(all_docs_df,all_annot_df, on='id')\n",
    "all_df_ns = pd.merge(all_docs_df_ns,all_annot_df, on='id')\n",
    "\n",
    "disease_list = all_df['disease'].unique().tolist()\n",
    "feature_list = ['All','ExtraTreeClassifier','SelectKBest','InfoGainAttributeVal']\n",
    "embedding_list = ['GloVe', 'FastText', 'USE']\n",
    "device = torch.device('cpu')\n",
    "\n",
    "#print(disease_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(all_df['tok_lem_text']):\n",
    "    Final_words = []\n",
    "    for word in entry:\n",
    "        Final_words.append(word)\n",
    "    all_df.loc[index, 'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(all_df_ns['tok_lem_text']):\n",
    "    Final_words = []\n",
    "    for word in entry:\n",
    "        Final_words.append(word)\n",
    "    all_df_ns.loc[index, 'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_df = all_df[all_df['disease'] == 'CHF']\n",
    "#print(all_df['tok_lem_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(df):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['text_final'], df['judgment'], test_size=0.20, shuffle=True)\n",
    "    X_train_ns, X_test_ns, y_train_ns, y_test_ns = train_test_split(df['text_final'], df['judgment'], test_size=0.20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "\n",
    "Train_Y  = Encoder.fit_transform(y_train)\n",
    "Test_Y  = Encoder.fit_transform(y_test)\n",
    "\n",
    "Train_Y_NS  = Encoder.fit_transform(y_train_ns)\n",
    "Test_Y_NS = Encoder.fit_transform(y_test_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=600)\n",
    "Tfidf_vect_NS = TfidfVectorizer(max_features = 600, stop_words = cachedStopWords)\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.fit_transform(X_train)\n",
    "Test_X_Tfidf = Tfidf_vect.fit_transform(X_test)\n",
    "\n",
    "Train_X_Tfidf_NS = Tfidf_vect_NS.fit_transform(X_train_ns)\n",
    "Test_X_Tfidf_NS = Tfidf_vect_NS.fit_transform(X_test_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "\n",
    "def getVocab(X_train, y_train, feature, max_tokens):\n",
    " \n",
    "    ## Step 1: Determine the Initial Vocabulary\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab = list(tokenizer.word_index.keys())\n",
    "\n",
    "    ## Step 2: Create term  matrix\n",
    "    vectors = tokenizer.texts_to_matrix(X_train, mode='count')\n",
    "\n",
    "    ## Do feature selection on term matrix (column headers are words)\n",
    "    X = vectors\n",
    "    y = y_train\n",
    "\n",
    "    ##Choose algorithm\n",
    "    if feature == 'SelectKBest':\n",
    "        selector = SelectKBest(score_func=f_classif, k=max_tokens).fit(X,y)\n",
    "    else: \n",
    "        if feature == 'InfoGainAttributeVal':\n",
    "            #This should be similar to the InfoGain?\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=max_tokens).fit(X,y)\n",
    "        else:\n",
    "            #default to ExtraTreeClassifier\n",
    "            estimator = ExtraTreeClassifier(random_state = seed)\n",
    "            #selector = SelectFromModel(estimator, max_features = tokens,threshold=-np.inf)\n",
    "            selector = SelectFromModel(estimator, max_features = max_tokens)\n",
    "            selector = selector.fit(X, y)\n",
    "\n",
    "    support_idx = selector.get_support(True)\n",
    "    \n",
    "    #print(\"Vocab:\", [vocab[i-1].replace(\"'\",\"\") for i in support_idx])\n",
    "    tokenizer2 = Tokenizer()\n",
    "    tokenizer2.fit_on_texts([vocab[i-1].replace(\"'\",\"\") for i in support_idx])\n",
    "    new_vocab = list(tokenizer2.word_index.keys())\n",
    "\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM -  Asthma : f1-score 0.0\n",
      "SVM -  Asthma : f1-macro 0.4752475247524752\n",
      "SVM -  Asthma : f1-micro 0.9056603773584906\n",
      "KNN -  Asthma : f1-score 0.0\n",
      "KNN -  Asthma : f1-macro 0.47000000000000003\n",
      "KNN -  Asthma : f1-micro 0.8867924528301887\n",
      "NB -  Asthma : f1-score 0.0\n",
      "NB -  Asthma : f1-macro 0.4752475247524752\n",
      "NB -  Asthma : f1-micro 0.9056603773584906\n",
      "Random Forest Accuracy Score ->  90.56603773584906\n",
      "RF -  Asthma : f1-score 0.0\n",
      "RF -  Asthma : f1-macro 0.4752475247524752\n",
      "RF -  Asthma : f1-micro 0.9056603773584906\n",
      "SVM -  CHF : f1-score 0.6298342541436464\n",
      "SVM -  CHF : f1-macro 0.5075776775305387\n",
      "SVM -  CHF : f1-micro 0.5379310344827586\n",
      "KNN -  CHF : f1-score 0.7522935779816514\n",
      "KNN -  CHF : f1-macro 0.5011467889908257\n",
      "KNN -  CHF : f1-micro 0.6275862068965518\n",
      "NB -  CHF : f1-score 0.771186440677966\n",
      "NB -  CHF : f1-macro 0.385593220338983\n",
      "NB -  CHF : f1-micro 0.6275862068965518\n",
      "Random Forest Accuracy Score ->  64.13793103448275\n",
      "RF -  CHF : f1-score 0.7739130434782608\n",
      "RF -  CHF : f1-macro 0.4536231884057971\n",
      "RF -  CHF : f1-micro 0.6413793103448275\n",
      "SVM -  Depression : f1-score 0.0\n",
      "SVM -  Depression : f1-macro 0.4308510638297872\n",
      "SVM -  Depression : f1-micro 0.7570093457943925\n",
      "KNN -  Depression : f1-score 0.0634920634920635\n",
      "KNN -  Depression : f1-macro 0.45092411393781257\n",
      "KNN -  Depression : f1-micro 0.7242990654205608\n",
      "NB -  Depression : f1-score 0.0\n",
      "NB -  Depression : f1-macro 0.4308510638297872\n",
      "NB -  Depression : f1-micro 0.7570093457943925\n",
      "Random Forest Accuracy Score ->  75.70093457943925\n",
      "RF -  Depression : f1-score 0.0\n",
      "RF -  Depression : f1-macro 0.4308510638297872\n",
      "RF -  Depression : f1-micro 0.7570093457943925\n",
      "SVM -  Diabetes : f1-score 0.5106382978723405\n",
      "SVM -  Diabetes : f1-macro 0.45739168779627387\n",
      "SVM -  Diabetes : f1-micro 0.46261682242990654\n",
      "KNN -  Diabetes : f1-score 0.8267477203647418\n",
      "KNN -  Diabetes : f1-macro 0.625495072303583\n",
      "KNN -  Diabetes : f1-micro 0.7336448598130842\n",
      "NB -  Diabetes : f1-score 0.8432432432432432\n",
      "NB -  Diabetes : f1-macro 0.4216216216216216\n",
      "NB -  Diabetes : f1-micro 0.7289719626168223\n",
      "Random Forest Accuracy Score ->  75.23364485981308\n",
      "RF -  Diabetes : f1-score 0.8555858310626703\n",
      "RF -  Diabetes : f1-macro 0.49336668602313843\n",
      "RF -  Diabetes : f1-micro 0.7523364485981309\n",
      "SVM -  Gallstones : f1-score 0.0\n",
      "SVM -  Gallstones : f1-macro 0.4472361809045226\n",
      "SVM -  Gallstones : f1-micro 0.8090909090909091\n",
      "KNN -  Gallstones : f1-score 0.0\n",
      "KNN -  Gallstones : f1-macro 0.4472361809045226\n",
      "KNN -  Gallstones : f1-micro 0.8090909090909091\n",
      "NB -  Gallstones : f1-score 0.0\n",
      "NB -  Gallstones : f1-macro 0.4472361809045226\n",
      "NB -  Gallstones : f1-micro 0.8090909090909091\n",
      "Random Forest Accuracy Score ->  80.9090909090909\n",
      "RF -  Gallstones : f1-score 0.0\n",
      "RF -  Gallstones : f1-macro 0.4472361809045226\n",
      "RF -  Gallstones : f1-micro 0.8090909090909091\n",
      "SVM -  Gout : f1-score 0.0\n",
      "SVM -  Gout : f1-macro 0.4674698795180723\n",
      "SVM -  Gout : f1-micro 0.8778280542986425\n",
      "KNN -  Gout : f1-score 0.0\n",
      "KNN -  Gout : f1-macro 0.4674698795180723\n",
      "KNN -  Gout : f1-micro 0.8778280542986425\n",
      "NB -  Gout : f1-score 0.0\n",
      "NB -  Gout : f1-macro 0.4674698795180723\n",
      "NB -  Gout : f1-micro 0.8778280542986425\n",
      "Random Forest Accuracy Score ->  87.78280542986425\n",
      "RF -  Gout : f1-score 0.0\n",
      "RF -  Gout : f1-macro 0.4674698795180723\n",
      "RF -  Gout : f1-micro 0.8778280542986425\n",
      "SVM -  Hypercholesterolemia : f1-score 0.4645161290322581\n",
      "SVM -  Hypercholesterolemia : f1-macro 0.5526043848624493\n",
      "SVM -  Hypercholesterolemia : f1-micro 0.5699481865284974\n",
      "KNN -  Hypercholesterolemia : f1-score 0.6861313868613139\n",
      "KNN -  Hypercholesterolemia : f1-macro 0.45913712200208556\n",
      "KNN -  Hypercholesterolemia : f1-micro 0.5544041450777202\n",
      "NB -  Hypercholesterolemia : f1-score 0.6785714285714285\n",
      "NB -  Hypercholesterolemia : f1-macro 0.6170634920634921\n",
      "NB -  Hypercholesterolemia : f1-micro 0.6269430051813472\n",
      "Random Forest Accuracy Score ->  63.212435233160626\n",
      "RF -  Hypercholesterolemia : f1-score 0.6978723404255319\n",
      "RF -  Hypercholesterolemia : f1-macro 0.6138368324644216\n",
      "RF -  Hypercholesterolemia : f1-micro 0.6321243523316062\n",
      "SVM -  Hypertriglyceridemia : f1-score 0.0\n",
      "SVM -  Hypertriglyceridemia : f1-macro 0.48325358851674644\n",
      "SVM -  Hypertriglyceridemia : f1-micro 0.9351851851851852\n",
      "KNN -  Hypertriglyceridemia : f1-score 0.0\n",
      "KNN -  Hypertriglyceridemia : f1-macro 0.48325358851674644\n",
      "KNN -  Hypertriglyceridemia : f1-micro 0.9351851851851852\n",
      "NB -  Hypertriglyceridemia : f1-score 0.0\n",
      "NB -  Hypertriglyceridemia : f1-macro 0.48325358851674644\n",
      "NB -  Hypertriglyceridemia : f1-micro 0.9351851851851852\n",
      "Random Forest Accuracy Score ->  93.51851851851852\n",
      "RF -  Hypertriglyceridemia : f1-score 0.0\n",
      "RF -  Hypertriglyceridemia : f1-macro 0.48325358851674644\n",
      "RF -  Hypertriglyceridemia : f1-micro 0.9351851851851852\n",
      "SVM -  OA : f1-score 0.0\n",
      "SVM -  OA : f1-macro 0.435483870967742\n",
      "SVM -  OA : f1-micro 0.7714285714285715\n",
      "KNN -  OA : f1-score 0.038461538461538464\n",
      "KNN -  OA : f1-macro 0.4512959866220736\n",
      "KNN -  OA : f1-micro 0.7619047619047619\n",
      "NB -  OA : f1-score 0.0\n",
      "NB -  OA : f1-macro 0.435483870967742\n",
      "NB -  OA : f1-micro 0.7714285714285715\n",
      "Random Forest Accuracy Score ->  77.14285714285715\n",
      "RF -  OA : f1-score 0.0\n",
      "RF -  OA : f1-macro 0.435483870967742\n",
      "RF -  OA : f1-micro 0.7714285714285715\n",
      "SVM -  OSA : f1-score 0.0\n",
      "SVM -  OSA : f1-macro 0.4632352941176471\n",
      "SVM -  OSA : f1-micro 0.863013698630137\n",
      "KNN -  OSA : f1-score 0.06666666666666667\n",
      "KNN -  OSA : f1-macro 0.49901960784313726\n",
      "KNN -  OSA : f1-micro 0.8721461187214612\n",
      "NB -  OSA : f1-score 0.0\n",
      "NB -  OSA : f1-macro 0.46454767726161367\n",
      "NB -  OSA : f1-micro 0.867579908675799\n",
      "Random Forest Accuracy Score ->  86.7579908675799\n",
      "RF -  OSA : f1-score 0.0\n",
      "RF -  OSA : f1-macro 0.46454767726161367\n",
      "RF -  OSA : f1-micro 0.867579908675799\n",
      "SVM -  Obesity : f1-score 0.5756457564575646\n",
      "SVM -  Obesity : f1-macro 0.3912711540908512\n",
      "SVM -  Obesity : f1-micro 0.44711538461538464\n",
      "KNN -  Obesity : f1-score 0.4807692307692307\n",
      "KNN -  Obesity : f1-macro 0.4807692307692307\n",
      "KNN -  Obesity : f1-micro 0.4807692307692308\n",
      "NB -  Obesity : f1-score 0.5714285714285714\n",
      "NB -  Obesity : f1-macro 0.4786967418546365\n",
      "NB -  Obesity : f1-micro 0.4951923076923077\n",
      "Random Forest Accuracy Score ->  56.25\n",
      "RF -  Obesity : f1-score 0.5955555555555555\n",
      "RF -  Obesity : f1-macro 0.5595578824898196\n",
      "RF -  Obesity : f1-micro 0.5625\n",
      "SVM -  CAD : f1-score 0.7295081967213116\n",
      "SVM -  CAD : f1-macro 0.6750989259468627\n",
      "SVM -  CAD : f1-micro 0.6842105263157895\n",
      "KNN -  CAD : f1-score 0.6844106463878327\n",
      "KNN -  CAD : f1-macro 0.5744633877100453\n",
      "KNN -  CAD : f1-micro 0.6028708133971292\n",
      "NB -  CAD : f1-score 0.7735849056603773\n",
      "NB -  CAD : f1-macro 0.5267924528301886\n",
      "NB -  CAD : f1-micro 0.6555023923444976\n",
      "Random Forest Accuracy Score ->  77.99043062200957\n",
      "RF -  CAD : f1-score 0.8391608391608391\n",
      "RF -  CAD : f1-macro 0.7453379953379953\n",
      "RF -  CAD : f1-micro 0.7799043062200957\n",
      "SVM -  Hypertension : f1-score 0.8877005347593583\n",
      "SVM -  Hypertension : f1-macro 0.44385026737967914\n",
      "SVM -  Hypertension : f1-micro 0.7980769230769231\n",
      "KNN -  Hypertension : f1-score 0.859504132231405\n",
      "KNN -  Hypertension : f1-macro 0.4486199906440044\n",
      "KNN -  Hypertension : f1-micro 0.7548076923076922\n",
      "NB -  Hypertension : f1-score 0.8877005347593583\n",
      "NB -  Hypertension : f1-macro 0.44385026737967914\n",
      "NB -  Hypertension : f1-micro 0.7980769230769231\n",
      "Random Forest Accuracy Score ->  79.8076923076923\n",
      "RF -  Hypertension : f1-score 0.8877005347593583\n",
      "RF -  Hypertension : f1-macro 0.44385026737967914\n",
      "RF -  Hypertension : f1-micro 0.7980769230769231\n",
      "SVM -  PVD : f1-score 0.0\n",
      "SVM -  PVD : f1-macro 0.44623655913978494\n",
      "SVM -  PVD : f1-micro 0.8058252427184465\n",
      "KNN -  PVD : f1-score 0.0\n",
      "KNN -  PVD : f1-macro 0.44623655913978494\n",
      "KNN -  PVD : f1-micro 0.8058252427184465\n",
      "NB -  PVD : f1-score 0.0\n",
      "NB -  PVD : f1-macro 0.44623655913978494\n",
      "NB -  PVD : f1-micro 0.8058252427184465\n",
      "Random Forest Accuracy Score ->  80.58252427184466\n",
      "RF -  PVD : f1-score 0.0\n",
      "RF -  PVD : f1-macro 0.44623655913978494\n",
      "RF -  PVD : f1-micro 0.8058252427184465\n",
      "SVM -  Venous Insufficiency : f1-score 0.0\n",
      "SVM -  Venous Insufficiency : f1-macro 0.47956403269754766\n",
      "SVM -  Venous Insufficiency : f1-micro 0.9214659685863874\n",
      "KNN -  Venous Insufficiency : f1-score 0.0\n",
      "KNN -  Venous Insufficiency : f1-macro 0.47956403269754766\n",
      "KNN -  Venous Insufficiency : f1-micro 0.9214659685863874\n",
      "NB -  Venous Insufficiency : f1-score 0.0\n",
      "NB -  Venous Insufficiency : f1-macro 0.47956403269754766\n",
      "NB -  Venous Insufficiency : f1-micro 0.9214659685863874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy Score ->  92.14659685863874\n",
      "RF -  Venous Insufficiency : f1-score 0.0\n",
      "RF -  Venous Insufficiency : f1-macro 0.47956403269754766\n",
      "RF -  Venous Insufficiency : f1-micro 0.9214659685863874\n",
      "SVM -  GERD : f1-score 0.0\n",
      "SVM -  GERD : f1-macro 0.42901234567901236\n",
      "SVM -  GERD : f1-micro 0.7513513513513513\n",
      "KNN -  GERD : f1-score 0.0\n",
      "KNN -  GERD : f1-macro 0.4254658385093168\n",
      "KNN -  GERD : f1-micro 0.7405405405405405\n",
      "NB -  GERD : f1-score 0.0\n",
      "NB -  GERD : f1-macro 0.42901234567901236\n",
      "NB -  GERD : f1-micro 0.7513513513513513\n",
      "Random Forest Accuracy Score ->  75.13513513513513\n",
      "RF -  GERD : f1-score 0.0\n",
      "RF -  GERD : f1-macro 0.42901234567901236\n",
      "RF -  GERD : f1-micro 0.7513513513513513\n"
     ]
    }
   ],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Tfidf_vect = TfidfVectorizer(max_features=600)\n",
    "Tfidf_vect_NS = TfidfVectorizer(max_features = 600, stop_words = cachedStopWords)\n",
    "\n",
    "for _,disease in enumerate(disease_list):\n",
    "    disease_data_df = all_df[all_df['disease'] == disease]\n",
    "    disease_data_ns_df = all_df_ns[all_df_ns['disease'] == disease]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(disease_data_df['text_final'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "    X_train_ns, X_test_ns, y_train_ns, y_test_ns = train_test_split(disease_data_ns_df['text_final'], disease_data_ns_df['judgment'], test_size=0.20, shuffle=True)\n",
    "    \n",
    "    Train_Y  = Encoder.fit_transform(y_train)\n",
    "    Test_Y  = Encoder.fit_transform(y_test)\n",
    "\n",
    "    Train_Y_NS  = Encoder.fit_transform(y_train_ns)\n",
    "    Test_Y_NS = Encoder.fit_transform(y_test_ns)\n",
    "    \n",
    "    Train_X_Tfidf = Tfidf_vect.fit_transform(X_train)\n",
    "    Test_X_Tfidf = Tfidf_vect.fit_transform(X_test)\n",
    "\n",
    "    Train_X_Tfidf_NS = Tfidf_vect_NS.fit_transform(X_train_ns)\n",
    "    Test_X_Tfidf_NS = Tfidf_vect_NS.fit_transform(X_test_ns)\n",
    "    \n",
    "    \n",
    "    #SVM\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf, y_train)\n",
    "    \n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_SVM)\n",
    "    f1_macro = f1_score(y_test, predictions_SVM,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_SVM,average='micro')\n",
    "\n",
    "    #print(\"SVM - \", disease, \": f1-score\", f1)\n",
    "    print(\"SVM - \", disease, \": f1-macro\", f1_macro)\n",
    "    print(\"SVM - \", disease, \": f1-micro\", f1_micro)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #KNN\n",
    "    # fit the training dataset on the KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=7)\n",
    "    clf = knn.fit(Train_X_Tfidf, y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_KNN = clf.predict(Test_X_Tfidf)\n",
    "\n",
    "    #auroc = roc_auc_score(truth, pred[:,1])\n",
    "    #f1 = f1_score(y_test, predictions_KNN)\n",
    "    f1_macro = f1_score(y_test, predictions_KNN,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_KNN,average='micro')\n",
    "\n",
    "    #print(\"KNN - \", disease, \": f1-score\", f1)\n",
    "    print(\"KNN - \", disease, \": f1-macro\", f1_macro)\n",
    "    print(\"KNN - \", disease, \": f1-micro\", f1_micro)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Naive Bayes\n",
    "    # fit the training dataset on the NB classifier\n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    Naive.fit(Train_X_Tfidf,y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_NB)\n",
    "    f1_macro = f1_score(y_test, predictions_NB,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_NB,average='micro')\n",
    "\n",
    "    #print(\"NB - \", disease, \": f1-score\", f1)\n",
    "    print(\"NB - \", disease, \": f1-macro\", f1_macro)\n",
    "    print(\"NB - \", disease, \": f1-micro\", f1_micro)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #RF\n",
    "    # fit the training dataset on the RF classifier\n",
    "    classifier=RandomForestClassifier(n_estimators =400,criterion=\"entropy\",random_state =0)\n",
    "    classifier.fit(Train_X_Tfidf,y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_RF = classifier.predict(Test_X_Tfidf)\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_RF)\n",
    "    f1_macro = f1_score(y_test, predictions_RF,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_RF,average='micro')\n",
    "\n",
    "    #print(\"RF - \", disease, \": f1-score\", f1)\n",
    "    print(\"RF - \", disease, \": f1-macro\", f1_macro)\n",
    "    print(\"RF - \", disease, \": f1-micro\", f1_micro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://link.springer.com/article/10.1007/BF00994018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m estimator \u001b[38;5;241m=\u001b[39m ExtraTreesClassifier()\n\u001b[0;32m      5\u001b[0m selector \u001b[38;5;241m=\u001b[39m RFECV(estimator, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m selector \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrain_X_Tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(selector)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# fit the training dataset on the SVM classifier\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:710\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    707\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    708\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[1;32m--> 710\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    715\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[0;32m    716\u001b[0m scores_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:711\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    707\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    708\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[0;32m    710\u001b[0m scores \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m--> 711\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    713\u001b[0m )\n\u001b[0;32m    715\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[0;32m    716\u001b[0m scores_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:37\u001b[0m, in \u001b[0;36m_rfe_single_fit\u001b[1;34m(rfe, estimator, X, y, train, test, scorer)\u001b[0m\n\u001b[0;32m     35\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n\u001b[0;32m     36\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, test, train)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mscores_\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:283\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[1;32m--> 283\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[0;32m    286\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[0;32m    287\u001b[0m     estimator,\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[0;32m    289\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    290\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    439\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    442\u001b[0m ]\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:187\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:937\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m ):\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    411\u001b[0m         splitter,\n\u001b[0;32m    412\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[1;32m--> 420\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "#estimator = ExtraTreesClassifier()\n",
    "#selector = RFECV(estimator, step=1, cv=5)\n",
    "#selector = selector.fit(Train_X_Tfidf, y_train)\n",
    "\n",
    "#print(selector)\n",
    "\n",
    "# fit the training dataset on the SVM classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf, y_train)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy: \",accuracy_score(predictions_SVM, y_test)*100)\n",
    "\n",
    "f1 = f1_score(y_test, predictions_SVM)\n",
    "f1_macro = f1_score(y_test, predictions_SVM,average='macro')\n",
    "f1_micro = f1_score(y_test, predictions_SVM,average='micro')\n",
    "\n",
    "print(f1)\n",
    "print(f1_macro)\n",
    "print(f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the SVM classifier\n",
    "SVM_NS = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM_NS.fit(Train_X_Tfidf_NS, y_train_ns)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM_NS = SVM.predict(Test_X_Tfidf_NS)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM NS Accuracy: \",accuracy_score(predictions_SVM_NS, y_test_ns)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbours (kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://link.springer.com/article/10.1007/BF00153759"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN Accuracy:  64.82758620689654\n",
      "0.7733333333333333\n",
      "0.49435897435897436\n",
      "0.6482758620689655\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "clf = knn.fit(Train_X_Tfidf, y_train)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_KNN = clf.predict(Test_X_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"kNN Accuracy: \",accuracy_score(predictions_KNN, y_test)*100)\n",
    "\n",
    "#print(predictions_KNN)\n",
    "\n",
    "#auroc = roc_auc_score(truth, pred[:,1])\n",
    "f1 = f1_score(y_test, predictions_KNN)\n",
    "f1_macro = f1_score(y_test, predictions_KNN,average='macro')\n",
    "f1_micro = f1_score(y_test, predictions_KNN,average='micro')\n",
    "\n",
    "print(f1)\n",
    "print(f1_macro)\n",
    "print(f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the KNN classifier\n",
    "knn_ns = KNeighborsClassifier(n_neighbors=7)\n",
    "clf_ns = knn_ns.fit(Train_X_Tfidf_NS, y_train_ns)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_KNN_NS = clf_ns.predict(Test_X_Tfidf_NS)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"kNN NS Accuracy: \",accuracy_score(predictions_KNN_NS, y_test_ns)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1302.4964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  68.96551724137932\n",
      "0.8034934497816593\n",
      "0.5328942658744362\n",
      "0.6896551724137931\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,y_train)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, y_test)*100)\n",
    "\n",
    "f1 = f1_score(y_test, predictions_NB)\n",
    "f1_macro = f1_score(y_test, predictions_NB,average='macro')\n",
    "f1_micro = f1_score(y_test, predictions_NB,average='micro')\n",
    "\n",
    "print(f1)\n",
    "print(f1_macro)\n",
    "print(f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive_NS = naive_bayes.MultinomialNB()\n",
    "Naive_NS.fit(Train_X_Tfidf_NS, y_train_ns)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB_NS = Naive.predict(Test_X_Tfidf_NS)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes NS Accuracy Score -> \",accuracy_score(predictions_NB_NS, y_test_ns)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://link.springer.com/article/10.1023/A:1010933404324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy Score ->  73.79310344827587\n",
      "0.8362068965517241\n",
      "0.5905172413793103\n",
      "0.7379310344827587\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the RF classifier\n",
    "classifier=RandomForestClassifier(n_estimators =400,criterion=\"entropy\",random_state =0)\n",
    "classifier.fit(Train_X_Tfidf,y_train)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_RF = classifier.predict(Test_X_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Random Forest Accuracy Score -> \",accuracy_score(predictions_RF, y_test)*100)\n",
    "\n",
    "f1 = f1_score(y_test, predictions_RF)\n",
    "f1_macro = f1_score(y_test, predictions_RF,average='macro')\n",
    "f1_micro = f1_score(y_test, predictions_RF,average='micro')\n",
    "\n",
    "print(f1)\n",
    "print(f1_macro)\n",
    "print(f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the RF classifier\n",
    "classifier_ns = RandomForestClassifier(n_estimators = 400, criterion = \"entropy\", random_state = 0)\n",
    "classifier_ns.fit(Train_X_Tfidf_NS, y_train_ns)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_RF_NS = classifier_ns.predict(Test_X_Tfidf_NS)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Random Forest NS Accuracy Score -> \",accuracy_score(predictions_RF_NS, y_test_ns)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://onlinelibrary.wiley.com/doi/10.1002/rsa.3240050207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performSVM(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature):\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf, y_train)\n",
    "\n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_SVM)\n",
    "    f1_macro = f1_score(y_test, predictions_SVM,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_SVM,average='micro')\n",
    "\n",
    "    #print(\"SVM - \", disease, \": f1-score\", f1)\n",
    "    print(\"SVM - \", feature, disease, \": f1-macro\", f1_macro)\n",
    "    print(\"SVM - \", feature, disease, \": f1-micro\", f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performKNN(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature):\n",
    "    knn = KNeighborsClassifier(n_neighbors=7)\n",
    "    clf = knn.fit(Train_X_Tfidf, y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_KNN = clf.predict(Test_X_Tfidf)\n",
    "\n",
    "    #auroc = roc_auc_score(truth, pred[:,1])\n",
    "    #f1 = f1_score(y_test, predictions_KNN)\n",
    "    f1_macro = f1_score(y_test, predictions_KNN,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_KNN,average='micro')\n",
    "\n",
    "    #print(\"KNN - \", disease, \": f1-score\", f1)\n",
    "    print(\"KNN - \", disease, \": f1-macro\", f1_macro)\n",
    "    print(\"KNN - \", disease, \": f1-micro\", f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performNB(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature):\n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    Naive.fit(Train_X_Tfidf,y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_NB)\n",
    "    f1_macro = f1_score(y_test, predictions_NB,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_NB,average='micro')\n",
    "\n",
    "    #print(\"NB - \", disease, \": f1-score\", f1)\n",
    "    print(\"NB - \", disease, \": f1-macro\", f1_macro)\n",
    "    print(\"NB - \", disease, \": f1-micro\", f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performRF(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature):\n",
    "    classifier=RandomForestClassifier(n_estimators =400,criterion=\"entropy\",random_state =0)\n",
    "    classifier.fit(Train_X_Tfidf,y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_RF = classifier.predict(Test_X_Tfidf)\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_RF)\n",
    "    f1_macro = f1_score(y_test, predictions_RF,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_RF,average='micro')\n",
    "\n",
    "    #print(\"RF - \", disease, \": f1-score\", f1)\n",
    "    print(\"RF - \", disease, \": f1-macro\", f1_macro)\n",
    "    print(\"RF - \", disease, \": f1-micro\", f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch_GloVe(X_Train):\n",
    "    embedding_size_used = 300\n",
    "    vec = torchtext.vocab.GloVe(name='6B', dim=embedding_size_used)    \n",
    "    #Xi, Yi = batch[0]\n",
    "    #batch_size = len(batch)\n",
    "\n",
    "    #X = torch.zeros(batch_size, len(Xi), embedding_size_used, dtype=torch.float)\n",
    "    #Y = torch.zeros((batch_size), dtype=torch.long)\n",
    "    \n",
    "    X = [0] * X_Train.shape[0]\n",
    "    \n",
    "    for i in range(len(X_Train)):\n",
    "        #x, y = batch[i]\n",
    "        #vectors = vec.get_vecs_by_tokens(voc.lookup_tokens(x.tolist()))\n",
    "        vectors = vec.get_vecs_by_tokens(X_Train.iloc[i]).float().numpy()\n",
    "        \n",
    "        #print(vectors.shape)\n",
    "        \n",
    "        #print(\"NONFLATTENED: \", vectors.shape)\n",
    "        #print(\"FLATTENED: \", vectors.flatten().shape)\n",
    "\n",
    "        X[i] = vectors.flatten()\n",
    "        #Y[i] = torch.tensor(float(y == True))\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch_FastText(batch):\n",
    "    embedding_size_used = 300\n",
    "    vec = torchtext.vocab.FastText()\n",
    "    Xi, Yi = batch[0]\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    X = torch.zeros(batch_size, len(Xi), embedding_size_used, dtype=torch.float)\n",
    "    Y = torch.zeros((batch_size), dtype=torch.long)\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        x, y = batch[i]\n",
    "        #vectors = vec.get_vecs_by_tokens(voc.lookup_tokens(x.tolist()))\n",
    "        vectors = vec.get_vecs_by_tokens(x)\n",
    "\n",
    "        X[i] = vectors.float()\n",
    "        Y[i] = torch.tensor(float(y == True))\n",
    "\n",
    "    return X,Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch_USE(batch):\n",
    "    embedding_size_used = 512\n",
    "\n",
    "    Xi, Yi = batch[0]\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    X = torch.zeros(batch_size, len(Xi), embedding_size_used, dtype=torch.float)\n",
    "    Y = torch.zeros((batch_size), dtype=torch.long)\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        x, y = batch[i]\n",
    "        \n",
    "        tensor_flow_vectors = embed(x)\n",
    "        array_vectors = tensor_flow_vectors.numpy()\n",
    "\n",
    "        X[i] = torch.tensor(array_vectors).float()\n",
    "        Y[i] = torch.tensor(float(y == True))\n",
    "\n",
    "    return X,Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateXTrainAndTest(X_train, X_test, Tfidf_vect):\n",
    "    X_train_values_list = Tfidf_vect.fit_transform(X_train).toarray()\n",
    "    X_training = pd.DataFrame(X_train_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "    X_training = np.asarray(X_training, dtype=float)\n",
    "    X_training = torch.from_numpy(X_training).to(device)\n",
    "\n",
    "    X_test_values_list = Tfidf_vect.transform(X_test).toarray()\n",
    "    X_testing = pd.DataFrame(X_test_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "    X_testing = np.asarray(X_testing, dtype=float)\n",
    "    X_testing = torch.from_numpy(X_testing).to(device)  \n",
    "    \n",
    "    return X_training, X_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "#from nltk.tokenize import WhitespaceTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "def getVocab(X_train, y_train, feature, max_tokens):\n",
    "    \n",
    "    #print(X_train)\n",
    " \n",
    "    ## Step 1: Determine the Initial Vocabulary\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab = list(tokenizer.word_index.keys())\n",
    "    \n",
    "    #tokenized = word_tokenize(X_train)\n",
    "    #span_generator = WhitespaceTokenizer().span_tokenize(X_train)\n",
    "    #spans = [span for span in span_generator]\n",
    "    #count = Counter(sum([list(x) for x in X_train], []))\n",
    "    #test = pd.DataFrame (X_train)\n",
    "    \n",
    "    #print(test)\n",
    "    #print(spans)\n",
    "    #print(count)\n",
    "\n",
    "    ## Step 2: Create term  matrix\n",
    "    vectors = tokenizer.texts_to_matrix(X_train, mode='count')\n",
    "\n",
    "    ## Do feature selection on term matrix (column headers are words)\n",
    "    X = vectors\n",
    "    #X = X_train\n",
    "    y = y_train\n",
    "    \n",
    "    #print(X.shape)\n",
    "    #print(y.shape)\n",
    "\n",
    "    ##Choose algorithm\n",
    "    if feature == 'SelectKBest':\n",
    "        selector = SelectKBest(score_func=f_classif, k=max_tokens).fit(X,y)\n",
    "    else: \n",
    "        if feature == 'InfoGainAttributeVal':\n",
    "            #This should be similar to the InfoGain?\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=max_tokens).fit(X,y)\n",
    "        else:\n",
    "            #default to ExtraTreeClassifier\n",
    "            estimator = ExtraTreeClassifier(random_state = seed)\n",
    "            #selector = SelectFromModel(estimator, max_features = tokens,threshold=-np.inf)\n",
    "            selector = SelectFromModel(estimator, max_features = max_tokens)\n",
    "            selector = selector.fit(X, y)\n",
    "\n",
    "    support_idx = selector.get_support(True)\n",
    "    \n",
    "    #print(\"Vocab:\", [vocab[i-1].replace(\"'\",\"\") for i in support_idx])\n",
    "    tokenizer2 = Tokenizer()\n",
    "    tokenizer2.fit_on_texts([vocab[i-1].replace(\"'\",\"\") for i in support_idx])\n",
    "    new_vocab = list(tokenizer2.word_index.keys())\n",
    "\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text_final'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text_final'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m disease_data_df \u001b[38;5;241m=\u001b[39m all_df[all_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisease\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m disease]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#disease_data_ns_df = all_df_ns[all_df_ns['disease'] == disease]\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mdisease_data_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_final\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, disease_data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjudgment\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.20\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#X_train_ns, X_test_ns, y_train_ns, y_test_ns = train_test_split(disease_data_ns_df['text_final'], disease_data_ns_df['judgment'], test_size=0.20, shuffle=True)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#print(X_train)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text_final'"
     ]
    }
   ],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Tfidf_vect = TfidfVectorizer(max_features=600)\n",
    "Tfidf_vect_NS = TfidfVectorizer(max_features = 600, stop_words = cachedStopWords)\n",
    "\n",
    "max_tokens = 600\n",
    "\n",
    "for _,disease in enumerate(disease_list):\n",
    "    for _,feature in enumerate(feature_list):\n",
    "        disease_data_df = all_df[all_df['disease'] == disease]\n",
    "        #disease_data_ns_df = all_df_ns[all_df_ns['disease'] == disease]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(disease_data_df['text_final'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "        #X_train_ns, X_test_ns, y_train_ns, y_test_ns = train_test_split(disease_data_ns_df['text_final'], disease_data_ns_df['judgment'], test_size=0.20, shuffle=True)\n",
    "    \n",
    "        #print(X_train)\n",
    "        \n",
    "        if feature != 'All':\n",
    "            vocab = getVocab(X_train,y_train, feature, max_tokens)\n",
    "            Tfidf_vect = TfidfVectorizer(max_features=max_tokens,vocabulary = vocab)\n",
    "        else:\n",
    "            Tfidf_vect = TfidfVectorizer(max_features=max_tokens)\n",
    "  \n",
    "        \n",
    "        X_train_values_list = Tfidf_vect.fit_transform(X_train).toarray()\n",
    "        X_training = pd.DataFrame(X_train_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "        X_training = np.asarray(X_training, dtype=float)\n",
    "        X_training = torch.from_numpy(X_training).to(device)\n",
    "\n",
    "        X_test_values_list = Tfidf_vect.transform(X_test).toarray()\n",
    "        X_testing = pd.DataFrame(X_test_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "        X_testing = np.asarray(X_testing, dtype=float)\n",
    "        X_testing = torch.from_numpy(X_testing).to(device)\n",
    "        \n",
    "        tokens_to_use = X_training.shape[1]\n",
    "        \n",
    "        #print(X_training)\n",
    "        #print(X_testing)\n",
    "\n",
    "\n",
    "        Train_Y  = Encoder.fit_transform(y_train)\n",
    "        Test_Y  = Encoder.fit_transform(y_test)\n",
    "\n",
    "        #Train_Y_NS  = Encoder.fit_transform(y_train_ns)\n",
    "        #Test_Y_NS = Encoder.fit_transform(y_test_ns)\n",
    "\n",
    "        #Train_X_Tfidf = Tfidf_vect.fit_transform(X_train)\n",
    "        #Test_X_Tfidf = Tfidf_vect.fit_transform(X_test)\n",
    "\n",
    "        #Train_X_Tfidf_NS = Tfidf_vect_NS.fit_transform(X_train_ns)\n",
    "        #Test_X_Tfidf_NS = Tfidf_vect_NS.fit_transform(X_test_ns)\n",
    "\n",
    "        performSVM(X_training, X_testing, Train_Y, Test_Y, feature)\n",
    "\n",
    "        performKNN(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test)\n",
    "\n",
    "        performNB(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test)\n",
    "\n",
    "        performRF(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM -  GloVe Asthma : f1-macro 0.4618834080717488\n",
      "SVM -  GloVe Asthma : f1-micro 0.8583333333333333\n",
      "KNN -  Asthma : f1-macro 0.4618834080717488\n",
      "KNN -  Asthma : f1-micro 0.8583333333333333\n",
      "RF -  Asthma : f1-macro 0.4618834080717488\n",
      "RF -  Asthma : f1-micro 0.8583333333333333\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [79]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m Test_Y  \u001b[38;5;241m=\u001b[39m Encoder\u001b[38;5;241m.\u001b[39mfit_transform(y_test)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m#Train_Y_NS  = Encoder.fit_transform(y_train_ns)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#Test_Y_NS = Encoder.fit_transform(y_test_ns)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m#Train_X_Tfidf_NS = Tfidf_vect_NS.fit_transform(X_train_ns)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m#Test_X_Tfidf_NS = Tfidf_vect_NS.fit_transform(X_test_ns)\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[43mperformSVM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrain_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTest_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m performKNN(X_train, X_test, Train_Y, Test_Y, embedding)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#performNB(X_train, X_test, Train_Y, Test_Y, embedding)\u001b[39;00m\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mperformSVM\u001b[1;34m(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperformSVM\u001b[39m(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature):\n\u001b[0;32m      2\u001b[0m     SVM \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mSVC(C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mSVM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrain_X_Tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     predictions_SVM \u001b[38;5;241m=\u001b[39m SVM\u001b[38;5;241m.\u001b[39mpredict(Test_X_Tfidf)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#f1 = f1_score(y_test, predictions_SVM)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:190\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    188\u001b[0m     check_consistent_length(X, y)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_targets(y)\n\u001b[0;32m    201\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m    202\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[0;32m    203\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 964\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric)\n\u001b[0;32m    981\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    750\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:872\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;124;03m    Return the values as a NumPy array.\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;124;03m          dtype='datetime64[ns]')\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Tfidf_vect = TfidfVectorizer(max_features=600)\n",
    "Tfidf_vect_NS = TfidfVectorizer(max_features = 600, stop_words = cachedStopWords)\n",
    "\n",
    "max_tokens = 600\n",
    "\n",
    "for _,disease in enumerate(disease_list):\n",
    "    for _, embedding in enumerate(embedding_list):\n",
    "        disease_data_df = all_df_expanded [all_df_expanded ['disease'] == disease]\n",
    "        #disease_data_ns_df = all_df_ns[all_df_ns['disease'] == disease]\n",
    "        \n",
    "        #print(disease_data_df)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(disease_data_df['vector_tokenized'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "\n",
    "        if embedding == 'GloVe':\n",
    "            X_train = vectorize_batch_GloVe(X_train)\n",
    "            X_test = vectorize_batch_GloVe(X_test)\n",
    "            #dataformat = 'vector_tokenized'\n",
    "        if embedding == 'FastText':\n",
    "            custom_collate=vectorize_batch_FastText\n",
    "            dataformat = 'vector_tokenized'\n",
    "        if embedding == 'USE':\n",
    "            custom_collate=vectorize_batch_USE\n",
    "            dataformat = 'sentence_tokenized'\n",
    "\n",
    "        Train_Y  = Encoder.fit_transform(y_train)\n",
    "        Test_Y  = Encoder.fit_transform(y_test)\n",
    "\n",
    "        performSVM(X_train, X_test, Train_Y, Test_Y, embedding)\n",
    "\n",
    "        performKNN(X_train, X_test, Train_Y, Test_Y, embedding)\n",
    "\n",
    "        #performNB(X_train, X_test, Train_Y, Test_Y, embedding)\n",
    "\n",
    "        performRF(X_train, X_test, Train_Y, Test_Y, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10pK5od01jfTHJyLN94dJxEube3sJszFm",
     "timestamp": 1678482671183
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
