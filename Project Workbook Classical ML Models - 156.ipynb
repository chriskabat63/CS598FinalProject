{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3d1ehXFWM5aE"
   },
   "source": [
    "Title:  Project Workbook Classical ML Models\n",
    "\n",
    "Authors:  Matthew Lopes and Chris Kabat\n",
    "\n",
    "This notebook was created to train the Classical Machine Learning Models to support our CS 598 DLH project. The paper we have chosen for the reproducibility project is:\n",
    "***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n",
    "\n",
    "Abstract:  The main goal of the paper is to extract Morbidity from clinical notes.  The idea was to use a combination of classical and deep learning methods to determine the best approach for classifying these notes in one or more of 16 morbidity conditions.  These models used a combination of NLP techniques including embeddings and bag of words implementations.  It also measured the effect including of stop words.  Lastly, it used ensemble techniques to tie together a number of the classical and deep learning models to provide the most accurate results.\n",
    "\n",
    "The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub.\n",
    "\n",
    "We are only training models using data that includes stop words.  \n",
    "\n",
    "In this workbook, we are taking the following steps:\n",
    "\n",
    "* Run Bag of Word Models\n",
    "* Run Embedding Models\n",
    "\n",
    " First we load the required libraries and retrieve our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import torchtext\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow_hub as hub\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection, svm, naive_bayes\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# define respective paths\n",
    "DATA_PATH = './obesity_data/'\n",
    "RESULTS_PATH = './results/'\n",
    "MODELS_PATH = './models/'\n",
    "\n",
    "if os.path.exists(RESULTS_PATH) == False:\n",
    "    os.mkdir(RESULTS_PATH)\n",
    "if os.path.exists(MODELS_PATH) == False:\n",
    "    os.mkdir(MODELS_PATH)\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "#Download info for USE\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "all_docs_df = pd.read_pickle(DATA_PATH + '/alldocs_df.pkl')\n",
    "all_annot_df = pd.read_pickle(DATA_PATH + '/allannot_df.pkl')\n",
    "all_df_expanded = pd.read_pickle(DATA_PATH + '/all_df_expanded.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dataset to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.merge(all_docs_df,all_annot_df, on='id')\n",
    "\n",
    "disease_list = all_df['disease'].unique().tolist()\n",
    "feature_list = ['All','ExtraTreeClassifier','SelectKBest','InfoGainAttributeVal']\n",
    "embedding_list = ['GloVe', 'FastText', 'USE']\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the f1-macro and f1-micro scores to a results file that we will pull into Power BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cols = ['Batch','Disease','Classifier','Feature', 'F1_MACRO', 'F1_MICRO', 'Total Run (secs)']\n",
    "\n",
    "def write_to_file(file, batch_name, disease, clfr, feature,f1_macro,f1_micro,runtime_sec):\n",
    "    #Pass TFIDF or Embeddings\n",
    "    \n",
    "    results_file = f'{RESULTS_PATH}CML_{file}_results.csv'\n",
    "\n",
    "    if os.path.exists(results_file):\n",
    "        results = pd.read_csv(results_file)\n",
    "    else:\n",
    "        results = pd.DataFrame(columns=result_cols)\n",
    "\n",
    "    result = pd.DataFrame(columns=result_cols,data=[[batch_name, disease,clfr,feature,f1_macro,f1_micro,runtime_sec]])\n",
    "    results = pd.concat([results,result])\n",
    "\n",
    "    #Save results - overwrite so we can see progress\n",
    "    results.to_csv(results_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a text_final column that we will use to train and test models with. Loop through the tokenized lemmatized words and create one string with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(all_df['tok_lem_text']):\n",
    "    Final_words = []\n",
    "    for word in entry:\n",
    "        Final_words.append(word)\n",
    "    all_df.loc[index, 'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform SVM by training the model on training data which includes TF-IDF, feature selection, or embeddings. Predict using the test dataset and run f1-micro and f1-macro to test accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performSVM(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature, disease):\n",
    "    model_name = 'SVM_' + feature + '_' + disease\n",
    "    start_time = time.time()\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    \n",
    "    # Train the model on the training dataset\n",
    "    SVM.fit(Train_X_Tfidf, y_train)\n",
    "\n",
    "    # Predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec = end_time-start_time\n",
    "\n",
    "    f1_macro = f1_score(y_test, predictions_SVM,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_SVM,average='micro')\n",
    "\n",
    "    print(\"SVM - \", feature, disease, \": f1-macro\", f1_macro)\n",
    "    print(\"SVM - \", feature, disease, \": f1-micro\", f1_micro)\n",
    "    \n",
    "    write_to_file(file, batch_name, disease, \"SVM\", feature,f1_macro, f1_micro, runtime_sec)\n",
    "\n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform KNN with 1 k and 5 k by training the model on training data which includes TF-IDF, feature selection, or embeddings. Predict using the test dataset and run f1-micro and f1-macro to test accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performKNN(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature, disease):\n",
    "    first_model_name = 'KNN1_' + feature + '_' + disease\n",
    "    second_model_name = 'KNN5_' + feature + '_' + disease\n",
    "    start_time = time.time()\n",
    "    knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "    \n",
    "    # Train the model on the training dataset\n",
    "    clf1 = knn1.fit(Train_X_Tfidf, y_train)\n",
    "    \n",
    "    # Predict the labels on validation dataset\n",
    "    predictions_KNN1 = clf1.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec1 = end_time-start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "    \n",
    "    # Train the model on the training dataset\n",
    "    clf5 = knn5.fit(Train_X_Tfidf, y_train)\n",
    "    \n",
    "    # Predict the labels on validation dataset\n",
    "    predictions_KNN5 = clf5.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec5 = end_time-start_time\n",
    "\n",
    "    f1_macro1 = f1_score(y_test, predictions_KNN1,average='macro')\n",
    "    f1_macro5 = f1_score(y_test, predictions_KNN5,average='macro')\n",
    "    f1_micro1 = f1_score(y_test, predictions_KNN1,average='micro')\n",
    "    f1_micro5 = f1_score(y_test, predictions_KNN5,average='micro')\n",
    "\n",
    "    print(\"KNN 1 k - \", feature, disease, \": f1-macro\", f1_macro1)\n",
    "    print(\"KNN 1 k - \", feature, disease, \": f1-micro\", f1_micro1)\n",
    "    print(\"KNN 5 k - \", feature, disease, \": f1-macro\", f1_macro5)\n",
    "    print(\"KNN 5 k - \", feature, disease, \": f1-micro\", f1_micro5)\n",
    "    \n",
    "    write_to_file(file, batch_name, disease, \"KNN n=1\", feature,f1_macro1,f1_micro1, runtime_sec1)\n",
    "    write_to_file(file, batch_name, disease, \"KNN n=5\", feature,f1_macro5,f1_micro5, runtime_sec5)\n",
    "\n",
    "    return f1_macro1, f1_micro1, f1_macro5, f1_micro5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Naive Bayes by training the model on training data which includes TF-IDF, feature selection, or embeddings. Predict using the test dataset and run f1-micro and f1-macro to test accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performNB(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature, disease):\n",
    "    model_name = 'NB_' + feature + '_' + disease\n",
    "    start_time = time.time()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(Train_X_Tfidf)\n",
    "    \n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    Naive.fit(scaler.transform(Train_X_Tfidf),y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec = end_time-start_time\n",
    "\n",
    "    f1_macro = f1_score(y_test, predictions_NB,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_NB,average='micro')\n",
    "\n",
    "    print(\"NB - \", feature, disease, \": f1-macro\", f1_macro)\n",
    "    print(\"NB - \", feature, disease, \": f1-micro\", f1_micro)\n",
    "\n",
    "    write_to_file(file, batch_name, disease, \"Naive Bayes\", feature,f1_macro, f1_micro, runtime_sec)\n",
    "    \n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Random Forest by training the model on training data which includes TF-IDF, feature selection, or embeddings. Predict using the test dataset and run f1-micro and f1-macro to test accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performRF(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature, disease):\n",
    "    model_name = 'RF_' + feature + '_' + disease\n",
    "    start_time = time.time()\n",
    "\n",
    "    classifier=RandomForestClassifier(n_estimators =400,criterion=\"entropy\",random_state =0)\n",
    "    classifier.fit(Train_X_Tfidf,y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_RF = classifier.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec = end_time-start_time\n",
    "\n",
    "    f1_macro = f1_score(y_test, predictions_RF,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_RF,average='micro')\n",
    "\n",
    "    print(\"RF - \", feature, disease, \": f1-macro\", f1_macro)\n",
    "    print(\"RF - \", feature, disease, \": f1-micro\", f1_micro)\n",
    "    \n",
    "    write_to_file(file, batch_name, disease, \"Random Forest\", feature,f1_macro, f1_micro, runtime_sec)\n",
    "\n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform GloVe embeddings. GloVe allows us to take a corpus of text, and intuitively transform each word in that corpus into a position in a high-dimensional space. This means that similar words will be placed together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch_GloVe(X_Train):\n",
    "    embedding_size_used = 300\n",
    "    vec = torchtext.vocab.GloVe(name='6B', dim=embedding_size_used)\n",
    "    \n",
    "    X =  np.zeros((X_Train.shape[0], embedding_size_used * len(X_Train.iloc[0])))\n",
    "    \n",
    "    for i in range(len(X_Train)):\n",
    "        vectors = vec.get_vecs_by_tokens(X_Train.iloc[i]).float().numpy()\n",
    "        \n",
    "        X[i,:] = vectors.flatten()\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform FastText embeddings. FastText supports training continuous bag of words (CBOW) or Skip-gram models using negative sampling, softmax or hierarchical softmax loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch_FastText(X_Train):\n",
    "    embedding_size_used = 300\n",
    "    vec = torchtext.vocab.FastText()\n",
    "    \n",
    "    X =  np.zeros((X_Train.shape[0], embedding_size_used * len(X_Train.iloc[0])))\n",
    "    \n",
    "    for i in range(len(X_Train)):\n",
    "        vectors = vec.get_vecs_by_tokens(X_Train.iloc[i]).float().numpy()\n",
    "        \n",
    "        X[i,:] = vectors.flatten()\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform USE embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch_USE(X_Train):\n",
    "    embedding_size_used = 512\n",
    "    \n",
    "    X =  np.zeros((X_Train.shape[0], embedding_size_used * len(X_Train.iloc[0])))\n",
    "    \n",
    "    for i in range(len(X_Train)):\n",
    "        tensor_flow_vectors = embed(X_Train.iloc[i])\n",
    "        array_vectors = tensor_flow_vectors.numpy()\n",
    "        \n",
    "        X[i,:] = array_vectors.flatten()\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform feature selection. Pass in the parameter and it will run the respective feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "from statistics import mean\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def getVocab(X_train, y_train, feature, max_tokens):\n",
    " \n",
    "    ## Step 1: Determine the Initial Vocabulary\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab = list(tokenizer.word_index.keys())\n",
    "\n",
    "    ## Step 2: Create term  matrix\n",
    "    vectors = tokenizer.texts_to_matrix(X_train, mode='count')\n",
    "\n",
    "    ## Do feature selection on term matrix (column headers are words)\n",
    "    X = vectors\n",
    "    y = y_train\n",
    "\n",
    "    ##Choose algorithm\n",
    "    if feature == 'SelectKBest':\n",
    "        selector = SelectKBest(score_func=f_classif, k=max_tokens).fit(X,y)\n",
    "    elif feature == 'InfoGainAttributeVal':\n",
    "        selector = SelectKBest(score_func=mutual_info_classif, k=max_tokens).fit(X,y)\n",
    "    else:\n",
    "        #default to ExtraTreeClassifier\n",
    "        estimator = ExtraTreeClassifier(random_state = seed)\n",
    "        selector = SelectFromModel(estimator, max_features = max_tokens)\n",
    "        selector = selector.fit(X, y)\n",
    "\n",
    "    support_idx = selector.get_support(True)\n",
    "    tokenizer2 = Tokenizer()\n",
    "    tokenizer2.fit_on_texts([vocab[i-1].replace(\"'\",\"\") for i in support_idx])\n",
    "    new_vocab = list(tokenizer2.word_index.keys())\n",
    "\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all models for every feature selection technique for every disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'TFIDF'\n",
    "result_time = datetime.datetime.now()\n",
    "result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "batch_name = f'CML_tfidf_results_{result_name}'\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Tfidf_vect = TfidfVectorizer(max_features=600)\n",
    "Tfidf_vect_NS = TfidfVectorizer(max_features = 600, stop_words = cachedStopWords)\n",
    "\n",
    "svm_f1_micro_scores = []\n",
    "svm_f1_macro_scores = []\n",
    "knn1_f1_micro_scores = []\n",
    "knn1_f1_macro_scores = []\n",
    "knn5_f1_micro_scores = []\n",
    "knn5_f1_macro_scores = []\n",
    "nb_f1_micro_scores = []\n",
    "nb_f1_macro_scores = []\n",
    "rf_f1_micro_scores = []\n",
    "rf_f1_macro_scores = []\n",
    "\n",
    "max_tokens = 600\n",
    "\n",
    "for _,feature in enumerate(feature_list):\n",
    "    for _,disease in enumerate(disease_list):\n",
    "        disease_data_df = all_df[all_df['disease'] == disease]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(disease_data_df['text_final'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "        \n",
    "        if feature != 'All':\n",
    "            vocab = getVocab(X_train,y_train, feature, max_tokens)\n",
    "            Tfidf_vect = TfidfVectorizer(max_features=max_tokens,vocabulary = vocab)\n",
    "        else:\n",
    "            Tfidf_vect = TfidfVectorizer(max_features=max_tokens)\n",
    "  \n",
    "        # TF-IDF vectorize the training dataset\n",
    "        X_train_values_list = Tfidf_vect.fit_transform(X_train).toarray()\n",
    "        X_training = pd.DataFrame(X_train_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "        X_training = np.asarray(X_training, dtype=float)\n",
    "        X_training = torch.from_numpy(X_training).to(device)\n",
    "\n",
    "        # TF-IDF vectorize the testing dataset\n",
    "        X_test_values_list = Tfidf_vect.transform(X_test).toarray()\n",
    "        X_testing = pd.DataFrame(X_test_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "        X_testing = np.asarray(X_testing, dtype=float)\n",
    "        X_testing = torch.from_numpy(X_testing).to(device)\n",
    "\n",
    "        # Encode labels\n",
    "        Train_Y  = Encoder.fit_transform(y_train)\n",
    "        Test_Y  = Encoder.fit_transform(y_test)\n",
    "\n",
    "        svm_f1_macro, svm_f1_micro = performSVM(X_training, X_testing, Train_Y, Test_Y, feature, disease)\n",
    "        svm_f1_macro_scores.append(svm_f1_macro)\n",
    "        svm_f1_micro_scores.append(svm_f1_micro)\n",
    "\n",
    "        knn_f1_macro1, knn_f1_micro1, knn_f1_macro5, knn_f1_micro5 = performKNN(X_training, X_testing, Train_Y, Test_Y, feature, disease)\n",
    "        knn1_f1_macro_scores.append(knn_f1_macro1)\n",
    "        knn1_f1_micro_scores.append(knn_f1_micro1)\n",
    "        knn5_f1_macro_scores.append(knn_f1_macro5)\n",
    "        knn5_f1_micro_scores.append(knn_f1_micro5)\n",
    "\n",
    "        nb_f1_macro, nb_f1_micro = performNB(X_training, X_testing, Train_Y, Test_Y, feature, disease)\n",
    "        nb_f1_macro_scores.append(nb_f1_macro)\n",
    "        nb_f1_micro_scores.append(nb_f1_micro)\n",
    "\n",
    "        rf_f1_macro, rf_f1_micro = performRF(X_training, X_testing, Train_Y, Test_Y, feature, disease)\n",
    "        rf_f1_macro_scores.append(rf_f1_macro)\n",
    "        rf_f1_micro_scores.append(rf_f1_micro)\n",
    "        \n",
    "    print(\"Average SVM - \", feature, \": f1-micro\", mean(svm_f1_micro_scores))\n",
    "    print(\"Average SVM - \", feature, \": f1-macro\", mean(svm_f1_macro_scores))\n",
    "    \n",
    "    print(\"Average KNN 1 - \", feature, \": f1-micro\", mean(knn1_f1_micro_scores))\n",
    "    print(\"Average KNN 1 - \", feature, \": f1-macro\", mean(knn1_f1_macro_scores))\n",
    "    print(\"Average KNN 5 - \", feature, \": f1-micro\", mean(knn5_f1_micro_scores))\n",
    "    print(\"Average KNN 5 - \", feature, \": f1-macro\", mean(knn5_f1_macro_scores))\n",
    "    \n",
    "    print(\"Average NB - \", feature, \": f1-micro\", mean(nb_f1_micro_scores))\n",
    "    print(\"Average NB - \", feature, \": f1-macro\", mean(nb_f1_macro_scores))\n",
    "    \n",
    "    print(\"Average RF - \", feature, \": f1-micro\", mean(rf_f1_micro_scores))\n",
    "    print(\"Average RF - \", feature, \": f1-macro\", mean(rf_f1_macro_scores))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all models for every embedding for every disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Embeddings'\n",
    "result_time = datetime.datetime.now()\n",
    "result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "batch_name = f'CML_tfidf_results_{result_name}'\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Tfidf_vect = TfidfVectorizer(max_features=600)\n",
    "Tfidf_vect_NS = TfidfVectorizer(max_features = 600, stop_words = cachedStopWords)\n",
    "\n",
    "svm_f1_micro_scores = []\n",
    "svm_f1_macro_scores = []\n",
    "knn1_f1_micro_scores = []\n",
    "knn1_f1_macro_scores = []\n",
    "knn5_f1_micro_scores = []\n",
    "knn5_f1_macro_scores = []\n",
    "nb_f1_micro_scores = []\n",
    "nb_f1_macro_scores = []\n",
    "rf_f1_micro_scores = []\n",
    "rf_f1_macro_scores = []\n",
    "\n",
    "max_tokens = 600\n",
    "\n",
    "for _, embedding in enumerate(embedding_list):\n",
    "    for _,disease in enumerate(disease_list):\n",
    "        disease_data_df = all_df_expanded [all_df_expanded ['disease'] == disease]\n",
    "\n",
    "        if embedding == 'GloVe':\n",
    "            X_train, X_test, y_train, y_test = train_test_split(disease_data_df['vector_tokenized'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "            X_train = vectorize_batch_GloVe(X_train)\n",
    "            X_test = vectorize_batch_GloVe(X_test)\n",
    "        if embedding == 'FastText':\n",
    "            X_train, X_test, y_train, y_test = train_test_split(disease_data_df['vector_tokenized'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "            X_train = vectorize_batch_FastText(X_train)\n",
    "            X_test = vectorize_batch_FastText(X_test)\n",
    "        if embedding == 'USE':\n",
    "            X_train, X_test, y_train, y_test = train_test_split(disease_data_df['sentence_tokenized'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "            X_train = vectorize_batch_USE(X_train)\n",
    "            X_test = vectorize_batch_USE(X_test)\n",
    "\n",
    "        Train_Y  = Encoder.fit_transform(y_train)\n",
    "        Test_Y  = Encoder.fit_transform(y_test)\n",
    "\n",
    "        svm_f1_macro, svm_f1_micro = performSVM(X_train, X_test, Train_Y, Test_Y, embedding, disease)\n",
    "        svm_f1_macro_scores.append(svm_f1_macro)\n",
    "        svm_f1_micro_scores.append(svm_f1_micro)\n",
    "\n",
    "        knn_f1_macro1, knn_f1_micro1, knn_f1_macro5, knn_f1_micro5 = performKNN(X_train, X_test, Train_Y, Test_Y, embedding, disease)\n",
    "        knn1_f1_macro_scores.append(knn_f1_macro1)\n",
    "        knn1_f1_micro_scores.append(knn_f1_micro1)\n",
    "        knn5_f1_macro_scores.append(knn_f1_macro5)\n",
    "        knn5_f1_micro_scores.append(knn_f1_micro5)\n",
    "\n",
    "        nb_f1_macro, nb_f1_micro = performNB(X_train, X_test, Train_Y, Test_Y, embedding, disease)\n",
    "        nb_f1_macro_scores.append(nb_f1_macro)\n",
    "        nb_f1_micro_scores.append(nb_f1_micro)\n",
    "\n",
    "        rf_f1_macro, rf_f1_micro = performRF(X_train, X_test, Train_Y, Test_Y, embedding, disease)\n",
    "        rf_f1_macro_scores.append(rf_f1_macro)\n",
    "        rf_f1_micro_scores.append(rf_f1_micro)\n",
    "        \n",
    "    print(\"Average SVM - \", embedding, \": f1-micro\", mean(svm_f1_micro_scores))\n",
    "    print(\"Average SVM - \", embedding, \": f1-macro\", mean(svm_f1_macro_scores))\n",
    "\n",
    "    print(\"Average KNN 1 - \", embedding, \": f1-micro\", mean(knn1_f1_micro_scores))\n",
    "    print(\"Average KNN 1 - \", embedding, \": f1-macro\", mean(knn1_f1_macro_scores))\n",
    "    print(\"Average KNN 5 - \", embedding, \": f1-micro\", mean(knn5_f1_micro_scores))\n",
    "    print(\"Average KNN 5 - \", embedding, \": f1-macro\", mean(knn5_f1_macro_scores))\n",
    "\n",
    "    print(\"Average NB - \", embedding, \": f1-micro\", mean(nb_f1_micro_scores))\n",
    "    print(\"Average NB - \", embedding, \": f1-macro\", mean(nb_f1_macro_scores))\n",
    "\n",
    "    print(\"Average RF - \", embedding, \": f1-micro\", mean(rf_f1_micro_scores))\n",
    "    print(\"Average RF - \", embedding, \": f1-macro\", mean(rf_f1_macro_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10pK5od01jfTHJyLN94dJxEube3sJszFm",
     "timestamp": 1678482671183
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
