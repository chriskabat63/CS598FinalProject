{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d1ehXFWM5aE"
   },
   "source": [
    "This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n",
    "***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv360l2IkNfO"
   },
   "source": [
    "The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow==2.10.*\n",
    "#pip install tensorflow-text==2.10.0\n",
    "#pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - All Features**\n",
    "\n",
    "![CML TFIDF All](images\\cml-tfidf-all.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - ExtraTreesClassifier Features**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-tfidf-extra.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - InfoGain Features**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-tfidf-infogain.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - SelectKBest Features**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-tfidf-selectkbest.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - Word Embeddings - No Stopwords**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-we-swno.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - Word Embeddings - Stopwords**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-we-swyes.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import torchtext\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection, svm, naive_bayes\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# define data path\n",
    "DATA_PATH = './obesity_data/'\n",
    "RESULTS_PATH = './results/'\n",
    "MODELS_PATH = './models/'\n",
    "\n",
    "if os.path.exists(RESULTS_PATH) == False:\n",
    "    os.mkdir(RESULTS_PATH)\n",
    "if os.path.exists(MODELS_PATH) == False:\n",
    "    os.mkdir(MODELS_PATH)\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "#Download info for USE\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "all_docs_df = pd.read_pickle(DATA_PATH + '/alldocs_df.pkl')\n",
    "all_docs_df_ns = pd.read_pickle(DATA_PATH + '/alldocs_df_ns.pkl')\n",
    "all_annot_df = pd.read_pickle(DATA_PATH + '/allannot_df.pkl')\n",
    "all_df_expanded = pd.read_pickle(DATA_PATH + '/all_df_expanded.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.merge(all_docs_df,all_annot_df, on='id')\n",
    "all_df_ns = pd.merge(all_docs_df_ns,all_annot_df, on='id')\n",
    "\n",
    "disease_list = all_df['disease'].unique().tolist()\n",
    "#disease_list = ['Asthma']\n",
    "feature_list = ['All','ExtraTreeClassifier','SelectKBest','InfoGainAttributeVal']\n",
    "embedding_list = ['GloVe', 'FastText', 'USE']\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cols = ['Batch','Disease','Classifier','Feature', 'F1_MACRO', 'F1_MICRO', 'Total Run (secs)']\n",
    "\n",
    "def write_to_file(file, batch_name, disease, clfr, feature,f1_macro,f1_micro,runtime_sec):\n",
    "    #Pass TFIDF or Embeddings\n",
    "    \n",
    "    results_file = f'{RESULTS_PATH}CML_{file}_results.csv'\n",
    "\n",
    "    if os.path.exists(results_file):\n",
    "        results = pd.read_csv(results_file)\n",
    "    else:\n",
    "        results = pd.DataFrame(columns=result_cols)\n",
    "\n",
    "    result = pd.DataFrame(columns=result_cols,data=[[batch_name, disease,clfr,feature,f1_macro,f1_micro,runtime_sec]])\n",
    "    results = pd.concat([results,result])\n",
    "\n",
    "    #Save results - overwrite so we can see progress\n",
    "    results.to_csv(results_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(all_df['tok_lem_text']):\n",
    "    Final_words = []\n",
    "    for word in entry:\n",
    "        Final_words.append(word)\n",
    "    all_df.loc[index, 'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(all_df_ns['tok_lem_text']):\n",
    "    Final_words = []\n",
    "    for word in entry:\n",
    "        Final_words.append(word)\n",
    "    all_df_ns.loc[index, 'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performSVM(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature):\n",
    "    start_time = time.time()\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf, y_train)\n",
    "\n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec = end_time-start_time\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_SVM)\n",
    "    f1_macro = f1_score(y_test, predictions_SVM,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_SVM,average='micro')\n",
    "\n",
    "    #print(\"SVM - \", disease, \": f1-score\", f1)\n",
    "    print(\"SVM - \", feature, disease, \": f1-macro\", f1_macro)\n",
    "    print(\"SVM - \", feature, disease, \": f1-micro\", f1_micro)\n",
    "    \n",
    "    write_to_file(file, batch_name, disease, \"SVM\", feature,f1_macro, f1_micro, runtime_sec)\n",
    "\n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performKNN(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature):\n",
    "\n",
    "    start_time = time.time()\n",
    "    knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "    clf1 = knn1.fit(Train_X_Tfidf, y_train)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_KNN1 = clf1.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec1 = end_time-start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "    clf5 = knn5.fit(Train_X_Tfidf, y_train)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_KNN5 = clf5.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec5 = end_time-start_time\n",
    "\n",
    "\n",
    "    #auroc = roc_auc_score(truth, pred[:,1])\n",
    "    #f1 = f1_score(y_test, predictions_KNN)\n",
    "    f1_macro1 = f1_score(y_test, predictions_KNN1,average='macro')\n",
    "    f1_macro5 = f1_score(y_test, predictions_KNN5,average='macro')\n",
    "    f1_micro1 = f1_score(y_test, predictions_KNN1,average='micro')\n",
    "    f1_micro5 = f1_score(y_test, predictions_KNN5,average='micro')\n",
    "\n",
    "    #print(\"KNN - \", disease, \": f1-score\", f1)\n",
    "    print(\"KNN 1 k - \", feature, disease, \": f1-macro\", f1_macro1)\n",
    "    print(\"KNN 1 k - \", feature, disease, \": f1-micro\", f1_micro1)\n",
    "    print(\"KNN 5 k - \", feature, disease, \": f1-macro\", f1_macro5)\n",
    "    print(\"KNN 5 k - \", feature, disease, \": f1-micro\", f1_micro5)\n",
    "    \n",
    "    write_to_file(file, batch_name, disease, \"KNN n=1\", feature,f1_macro1,f1_micro1, runtime_sec1)\n",
    "    write_to_file(file, batch_name, disease, \"KNN n=5\", feature,f1_macro5,f1_micro5, runtime_sec5)\n",
    "\n",
    "    return f1_macro1, f1_micro1, f1_macro5, f1_micro5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performNB(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature):\n",
    "    start_time = time.time()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(Train_X_Tfidf)\n",
    "    \n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    Naive.fit(scaler.transform(Train_X_Tfidf),y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec = end_time-start_time\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_NB)\n",
    "    f1_macro = f1_score(y_test, predictions_NB,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_NB,average='micro')\n",
    "\n",
    "    #print(\"NB - \", disease, \": f1-score\", f1)\n",
    "    print(\"NB - \", feature, disease, \": f1-macro\", f1_macro)\n",
    "    print(\"NB - \", feature, disease, \": f1-micro\", f1_micro)\n",
    "\n",
    "    write_to_file(file, batch_name, disease, \"Naive Bayes\", feature,f1_macro, f1_micro, runtime_sec)\n",
    "    \n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performRF(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature):\n",
    "    start_time = time.time()\n",
    "\n",
    "    classifier=RandomForestClassifier(n_estimators =400,criterion=\"entropy\",random_state =0)\n",
    "    classifier.fit(Train_X_Tfidf,y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_RF = classifier.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec = end_time-start_time\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_RF)\n",
    "    f1_macro = f1_score(y_test, predictions_RF,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_RF,average='micro')\n",
    "\n",
    "    #print(\"RF - \", disease, \": f1-score\", f1)\n",
    "    print(\"RF - \", feature, disease, \": f1-macro\", f1_macro)\n",
    "    print(\"RF - \", feature, disease, \": f1-micro\", f1_micro)\n",
    "    \n",
    "    write_to_file(file, batch_name, disease, \"Random Forest\", feature,f1_macro, f1_micro, runtime_sec)\n",
    "\n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch_GloVe(X_Train):\n",
    "    embedding_size_used = 300\n",
    "    vec = torchtext.vocab.GloVe(name='6B', dim=embedding_size_used)\n",
    "    \n",
    "    X =  np.zeros((X_Train.shape[0], embedding_size_used * len(X_Train.iloc[0])))\n",
    "    \n",
    "    for i in range(len(X_Train)):\n",
    "        vectors = vec.get_vecs_by_tokens(X_Train.iloc[i]).float().numpy()\n",
    "        \n",
    "        X[i,:] = vectors.flatten()\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch_FastText(X_Train):\n",
    "    embedding_size_used = 300\n",
    "    vec = torchtext.vocab.FastText()\n",
    "    \n",
    "    X =  np.zeros((X_Train.shape[0], embedding_size_used * len(X_Train.iloc[0])))\n",
    "    \n",
    "    for i in range(len(X_Train)):\n",
    "        vectors = vec.get_vecs_by_tokens(X_Train.iloc[i]).float().numpy()\n",
    "        \n",
    "        X[i,:] = vectors.flatten()\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch_USE(X_Train):\n",
    "    embedding_size_used = 512\n",
    "    \n",
    "    X =  np.zeros((X_Train.shape[0], embedding_size_used * len(X_Train.iloc[0])))\n",
    "    \n",
    "    for i in range(len(X_Train)):\n",
    "        tensor_flow_vectors = embed(X_Train.iloc[i])\n",
    "        array_vectors = tensor_flow_vectors.numpy()\n",
    "        \n",
    "        X[i,:] = array_vectors.flatten()\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateXTrainAndTest(X_train, X_test, Tfidf_vect):\n",
    "    X_train_values_list = Tfidf_vect.fit_transform(X_train).toarray()\n",
    "    X_training = pd.DataFrame(X_train_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "    X_training = np.asarray(X_training, dtype=float)\n",
    "    X_training = torch.from_numpy(X_training).to(device)\n",
    "\n",
    "    X_test_values_list = Tfidf_vect.transform(X_test).toarray()\n",
    "    X_testing = pd.DataFrame(X_test_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "    X_testing = np.asarray(X_testing, dtype=float)\n",
    "    X_testing = torch.from_numpy(X_testing).to(device)  \n",
    "    \n",
    "    return X_training, X_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "from statistics import mean\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def getVocab(X_train, y_train, feature, max_tokens):\n",
    " \n",
    "    ## Step 1: Determine the Initial Vocabulary\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab = list(tokenizer.word_index.keys())\n",
    "\n",
    "    ## Step 2: Create term  matrix\n",
    "    vectors = tokenizer.texts_to_matrix(X_train, mode='count')\n",
    "\n",
    "    ## Do feature selection on term matrix (column headers are words)\n",
    "    X = vectors\n",
    "    y = y_train\n",
    "\n",
    "    ##Choose algorithm\n",
    "    if feature == 'SelectKBest':\n",
    "        selector = SelectKBest(score_func=f_classif, k=max_tokens).fit(X,y)\n",
    "    else: \n",
    "        if feature == 'InfoGainAttributeVal':\n",
    "            #This should be similar to the InfoGain?\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=max_tokens).fit(X,y)\n",
    "        else:\n",
    "            #default to ExtraTreeClassifier\n",
    "            estimator = ExtraTreeClassifier(random_state = seed)\n",
    "            selector = SelectFromModel(estimator, max_features = max_tokens)\n",
    "            selector = selector.fit(X, y)\n",
    "\n",
    "    support_idx = selector.get_support(True)\n",
    "    tokenizer2 = Tokenizer()\n",
    "    tokenizer2.fit_on_texts([vocab[i-1].replace(\"'\",\"\") for i in support_idx])\n",
    "    new_vocab = list(tokenizer2.word_index.keys())\n",
    "\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'TFIDF'\n",
    "result_time = datetime.datetime.now()\n",
    "result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "batch_name = f'CML_tfidf_results_{result_name}'\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Tfidf_vect = TfidfVectorizer(max_features=600)\n",
    "Tfidf_vect_NS = TfidfVectorizer(max_features = 600, stop_words = cachedStopWords)\n",
    "\n",
    "svm_f1_micro_scores = []\n",
    "svm_f1_macro_scores = []\n",
    "knn1_f1_micro_scores = []\n",
    "knn1_f1_macro_scores = []\n",
    "knn5_f1_micro_scores = []\n",
    "knn5_f1_macro_scores = []\n",
    "nb_f1_micro_scores = []\n",
    "nb_f1_macro_scores = []\n",
    "rf_f1_micro_scores = []\n",
    "rf_f1_macro_scores = []\n",
    "\n",
    "max_tokens = 600\n",
    "\n",
    "for _,feature in enumerate(feature_list):\n",
    "    for _,disease in enumerate(disease_list):\n",
    "        disease_data_df = all_df[all_df['disease'] == disease]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(disease_data_df['text_final'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "        \n",
    "        if feature != 'All':\n",
    "            vocab = getVocab(X_train,y_train, feature, max_tokens)\n",
    "            Tfidf_vect = TfidfVectorizer(max_features=max_tokens,vocabulary = vocab)\n",
    "        else:\n",
    "            Tfidf_vect = TfidfVectorizer(max_features=max_tokens)\n",
    "  \n",
    "        \n",
    "        X_train_values_list = Tfidf_vect.fit_transform(X_train).toarray()\n",
    "        X_training = pd.DataFrame(X_train_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "        X_training = np.asarray(X_training, dtype=float)\n",
    "        X_training = torch.from_numpy(X_training).to(device)\n",
    "\n",
    "        X_test_values_list = Tfidf_vect.transform(X_test).toarray()\n",
    "        X_testing = pd.DataFrame(X_test_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "        X_testing = np.asarray(X_testing, dtype=float)\n",
    "        X_testing = torch.from_numpy(X_testing).to(device)\n",
    "        \n",
    "        #tokens_to_use = X_training.shape[1]\n",
    "\n",
    "        Train_Y  = Encoder.fit_transform(y_train)\n",
    "        Test_Y  = Encoder.fit_transform(y_test)\n",
    "\n",
    "        svm_f1_macro, svm_f1_micro = performSVM(X_training, X_testing, Train_Y, Test_Y, feature)\n",
    "        svm_f1_macro_scores.append(svm_f1_macro)\n",
    "        svm_f1_micro_scores.append(svm_f1_micro)\n",
    "\n",
    "        knn_f1_macro1, knn_f1_micro1, knn_f1_macro5, knn_f1_micro5 = performKNN(X_training, X_testing, Train_Y, Test_Y, feature)\n",
    "        knn1_f1_macro_scores.append(knn_f1_macro1)\n",
    "        knn1_f1_micro_scores.append(knn_f1_micro1)\n",
    "        knn5_f1_macro_scores.append(knn_f1_macro5)\n",
    "        knn5_f1_micro_scores.append(knn_f1_micro5)\n",
    "\n",
    "        nb_f1_macro, nb_f1_micro = performNB(X_training, X_testing, Train_Y, Test_Y, feature)\n",
    "        nb_f1_macro_scores.append(nb_f1_macro)\n",
    "        nb_f1_micro_scores.append(nb_f1_micro)\n",
    "\n",
    "        rf_f1_macro, rf_f1_micro = performRF(X_training, X_testing, Train_Y, Test_Y, feature)\n",
    "        rf_f1_macro_scores.append(rf_f1_macro)\n",
    "        rf_f1_micro_scores.append(rf_f1_micro)\n",
    "        \n",
    "    print(\"Average SVM - \", feature, \": f1-micro\", mean(svm_f1_micro_scores))\n",
    "    print(\"Average SVM - \", feature, \": f1-macro\", mean(svm_f1_macro_scores))\n",
    "    \n",
    "    print(\"Average KNN 1 - \", feature, \": f1-micro\", mean(knn1_f1_micro_scores))\n",
    "    print(\"Average KNN 1 - \", feature, \": f1-macro\", mean(knn1_f1_macro_scores))\n",
    "    print(\"Average KNN 5 - \", feature, \": f1-micro\", mean(knn5_f1_micro_scores))\n",
    "    print(\"Average KNN 5 - \", feature, \": f1-macro\", mean(knn5_f1_macro_scores))\n",
    "    \n",
    "    print(\"Average NB - \", feature, \": f1-micro\", mean(nb_f1_micro_scores))\n",
    "    print(\"Average NB - \", feature, \": f1-macro\", mean(nb_f1_macro_scores))\n",
    "    \n",
    "    print(\"Average RF - \", feature, \": f1-micro\", mean(rf_f1_micro_scores))\n",
    "    print(\"Average RF - \", feature, \": f1-macro\", mean(rf_f1_macro_scores))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Embeddings'\n",
    "result_time = datetime.datetime.now()\n",
    "result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "batch_name = f'CML_tfidf_results_{result_name}'\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Tfidf_vect = TfidfVectorizer(max_features=600)\n",
    "Tfidf_vect_NS = TfidfVectorizer(max_features = 600, stop_words = cachedStopWords)\n",
    "\n",
    "svm_f1_micro_scores = []\n",
    "svm_f1_macro_scores = []\n",
    "knn1_f1_micro_scores = []\n",
    "knn1_f1_macro_scores = []\n",
    "knn5_f1_micro_scores = []\n",
    "knn5_f1_macro_scores = []\n",
    "nb_f1_micro_scores = []\n",
    "nb_f1_macro_scores = []\n",
    "rf_f1_micro_scores = []\n",
    "rf_f1_macro_scores = []\n",
    "\n",
    "max_tokens = 600\n",
    "\n",
    "for _, embedding in enumerate(embedding_list):\n",
    "    for _,disease in enumerate(disease_list):\n",
    "        disease_data_df = all_df_expanded [all_df_expanded ['disease'] == disease]\n",
    "\n",
    "        if embedding == 'GloVe':\n",
    "            X_train, X_test, y_train, y_test = train_test_split(disease_data_df['vector_tokenized'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "            X_train = vectorize_batch_GloVe(X_train)\n",
    "            X_test = vectorize_batch_GloVe(X_test)\n",
    "        if embedding == 'FastText':\n",
    "            X_train, X_test, y_train, y_test = train_test_split(disease_data_df['vector_tokenized'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "            X_train = vectorize_batch_FastText(X_train)\n",
    "            X_test = vectorize_batch_FastText(X_test)\n",
    "        if embedding == 'USE':\n",
    "            X_train, X_test, y_train, y_test = train_test_split(disease_data_df['sentence_tokenized'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "            X_train = vectorize_batch_USE(X_train)\n",
    "            X_test = vectorize_batch_USE(X_test)\n",
    "\n",
    "        Train_Y  = Encoder.fit_transform(y_train)\n",
    "        Test_Y  = Encoder.fit_transform(y_test)\n",
    "\n",
    "        svm_f1_macro, svm_f1_micro = performSVM(X_train, X_test, Train_Y, Test_Y, embedding)\n",
    "        svm_f1_macro_scores.append(svm_f1_macro)\n",
    "        svm_f1_micro_scores.append(svm_f1_micro)\n",
    "\n",
    "        knn_f1_macro1, knn_f1_micro1, knn_f1_macro5, knn_f1_micro5 = performKNN(X_train, X_test, Train_Y, Test_Y, embedding)\n",
    "        knn1_f1_macro_scores.append(knn_f1_macro1)\n",
    "        knn1_f1_micro_scores.append(knn_f1_micro1)\n",
    "        knn5_f1_macro_scores.append(knn_f1_macro5)\n",
    "        knn5_f1_micro_scores.append(knn_f1_micro5)\n",
    "\n",
    "        nb_f1_macro, nb_f1_micro = performNB(X_train, X_test, Train_Y, Test_Y, embedding)\n",
    "        nb_f1_macro_scores.append(nb_f1_macro)\n",
    "        nb_f1_micro_scores.append(nb_f1_micro)\n",
    "\n",
    "        rf_f1_macro, rf_f1_micro = performRF(X_train, X_test, Train_Y, Test_Y, embedding)\n",
    "        rf_f1_macro_scores.append(rf_f1_macro)\n",
    "        rf_f1_micro_scores.append(rf_f1_micro)\n",
    "        \n",
    "    print(\"Average SVM - \", embedding, \": f1-micro\", mean(svm_f1_micro_scores))\n",
    "    print(\"Average SVM - \", embedding, \": f1-macro\", mean(svm_f1_macro_scores))\n",
    "\n",
    "    print(\"Average KNN 1 - \", embedding, \": f1-micro\", mean(knn1_f1_micro_scores))\n",
    "    print(\"Average KNN 1 - \", embedding, \": f1-macro\", mean(knn1_f1_macro_scores))\n",
    "    print(\"Average KNN 5 - \", embedding, \": f1-micro\", mean(knn5_f1_micro_scores))\n",
    "    print(\"Average KNN 5 - \", embedding, \": f1-macro\", mean(knn5_f1_macro_scores))\n",
    "\n",
    "    print(\"Average NB - \", embedding, \": f1-micro\", mean(nb_f1_micro_scores))\n",
    "    print(\"Average NB - \", embedding, \": f1-macro\", mean(nb_f1_macro_scores))\n",
    "\n",
    "    print(\"Average RF - \", embedding, \": f1-micro\", mean(rf_f1_micro_scores))\n",
    "    print(\"Average RF - \", embedding, \": f1-macro\", mean(rf_f1_macro_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10pK5od01jfTHJyLN94dJxEube3sJszFm",
     "timestamp": 1678482671183
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
