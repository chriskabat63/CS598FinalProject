{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d1ehXFWM5aE"
   },
   "source": [
    "This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n",
    "***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv360l2IkNfO"
   },
   "source": [
    "The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow==2.10.*\n",
    "#pip install tensorflow-text==2.10.0\n",
    "#pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - All Features**\n",
    "\n",
    "![CML TFIDF All](images\\cml-tfidf-all.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - ExtraTreesClassifier Features**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-tfidf-extra.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - InfoGain Features**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-tfidf-infogain.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - SelectKBest Features**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-tfidf-selectkbest.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - Word Embeddings - No Stopwords**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-we-swno.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - Word Embeddings - Stopwords**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-we-swyes.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mclop\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import torchtext\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection, svm, naive_bayes\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# define data path\n",
    "DATA_PATH = './obesity_data/'\n",
    "RESULTS_PATH = './results/'\n",
    "MODELS_PATH = './models/'\n",
    "\n",
    "if os.path.exists(RESULTS_PATH) == False:\n",
    "    os.mkdir(RESULTS_PATH)\n",
    "if os.path.exists(MODELS_PATH) == False:\n",
    "    os.mkdir(MODELS_PATH)\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "#Download info for USE\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "all_docs_df = pd.read_pickle(DATA_PATH + '/alldocs_df.pkl')\n",
    "all_docs_df_ns = pd.read_pickle(DATA_PATH + '/alldocs_df_ns.pkl')\n",
    "all_annot_df = pd.read_pickle(DATA_PATH + '/allannot_df.pkl')\n",
    "all_df_expanded = pd.read_pickle(DATA_PATH + '/all_df_expanded.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.merge(all_docs_df,all_annot_df, on='id')\n",
    "all_df_ns = pd.merge(all_docs_df_ns,all_annot_df, on='id')\n",
    "\n",
    "disease_list = all_df['disease'].unique().tolist()\n",
    "#disease_list = ['Asthma']\n",
    "feature_list = ['All','ExtraTreeClassifier','SelectKBest','InfoGainAttributeVal']\n",
    "embedding_list = ['GloVe', 'FastText', 'USE']\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cols = ['Batch','Disease','Classifier','Feature', 'F1_MACRO', 'F1_MICRO', 'Total Run (secs)']\n",
    "\n",
    "def write_to_file(file, batch_name, disease, clfr, feature,f1_macro,f1_micro,runtime_sec):\n",
    "    #Pass TFIDF or Embeddings\n",
    "    \n",
    "    results_file = f'{RESULTS_PATH}CML_{file}_results.csv'\n",
    "\n",
    "    if os.path.exists(results_file):\n",
    "        results = pd.read_csv(results_file)\n",
    "    else:\n",
    "        results = pd.DataFrame(columns=result_cols)\n",
    "\n",
    "    result = pd.DataFrame(columns=result_cols,data=[[batch_name, disease,clfr,feature,f1_macro,f1_micro,runtime_sec]])\n",
    "    results = pd.concat([results,result])\n",
    "\n",
    "    #Save results - overwrite so we can see progress\n",
    "    results.to_csv(results_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(all_df['tok_lem_text']):\n",
    "    Final_words = []\n",
    "    for word in entry:\n",
    "        Final_words.append(word)\n",
    "    all_df.loc[index, 'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(all_df_ns['tok_lem_text']):\n",
    "    Final_words = []\n",
    "    for word in entry:\n",
    "        Final_words.append(word)\n",
    "    all_df_ns.loc[index, 'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performSVM(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature):\n",
    "    start_time = time.time()\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf, y_train)\n",
    "\n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec = end_time-start_time\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_SVM)\n",
    "    f1_macro = f1_score(y_test, predictions_SVM,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_SVM,average='micro')\n",
    "\n",
    "    #print(\"SVM - \", disease, \": f1-score\", f1)\n",
    "    print(\"SVM - \", feature, disease, \": f1-macro\", f1_macro)\n",
    "    print(\"SVM - \", feature, disease, \": f1-micro\", f1_micro)\n",
    "    \n",
    "    write_to_file(file, batch_name, disease, \"SVM\", feature,f1_macro, f1_micro, runtime_sec)\n",
    "\n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performKNN(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature):\n",
    "\n",
    "    start_time = time.time()\n",
    "    knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "    clf1 = knn1.fit(Train_X_Tfidf, y_train)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_KNN1 = clf1.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec1 = end_time-start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "    clf5 = knn5.fit(Train_X_Tfidf, y_train)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_KNN5 = clf5.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec5 = end_time-start_time\n",
    "\n",
    "\n",
    "    #auroc = roc_auc_score(truth, pred[:,1])\n",
    "    #f1 = f1_score(y_test, predictions_KNN)\n",
    "    f1_macro1 = f1_score(y_test, predictions_KNN1,average='macro')\n",
    "    f1_macro5 = f1_score(y_test, predictions_KNN5,average='macro')\n",
    "    f1_micro1 = f1_score(y_test, predictions_KNN1,average='micro')\n",
    "    f1_micro5 = f1_score(y_test, predictions_KNN5,average='micro')\n",
    "\n",
    "    #print(\"KNN - \", disease, \": f1-score\", f1)\n",
    "    print(\"KNN 1 k - \", feature, disease, \": f1-macro\", f1_macro1)\n",
    "    print(\"KNN 1 k - \", feature, disease, \": f1-micro\", f1_micro1)\n",
    "    print(\"KNN 5 k - \", feature, disease, \": f1-macro\", f1_macro5)\n",
    "    print(\"KNN 5 k - \", feature, disease, \": f1-micro\", f1_micro5)\n",
    "    \n",
    "    write_to_file(file, batch_name, disease, \"KNN n=1\", feature,f1_macro1,f1_micro1, runtime_sec1)\n",
    "    write_to_file(file, batch_name, disease, \"KNN n=5\", feature,f1_macro5,f1_micro5, runtime_sec5)\n",
    "\n",
    "    return f1_macro1, f1_micro1, f1_macro5, f1_micro5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performNB(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature):\n",
    "    start_time = time.time()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(Train_X_Tfidf)\n",
    "    \n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    Naive.fit(scaler.transform(Train_X_Tfidf),y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec = end_time-start_time\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_NB)\n",
    "    f1_macro = f1_score(y_test, predictions_NB,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_NB,average='micro')\n",
    "\n",
    "    #print(\"NB - \", disease, \": f1-score\", f1)\n",
    "    print(\"NB - \", feature, disease, \": f1-macro\", f1_macro)\n",
    "    print(\"NB - \", feature, disease, \": f1-micro\", f1_micro)\n",
    "\n",
    "    write_to_file(file, batch_name, disease, \"Naive Bayes\", feature,f1_macro, f1_micro, runtime_sec)\n",
    "    \n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performRF(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature):\n",
    "    start_time = time.time()\n",
    "\n",
    "    classifier=RandomForestClassifier(n_estimators =400,criterion=\"entropy\",random_state =0)\n",
    "    classifier.fit(Train_X_Tfidf,y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_RF = classifier.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec = end_time-start_time\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_RF)\n",
    "    f1_macro = f1_score(y_test, predictions_RF,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_RF,average='micro')\n",
    "\n",
    "    #print(\"RF - \", disease, \": f1-score\", f1)\n",
    "    print(\"RF - \", feature, disease, \": f1-macro\", f1_macro)\n",
    "    print(\"RF - \", feature, disease, \": f1-micro\", f1_micro)\n",
    "    \n",
    "    write_to_file(file, batch_name, disease, \"Random Forest\", feature,f1_macro, f1_micro, runtime_sec)\n",
    "\n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch_GloVe(X_Train):\n",
    "    embedding_size_used = 300\n",
    "    vec = torchtext.vocab.GloVe(name='6B', dim=embedding_size_used)\n",
    "    \n",
    "    X =  np.zeros((X_Train.shape[0], embedding_size_used * len(X_Train.iloc[0])))\n",
    "    \n",
    "    for i in range(len(X_Train)):\n",
    "        vectors = vec.get_vecs_by_tokens(X_Train.iloc[i]).float().numpy()\n",
    "        \n",
    "        X[i,:] = vectors.flatten()\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch_FastText(X_Train):\n",
    "    embedding_size_used = 300\n",
    "    vec = torchtext.vocab.FastText()\n",
    "    \n",
    "    X =  np.zeros((X_Train.shape[0], embedding_size_used * len(X_Train.iloc[0])))\n",
    "    \n",
    "    for i in range(len(X_Train)):\n",
    "        vectors = vec.get_vecs_by_tokens(X_Train.iloc[i]).float().numpy()\n",
    "        \n",
    "        X[i,:] = vectors.flatten()\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch_USE(X_Train):\n",
    "    embedding_size_used = 512\n",
    "    \n",
    "    X =  np.zeros((X_Train.shape[0], embedding_size_used * len(X_Train.iloc[0])))\n",
    "    \n",
    "    for i in range(len(X_Train)):\n",
    "        tensor_flow_vectors = embed(X_Train.iloc[i])\n",
    "        array_vectors = tensor_flow_vectors.numpy()\n",
    "        \n",
    "        X[i,:] = array_vectors.flatten()\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateXTrainAndTest(X_train, X_test, Tfidf_vect):\n",
    "    X_train_values_list = Tfidf_vect.fit_transform(X_train).toarray()\n",
    "    X_training = pd.DataFrame(X_train_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "    X_training = np.asarray(X_training, dtype=float)\n",
    "    X_training = torch.from_numpy(X_training).to(device)\n",
    "\n",
    "    X_test_values_list = Tfidf_vect.transform(X_test).toarray()\n",
    "    X_testing = pd.DataFrame(X_test_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "    X_testing = np.asarray(X_testing, dtype=float)\n",
    "    X_testing = torch.from_numpy(X_testing).to(device)  \n",
    "    \n",
    "    return X_training, X_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "from statistics import mean\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def getVocab(X_train, y_train, feature, max_tokens):\n",
    " \n",
    "    ## Step 1: Determine the Initial Vocabulary\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab = list(tokenizer.word_index.keys())\n",
    "\n",
    "    ## Step 2: Create term  matrix\n",
    "    vectors = tokenizer.texts_to_matrix(X_train, mode='count')\n",
    "\n",
    "    ## Do feature selection on term matrix (column headers are words)\n",
    "    X = vectors\n",
    "    y = y_train\n",
    "\n",
    "    ##Choose algorithm\n",
    "    if feature == 'SelectKBest':\n",
    "        selector = SelectKBest(score_func=f_classif, k=max_tokens).fit(X,y)\n",
    "    else: \n",
    "        if feature == 'InfoGainAttributeVal':\n",
    "            #This should be similar to the InfoGain?\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=max_tokens).fit(X,y)\n",
    "        else:\n",
    "            #default to ExtraTreeClassifier\n",
    "            estimator = ExtraTreeClassifier(random_state = seed)\n",
    "            selector = SelectFromModel(estimator, max_features = max_tokens)\n",
    "            selector = selector.fit(X, y)\n",
    "\n",
    "    support_idx = selector.get_support(True)\n",
    "    tokenizer2 = Tokenizer()\n",
    "    tokenizer2.fit_on_texts([vocab[i-1].replace(\"'\",\"\") for i in support_idx])\n",
    "    new_vocab = list(tokenizer2.word_index.keys())\n",
    "\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'TFIDF'\n",
    "result_time = datetime.datetime.now()\n",
    "result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "batch_name = f'CML_tfidf_results_{result_name}'\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Tfidf_vect = TfidfVectorizer(max_features=600)\n",
    "Tfidf_vect_NS = TfidfVectorizer(max_features = 600, stop_words = cachedStopWords)\n",
    "\n",
    "svm_f1_micro_scores = []\n",
    "svm_f1_macro_scores = []\n",
    "knn1_f1_micro_scores = []\n",
    "knn1_f1_macro_scores = []\n",
    "knn5_f1_micro_scores = []\n",
    "knn5_f1_macro_scores = []\n",
    "nb_f1_micro_scores = []\n",
    "nb_f1_macro_scores = []\n",
    "rf_f1_micro_scores = []\n",
    "rf_f1_macro_scores = []\n",
    "\n",
    "max_tokens = 600\n",
    "\n",
    "for _,feature in enumerate(feature_list):\n",
    "    for _,disease in enumerate(disease_list):\n",
    "        disease_data_df = all_df[all_df['disease'] == disease]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(disease_data_df['text_final'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "        \n",
    "        if feature != 'All':\n",
    "            vocab = getVocab(X_train,y_train, feature, max_tokens)\n",
    "            Tfidf_vect = TfidfVectorizer(max_features=max_tokens,vocabulary = vocab)\n",
    "        else:\n",
    "            Tfidf_vect = TfidfVectorizer(max_features=max_tokens)\n",
    "  \n",
    "        \n",
    "        X_train_values_list = Tfidf_vect.fit_transform(X_train).toarray()\n",
    "        X_training = pd.DataFrame(X_train_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "        X_training = np.asarray(X_training, dtype=float)\n",
    "        X_training = torch.from_numpy(X_training).to(device)\n",
    "\n",
    "        X_test_values_list = Tfidf_vect.transform(X_test).toarray()\n",
    "        X_testing = pd.DataFrame(X_test_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "        X_testing = np.asarray(X_testing, dtype=float)\n",
    "        X_testing = torch.from_numpy(X_testing).to(device)\n",
    "        \n",
    "        #tokens_to_use = X_training.shape[1]\n",
    "\n",
    "        Train_Y  = Encoder.fit_transform(y_train)\n",
    "        Test_Y  = Encoder.fit_transform(y_test)\n",
    "\n",
    "        svm_f1_macro, svm_f1_micro = performSVM(X_training, X_testing, Train_Y, Test_Y, feature)\n",
    "        svm_f1_macro_scores.append(svm_f1_macro)\n",
    "        svm_f1_micro_scores.append(svm_f1_micro)\n",
    "\n",
    "        knn_f1_macro1, knn_f1_micro1, knn_f1_macro5, knn_f1_micro5 = performKNN(X_training, X_testing, Train_Y, Test_Y, feature)\n",
    "        knn1_f1_macro_scores.append(knn_f1_macro1)\n",
    "        knn1_f1_micro_scores.append(knn_f1_micro1)\n",
    "        knn5_f1_macro_scores.append(knn_f1_macro5)\n",
    "        knn5_f1_micro_scores.append(knn_f1_micro5)\n",
    "\n",
    "        nb_f1_macro, nb_f1_micro = performNB(X_training, X_testing, Train_Y, Test_Y, feature)\n",
    "        nb_f1_macro_scores.append(nb_f1_macro)\n",
    "        nb_f1_micro_scores.append(nb_f1_micro)\n",
    "\n",
    "        rf_f1_macro, rf_f1_micro = performRF(X_training, X_testing, Train_Y, Test_Y, feature)\n",
    "        rf_f1_macro_scores.append(rf_f1_macro)\n",
    "        rf_f1_micro_scores.append(rf_f1_micro)\n",
    "        \n",
    "    print(\"Average SVM - \", feature, \": f1-micro\", mean(svm_f1_micro_scores))\n",
    "    print(\"Average SVM - \", feature, \": f1-macro\", mean(svm_f1_macro_scores))\n",
    "    \n",
    "    print(\"Average KNN 1 - \", feature, \": f1-micro\", mean(knn1_f1_micro_scores))\n",
    "    print(\"Average KNN 1 - \", feature, \": f1-macro\", mean(knn1_f1_macro_scores))\n",
    "    print(\"Average KNN 5 - \", feature, \": f1-micro\", mean(knn5_f1_micro_scores))\n",
    "    print(\"Average KNN 5 - \", feature, \": f1-macro\", mean(knn5_f1_macro_scores))\n",
    "    \n",
    "    print(\"Average NB - \", feature, \": f1-micro\", mean(nb_f1_micro_scores))\n",
    "    print(\"Average NB - \", feature, \": f1-macro\", mean(nb_f1_macro_scores))\n",
    "    \n",
    "    print(\"Average RF - \", feature, \": f1-micro\", mean(rf_f1_micro_scores))\n",
    "    print(\"Average RF - \", feature, \": f1-macro\", mean(rf_f1_macro_scores))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM -  GloVe Asthma : f1-macro 0.4557823129251701\n",
      "SVM -  GloVe Asthma : f1-micro 0.8375\n",
      "KNN 1 k -  GloVe Asthma : f1-macro 0.4557823129251701\n",
      "KNN 1 k -  GloVe Asthma : f1-micro 0.8375\n",
      "KNN 5 k -  GloVe Asthma : f1-macro 0.4557823129251701\n",
      "KNN 5 k -  GloVe Asthma : f1-micro 0.8375\n",
      "NB -  GloVe Asthma : f1-macro 0.5340688479568161\n",
      "NB -  GloVe Asthma : f1-micro 0.8291666666666667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m nb_f1_macro_scores\u001b[38;5;241m.\u001b[39mappend(nb_f1_macro)\n\u001b[0;32m     55\u001b[0m nb_f1_micro_scores\u001b[38;5;241m.\u001b[39mappend(nb_f1_micro)\n\u001b[1;32m---> 57\u001b[0m rf_f1_macro, rf_f1_micro \u001b[38;5;241m=\u001b[39m \u001b[43mperformRF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrain_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTest_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m rf_f1_macro_scores\u001b[38;5;241m.\u001b[39mappend(rf_f1_macro)\n\u001b[0;32m     59\u001b[0m rf_f1_micro_scores\u001b[38;5;241m.\u001b[39mappend(rf_f1_micro)\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mperformRF\u001b[1;34m(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature)\u001b[0m\n\u001b[0;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      4\u001b[0m classifier\u001b[38;5;241m=\u001b[39mRandomForestClassifier(n_estimators \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,random_state \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrain_X_Tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# predict the labels on validation dataset\u001b[39;00m\n\u001b[0;32m      8\u001b[0m predictions_RF \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(Test_X_Tfidf)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    439\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    442\u001b[0m ]\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    183\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 185\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:937\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m ):\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    411\u001b[0m         splitter,\n\u001b[0;32m    412\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[1;32m--> 420\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file = 'Embeddings'\n",
    "result_time = datetime.datetime.now()\n",
    "result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "batch_name = f'CML_tfidf_results_{result_name}'\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Tfidf_vect = TfidfVectorizer(max_features=600)\n",
    "Tfidf_vect_NS = TfidfVectorizer(max_features = 600, stop_words = cachedStopWords)\n",
    "\n",
    "svm_f1_micro_scores = []\n",
    "svm_f1_macro_scores = []\n",
    "knn1_f1_micro_scores = []\n",
    "knn1_f1_macro_scores = []\n",
    "knn5_f1_micro_scores = []\n",
    "knn5_f1_macro_scores = []\n",
    "nb_f1_micro_scores = []\n",
    "nb_f1_macro_scores = []\n",
    "rf_f1_micro_scores = []\n",
    "rf_f1_macro_scores = []\n",
    "\n",
    "max_tokens = 600\n",
    "\n",
    "for _, embedding in enumerate(embedding_list):\n",
    "    for _,disease in enumerate(disease_list):\n",
    "        disease_data_df = all_df_expanded [all_df_expanded ['disease'] == disease]\n",
    "\n",
    "        if embedding == 'GloVe':\n",
    "            X_train, X_test, y_train, y_test = train_test_split(disease_data_df['vector_tokenized'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "            X_train = vectorize_batch_GloVe(X_train)\n",
    "            X_test = vectorize_batch_GloVe(X_test)\n",
    "        if embedding == 'FastText':\n",
    "            X_train, X_test, y_train, y_test = train_test_split(disease_data_df['vector_tokenized'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "            X_train = vectorize_batch_FastText(X_train)\n",
    "            X_test = vectorize_batch_FastText(X_test)\n",
    "        if embedding == 'USE':\n",
    "            X_train, X_test, y_train, y_test = train_test_split(disease_data_df['sentence_tokenized'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "            X_train = vectorize_batch_USE(X_train)\n",
    "            X_test = vectorize_batch_USE(X_test)\n",
    "\n",
    "        Train_Y  = Encoder.fit_transform(y_train)\n",
    "        Test_Y  = Encoder.fit_transform(y_test)\n",
    "\n",
    "        svm_f1_macro, svm_f1_micro = performSVM(X_train, X_test, Train_Y, Test_Y, embedding)\n",
    "        svm_f1_macro_scores.append(svm_f1_macro)\n",
    "        svm_f1_micro_scores.append(svm_f1_micro)\n",
    "\n",
    "        knn_f1_macro1, knn_f1_micro1, knn_f1_macro5, knn_f1_micro5 = performKNN(X_train, X_test, Train_Y, Test_Y, embedding)\n",
    "        knn1_f1_macro_scores.append(knn_f1_macro1)\n",
    "        knn1_f1_micro_scores.append(knn_f1_micro1)\n",
    "        knn5_f1_macro_scores.append(knn_f1_macro5)\n",
    "        knn5_f1_micro_scores.append(knn_f1_micro5)\n",
    "\n",
    "        nb_f1_macro, nb_f1_micro = performNB(X_train, X_test, Train_Y, Test_Y, embedding)\n",
    "        nb_f1_macro_scores.append(nb_f1_macro)\n",
    "        nb_f1_micro_scores.append(nb_f1_micro)\n",
    "\n",
    "        rf_f1_macro, rf_f1_micro = performRF(X_train, X_test, Train_Y, Test_Y, embedding)\n",
    "        rf_f1_macro_scores.append(rf_f1_macro)\n",
    "        rf_f1_micro_scores.append(rf_f1_micro)\n",
    "        \n",
    "    print(\"Average SVM - \", embedding, \": f1-micro\", mean(svm_f1_micro_scores))\n",
    "    print(\"Average SVM - \", embedding, \": f1-macro\", mean(svm_f1_macro_scores))\n",
    "\n",
    "    print(\"Average KNN 1 - \", embedding, \": f1-micro\", mean(knn1_f1_micro_scores))\n",
    "    print(\"Average KNN 1 - \", embedding, \": f1-macro\", mean(knn1_f1_macro_scores))\n",
    "    print(\"Average KNN 5 - \", embedding, \": f1-micro\", mean(knn5_f1_micro_scores))\n",
    "    print(\"Average KNN 5 - \", embedding, \": f1-macro\", mean(knn5_f1_macro_scores))\n",
    "\n",
    "    print(\"Average NB - \", embedding, \": f1-micro\", mean(nb_f1_micro_scores))\n",
    "    print(\"Average NB - \", embedding, \": f1-macro\", mean(nb_f1_macro_scores))\n",
    "\n",
    "    print(\"Average RF - \", embedding, \": f1-micro\", mean(rf_f1_micro_scores))\n",
    "    print(\"Average RF - \", embedding, \": f1-macro\", mean(rf_f1_macro_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10pK5od01jfTHJyLN94dJxEube3sJszFm",
     "timestamp": 1678482671183
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
