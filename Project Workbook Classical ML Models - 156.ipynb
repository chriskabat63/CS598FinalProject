{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d1ehXFWM5aE"
   },
   "source": [
    "This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n",
    "***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv360l2IkNfO"
   },
   "source": [
    "The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow==2.10.*\n",
    "#pip install tensorflow-text==2.10.0\n",
    "#pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - All Features**\n",
    "\n",
    "![CML TFIDF All](images\\cml-tfidf-all.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - ExtraTreesClassifier Features**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-tfidf-extra.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - InfoGain Features**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-tfidf-infogain.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - SelectKBest Features**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-tfidf-selectkbest.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - Word Embeddings - No Stopwords**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-we-swno.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - Word Embeddings - Stopwords**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-we-swyes.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mclop\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import torchtext\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow_hub as hub\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection, svm, naive_bayes\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# define data path\n",
    "DATA_PATH = './obesity_data/'\n",
    "RESULTS_PATH = './results/'\n",
    "MODELS_PATH = './models/'\n",
    "\n",
    "if os.path.exists(RESULTS_PATH) == False:\n",
    "    os.mkdir(RESULTS_PATH)\n",
    "if os.path.exists(MODELS_PATH) == False:\n",
    "    os.mkdir(MODELS_PATH)\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "#Download info for USE\n",
    "#embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "all_docs_df = pd.read_pickle(DATA_PATH + '/alldocs_df.pkl')\n",
    "all_docs_df_ns = pd.read_pickle(DATA_PATH + '/alldocs_df_ns.pkl')\n",
    "all_annot_df = pd.read_pickle(DATA_PATH + '/allannot_df.pkl')\n",
    "all_df_expanded = pd.read_pickle(DATA_PATH + '/all_df_expanded.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Asthma', 'CHF', 'Depression', 'Diabetes', 'Gallstones', 'Gout', 'Hypercholesterolemia', 'Hypertriglyceridemia', 'OA', 'OSA', 'Obesity', 'CAD', 'Hypertension', 'PVD', 'Venous Insufficiency', 'GERD']\n"
     ]
    }
   ],
   "source": [
    "all_df = pd.merge(all_docs_df,all_annot_df, on='id')\n",
    "all_df_ns = pd.merge(all_docs_df_ns,all_annot_df, on='id')\n",
    "\n",
    "disease_list = all_df['disease'].unique().tolist()\n",
    "print(disease_list)\n",
    "#disease_list = ['Asthma']\n",
    "feature_list = ['All','ExtraTreeClassifier','SelectKBest','InfoGainAttributeVal']\n",
    "embedding_list = ['GloVe', 'FastText', 'USE']\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cols = ['Batch','Disease','Classifier','Feature', 'F1_MACRO', 'F1_MICRO', 'Total Run (secs)']\n",
    "\n",
    "def write_to_file(file, batch_name, disease, clfr, feature,f1_macro,f1_micro,runtime_sec):\n",
    "    #Pass TFIDF or Embeddings\n",
    "    \n",
    "    results_file = f'{RESULTS_PATH}CML_{file}_results.csv'\n",
    "\n",
    "    if os.path.exists(results_file):\n",
    "        results = pd.read_csv(results_file)\n",
    "    else:\n",
    "        results = pd.DataFrame(columns=result_cols)\n",
    "\n",
    "    result = pd.DataFrame(columns=result_cols,data=[[batch_name, disease,clfr,feature,f1_macro,f1_micro,runtime_sec]])\n",
    "    results = pd.concat([results,result])\n",
    "\n",
    "    #Save results - overwrite so we can see progress\n",
    "    results.to_csv(results_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(all_df['tok_lem_text']):\n",
    "    Final_words = []\n",
    "    for word in entry:\n",
    "        Final_words.append(word)\n",
    "    all_df.loc[index, 'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(all_df_ns['tok_lem_text']):\n",
    "    Final_words = []\n",
    "    for word in entry:\n",
    "        Final_words.append(word)\n",
    "    all_df_ns.loc[index, 'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performSVM(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature, disease):\n",
    "    model_name = 'SVM_' + feature + '_' + disease\n",
    "    start_time = time.time()\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf, y_train)\n",
    "\n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec = end_time-start_time\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_SVM)\n",
    "    f1_macro = f1_score(y_test, predictions_SVM,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_SVM,average='micro')\n",
    "\n",
    "    #print(\"SVM - \", disease, \": f1-score\", f1)\n",
    "    print(\"SVM - \", feature, disease, \": f1-macro\", f1_macro)\n",
    "    print(\"SVM - \", feature, disease, \": f1-micro\", f1_micro)\n",
    "    \n",
    "    write_to_file(file, batch_name, disease, \"SVM\", feature,f1_macro, f1_micro, runtime_sec)\n",
    "    \n",
    "    #Save model\n",
    "    torch.save(SVM, f'{MODELS_PATH}{model_name}.pkl')\n",
    "\n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performKNN(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature, disease):\n",
    "    first_model_name = 'KNN1_' + feature + '_' + disease\n",
    "    second_model_name = 'KNN5_' + feature + '_' + disease\n",
    "    start_time = time.time()\n",
    "    knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "    clf1 = knn1.fit(Train_X_Tfidf, y_train)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_KNN1 = clf1.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec1 = end_time-start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "    clf5 = knn5.fit(Train_X_Tfidf, y_train)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_KNN5 = clf5.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec5 = end_time-start_time\n",
    "\n",
    "\n",
    "    #auroc = roc_auc_score(truth, pred[:,1])\n",
    "    #f1 = f1_score(y_test, predictions_KNN)\n",
    "    f1_macro1 = f1_score(y_test, predictions_KNN1,average='macro')\n",
    "    f1_macro5 = f1_score(y_test, predictions_KNN5,average='macro')\n",
    "    f1_micro1 = f1_score(y_test, predictions_KNN1,average='micro')\n",
    "    f1_micro5 = f1_score(y_test, predictions_KNN5,average='micro')\n",
    "\n",
    "    #print(\"KNN - \", disease, \": f1-score\", f1)\n",
    "    print(\"KNN 1 k - \", feature, disease, \": f1-macro\", f1_macro1)\n",
    "    print(\"KNN 1 k - \", feature, disease, \": f1-micro\", f1_micro1)\n",
    "    print(\"KNN 5 k - \", feature, disease, \": f1-macro\", f1_macro5)\n",
    "    print(\"KNN 5 k - \", feature, disease, \": f1-micro\", f1_micro5)\n",
    "    \n",
    "    write_to_file(file, batch_name, disease, \"KNN n=1\", feature,f1_macro1,f1_micro1, runtime_sec1)\n",
    "    write_to_file(file, batch_name, disease, \"KNN n=5\", feature,f1_macro5,f1_micro5, runtime_sec5)\n",
    "    \n",
    "    #Save model\n",
    "    torch.save(clf1, f'{MODELS_PATH}{first_model_name}.pkl')\n",
    "    torch.save(clf5, f'{MODELS_PATH}{second_model_name}.pkl')\n",
    "\n",
    "    return f1_macro1, f1_micro1, f1_macro5, f1_micro5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performNB(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature, disease):\n",
    "    model_name = 'NB_' + feature + '_' + disease\n",
    "    start_time = time.time()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(Train_X_Tfidf)\n",
    "    \n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    Naive.fit(scaler.transform(Train_X_Tfidf),y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec = end_time-start_time\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_NB)\n",
    "    f1_macro = f1_score(y_test, predictions_NB,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_NB,average='micro')\n",
    "\n",
    "    #print(\"NB - \", disease, \": f1-score\", f1)\n",
    "    print(\"NB - \", feature, disease, \": f1-macro\", f1_macro)\n",
    "    print(\"NB - \", feature, disease, \": f1-micro\", f1_micro)\n",
    "\n",
    "    write_to_file(file, batch_name, disease, \"Naive Bayes\", feature,f1_macro, f1_micro, runtime_sec)\n",
    "    \n",
    "    #Save model\n",
    "    torch.save(Naive, f'{MODELS_PATH}{model_name}.pkl')\n",
    "    \n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performRF(Train_X_Tfidf, Test_X_Tfidf, y_train, y_test, feature, disease):\n",
    "    model_name = 'RF_' + feature + '_' + disease\n",
    "    start_time = time.time()\n",
    "\n",
    "    classifier=RandomForestClassifier(n_estimators =400,criterion=\"entropy\",random_state =0)\n",
    "    classifier.fit(Train_X_Tfidf,y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_RF = classifier.predict(Test_X_Tfidf)\n",
    "    end_time = time.time()\n",
    "    runtime_sec = end_time-start_time\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_RF)\n",
    "    f1_macro = f1_score(y_test, predictions_RF,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_RF,average='micro')\n",
    "\n",
    "    #print(\"RF - \", disease, \": f1-score\", f1)\n",
    "    print(\"RF - \", feature, disease, \": f1-macro\", f1_macro)\n",
    "    print(\"RF - \", feature, disease, \": f1-micro\", f1_micro)\n",
    "    \n",
    "    write_to_file(file, batch_name, disease, \"Random Forest\", feature,f1_macro, f1_micro, runtime_sec)\n",
    "    \n",
    "    #Save model\n",
    "    torch.save(classifier, f'{MODELS_PATH}{model_name}.pkl')\n",
    "\n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch_GloVe(X_Train):\n",
    "    embedding_size_used = 300\n",
    "    vec = torchtext.vocab.GloVe(name='6B', dim=embedding_size_used)\n",
    "    \n",
    "    X =  np.zeros((X_Train.shape[0], embedding_size_used * len(X_Train.iloc[0])))\n",
    "    \n",
    "    for i in range(len(X_Train)):\n",
    "        vectors = vec.get_vecs_by_tokens(X_Train.iloc[i]).float().numpy()\n",
    "        \n",
    "        X[i,:] = vectors.flatten()\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch_FastText(X_Train):\n",
    "    embedding_size_used = 300\n",
    "    vec = torchtext.vocab.FastText()\n",
    "    \n",
    "    X =  np.zeros((X_Train.shape[0], embedding_size_used * len(X_Train.iloc[0])))\n",
    "    \n",
    "    for i in range(len(X_Train)):\n",
    "        vectors = vec.get_vecs_by_tokens(X_Train.iloc[i]).float().numpy()\n",
    "        \n",
    "        X[i,:] = vectors.flatten()\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_batch_USE(X_Train):\n",
    "    embedding_size_used = 512\n",
    "    \n",
    "    X =  np.zeros((X_Train.shape[0], embedding_size_used * len(X_Train.iloc[0])))\n",
    "    \n",
    "    for i in range(len(X_Train)):\n",
    "        tensor_flow_vectors = embed(X_Train.iloc[i])\n",
    "        array_vectors = tensor_flow_vectors.numpy()\n",
    "        \n",
    "        X[i,:] = array_vectors.flatten()\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateXTrainAndTest(X_train, X_test, Tfidf_vect):\n",
    "    X_train_values_list = Tfidf_vect.fit_transform(X_train).toarray()\n",
    "    X_training = pd.DataFrame(X_train_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "    X_training = np.asarray(X_training, dtype=float)\n",
    "    X_training = torch.from_numpy(X_training).to(device)\n",
    "\n",
    "    X_test_values_list = Tfidf_vect.transform(X_test).toarray()\n",
    "    X_testing = pd.DataFrame(X_test_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "    X_testing = np.asarray(X_testing, dtype=float)\n",
    "    X_testing = torch.from_numpy(X_testing).to(device)  \n",
    "    \n",
    "    return X_training, X_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "from statistics import mean\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def getVocab(X_train, y_train, feature, max_tokens):\n",
    " \n",
    "    ## Step 1: Determine the Initial Vocabulary\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab = list(tokenizer.word_index.keys())\n",
    "\n",
    "    ## Step 2: Create term  matrix\n",
    "    vectors = tokenizer.texts_to_matrix(X_train, mode='count')\n",
    "\n",
    "    ## Do feature selection on term matrix (column headers are words)\n",
    "    X = vectors\n",
    "    y = y_train\n",
    "\n",
    "    ##Choose algorithm\n",
    "    if feature == 'SelectKBest':\n",
    "        selector = SelectKBest(score_func=f_classif, k=max_tokens).fit(X,y)\n",
    "    else: \n",
    "        if feature == 'InfoGainAttributeVal':\n",
    "            #This should be similar to the InfoGain?\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=max_tokens).fit(X,y)\n",
    "        else:\n",
    "            #default to ExtraTreeClassifier\n",
    "            estimator = ExtraTreeClassifier(random_state = seed)\n",
    "            selector = SelectFromModel(estimator, max_features = max_tokens)\n",
    "            selector = selector.fit(X, y)\n",
    "\n",
    "    support_idx = selector.get_support(True)\n",
    "    tokenizer2 = Tokenizer()\n",
    "    tokenizer2.fit_on_texts([vocab[i-1].replace(\"'\",\"\") for i in support_idx])\n",
    "    new_vocab = list(tokenizer2.word_index.keys())\n",
    "\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN 1 k -  All Asthma : f1-macro 0.5623043623043623\n",
      "KNN 1 k -  All Asthma : f1-micro 0.8537735849056604\n",
      "KNN 5 k -  All Asthma : f1-macro 0.5846627483907081\n",
      "KNN 5 k -  All Asthma : f1-micro 0.9009433962264151\n",
      "KNN 1 k -  All CHF : f1-macro 0.6410891089108911\n",
      "KNN 1 k -  All CHF : f1-micro 0.696551724137931\n",
      "KNN 5 k -  All CHF : f1-macro 0.5880681818181819\n",
      "KNN 5 k -  All CHF : f1-micro 0.7103448275862069\n",
      "KNN 1 k -  All Depression : f1-macro 0.5882870838360452\n",
      "KNN 1 k -  All Depression : f1-micro 0.7242990654205608\n",
      "KNN 5 k -  All Depression : f1-macro 0.5879792399129417\n",
      "KNN 5 k -  All Depression : f1-micro 0.7850467289719625\n",
      "KNN 1 k -  All Diabetes : f1-macro 0.6443392425463335\n",
      "KNN 1 k -  All Diabetes : f1-micro 0.6915887850467289\n",
      "KNN 5 k -  All Diabetes : f1-macro 0.597922192749779\n",
      "KNN 5 k -  All Diabetes : f1-micro 0.6822429906542056\n",
      "KNN 1 k -  All Gallstones : f1-macro 0.53125\n",
      "KNN 1 k -  All Gallstones : f1-micro 0.7\n",
      "KNN 5 k -  All Gallstones : f1-macro 0.5009829124451837\n",
      "KNN 5 k -  All Gallstones : f1-micro 0.7954545454545455\n",
      "KNN 1 k -  All Gout : f1-macro 0.581946936993743\n",
      "KNN 1 k -  All Gout : f1-micro 0.8235294117647058\n",
      "KNN 5 k -  All Gout : f1-macro 0.5463875205254516\n",
      "KNN 5 k -  All Gout : f1-micro 0.8642533936651583\n",
      "KNN 1 k -  All Hypercholesterolemia : f1-macro 0.5631936301911489\n",
      "KNN 1 k -  All Hypercholesterolemia : f1-micro 0.5699481865284974\n",
      "KNN 5 k -  All Hypercholesterolemia : f1-macro 0.598952705510764\n",
      "KNN 5 k -  All Hypercholesterolemia : f1-micro 0.6113989637305699\n",
      "KNN 1 k -  All Hypertriglyceridemia : f1-macro 0.5245046923879041\n",
      "KNN 1 k -  All Hypertriglyceridemia : f1-micro 0.9120370370370371\n",
      "KNN 5 k -  All Hypertriglyceridemia : f1-macro 0.48936170212765956\n",
      "KNN 5 k -  All Hypertriglyceridemia : f1-micro 0.9583333333333334\n",
      "KNN 1 k -  All OA : f1-macro 0.5302352431808343\n",
      "KNN 1 k -  All OA : f1-micro 0.7047619047619048\n",
      "KNN 5 k -  All OA : f1-macro 0.5424836601307189\n",
      "KNN 5 k -  All OA : f1-micro 0.8047619047619048\n",
      "KNN 1 k -  All OSA : f1-macro 0.5860715324222158\n",
      "KNN 1 k -  All OSA : f1-micro 0.7625570776255708\n",
      "KNN 5 k -  All OSA : f1-macro 0.5978699963275799\n",
      "KNN 5 k -  All OSA : f1-micro 0.8401826484018263\n",
      "KNN 1 k -  All Obesity : f1-macro 0.5647321428571428\n",
      "KNN 1 k -  All Obesity : f1-micro 0.5673076923076923\n",
      "KNN 5 k -  All Obesity : f1-macro 0.5935951200914982\n",
      "KNN 5 k -  All Obesity : f1-micro 0.6057692307692307\n",
      "KNN 1 k -  All CAD : f1-macro 0.6731625165403585\n",
      "KNN 1 k -  All CAD : f1-micro 0.6889952153110048\n",
      "KNN 5 k -  All CAD : f1-macro 0.688540596094553\n",
      "KNN 5 k -  All CAD : f1-micro 0.722488038277512\n",
      "KNN 1 k -  All Hypertension : f1-macro 0.5139477143274427\n",
      "KNN 1 k -  All Hypertension : f1-micro 0.6923076923076923\n",
      "KNN 5 k -  All Hypertension : f1-macro 0.5019303132483018\n",
      "KNN 5 k -  All Hypertension : f1-micro 0.7644230769230768\n",
      "KNN 1 k -  All PVD : f1-macro 0.4928030303030303\n",
      "KNN 1 k -  All PVD : f1-micro 0.7475728155339806\n",
      "KNN 5 k -  All PVD : f1-macro 0.5196319405054516\n",
      "KNN 5 k -  All PVD : f1-micro 0.8203883495145631\n",
      "KNN 1 k -  All Venous Insufficiency : f1-macro 0.5815336463223788\n",
      "KNN 1 k -  All Venous Insufficiency : f1-micro 0.8900523560209426\n",
      "KNN 5 k -  All Venous Insufficiency : f1-macro 0.47527472527472525\n",
      "KNN 5 k -  All Venous Insufficiency : f1-micro 0.9057591623036649\n",
      "KNN 1 k -  All GERD : f1-macro 0.5152309204061578\n",
      "KNN 1 k -  All GERD : f1-micro 0.654054054054054\n",
      "KNN 5 k -  All GERD : f1-macro 0.489889705882353\n",
      "KNN 5 k -  All GERD : f1-micro 0.7081081081081082\n"
     ]
    },
    {
     "ename": "StatisticsError",
     "evalue": "mean requires at least one data point",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStatisticsError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m     knn5_f1_micro_scores\u001b[38;5;241m.\u001b[39mappend(knn_f1_micro5)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m#nb_f1_macro, nb_f1_micro = performNB(X_training, X_testing, Train_Y, Test_Y, feature, disease)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m#nb_f1_macro_scores.append(nb_f1_macro)\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m#nb_f1_micro_scores.append(nb_f1_micro)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m#rf_f1_macro_scores.append(rf_f1_macro)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m#rf_f1_micro_scores.append(rf_f1_micro)\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage SVM - \u001b[39m\u001b[38;5;124m\"\u001b[39m, feature, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: f1-micro\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43msvm_f1_micro_scores\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage SVM - \u001b[39m\u001b[38;5;124m\"\u001b[39m, feature, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: f1-macro\u001b[39m\u001b[38;5;124m\"\u001b[39m, mean(svm_f1_macro_scores))\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage KNN 1 - \u001b[39m\u001b[38;5;124m\"\u001b[39m, feature, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: f1-micro\u001b[39m\u001b[38;5;124m\"\u001b[39m, mean(knn1_f1_micro_scores))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\statistics.py:315\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    313\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StatisticsError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean requires at least one data point\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    316\u001b[0m T, total, count \u001b[38;5;241m=\u001b[39m _sum(data)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m count \u001b[38;5;241m==\u001b[39m n\n",
      "\u001b[1;31mStatisticsError\u001b[0m: mean requires at least one data point"
     ]
    }
   ],
   "source": [
    "file = 'TFIDF'\n",
    "result_time = datetime.datetime.now()\n",
    "result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "batch_name = f'CML_tfidf_results_{result_name}'\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Tfidf_vect = TfidfVectorizer(max_features=600)\n",
    "Tfidf_vect_NS = TfidfVectorizer(max_features = 600, stop_words = cachedStopWords)\n",
    "\n",
    "svm_f1_micro_scores = []\n",
    "svm_f1_macro_scores = []\n",
    "knn1_f1_micro_scores = []\n",
    "knn1_f1_macro_scores = []\n",
    "knn5_f1_micro_scores = []\n",
    "knn5_f1_macro_scores = []\n",
    "nb_f1_micro_scores = []\n",
    "nb_f1_macro_scores = []\n",
    "rf_f1_micro_scores = []\n",
    "rf_f1_macro_scores = []\n",
    "\n",
    "max_tokens = 600\n",
    "\n",
    "for _,feature in enumerate(feature_list):\n",
    "    for _,disease in enumerate(disease_list):\n",
    "        disease_data_df = all_df[all_df['disease'] == disease]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(disease_data_df['text_final'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "        \n",
    "        if feature != 'All':\n",
    "            vocab = getVocab(X_train,y_train, feature, max_tokens)\n",
    "            Tfidf_vect = TfidfVectorizer(max_features=max_tokens,vocabulary = vocab)\n",
    "        else:\n",
    "            Tfidf_vect = TfidfVectorizer(max_features=max_tokens)\n",
    "  \n",
    "        \n",
    "        X_train_values_list = Tfidf_vect.fit_transform(X_train).toarray()\n",
    "        X_training = pd.DataFrame(X_train_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "        X_training = np.asarray(X_training, dtype=float)\n",
    "        X_training = torch.from_numpy(X_training).to(device)\n",
    "\n",
    "        X_test_values_list = Tfidf_vect.transform(X_test).toarray()\n",
    "        X_testing = pd.DataFrame(X_test_values_list, columns=Tfidf_vect.get_feature_names_out())\n",
    "        X_testing = np.asarray(X_testing, dtype=float)\n",
    "        X_testing = torch.from_numpy(X_testing).to(device)\n",
    "        \n",
    "        #tokens_to_use = X_training.shape[1]\n",
    "\n",
    "        Train_Y  = Encoder.fit_transform(y_train)\n",
    "        Test_Y  = Encoder.fit_transform(y_test)\n",
    "\n",
    "        svm_f1_macro, svm_f1_micro = performSVM(X_training, X_testing, Train_Y, Test_Y, feature, disease)\n",
    "        svm_f1_macro_scores.append(svm_f1_macro)\n",
    "        svm_f1_micro_scores.append(svm_f1_micro)\n",
    "\n",
    "        knn_f1_macro1, knn_f1_micro1, knn_f1_macro5, knn_f1_micro5 = performKNN(X_training, X_testing, Train_Y, Test_Y, feature, disease)\n",
    "        knn1_f1_macro_scores.append(knn_f1_macro1)\n",
    "        knn1_f1_micro_scores.append(knn_f1_micro1)\n",
    "        knn5_f1_macro_scores.append(knn_f1_macro5)\n",
    "        knn5_f1_micro_scores.append(knn_f1_micro5)\n",
    "\n",
    "        nb_f1_macro, nb_f1_micro = performNB(X_training, X_testing, Train_Y, Test_Y, feature, disease)\n",
    "        nb_f1_macro_scores.append(nb_f1_macro)\n",
    "        nb_f1_micro_scores.append(nb_f1_micro)\n",
    "\n",
    "        rf_f1_macro, rf_f1_micro = performRF(X_training, X_testing, Train_Y, Test_Y, feature, disease)\n",
    "        rf_f1_macro_scores.append(rf_f1_macro)\n",
    "        rf_f1_micro_scores.append(rf_f1_micro)\n",
    "        \n",
    "    print(\"Average SVM - \", feature, \": f1-micro\", mean(svm_f1_micro_scores))\n",
    "    print(\"Average SVM - \", feature, \": f1-macro\", mean(svm_f1_macro_scores))\n",
    "    \n",
    "    print(\"Average KNN 1 - \", feature, \": f1-micro\", mean(knn1_f1_micro_scores))\n",
    "    print(\"Average KNN 1 - \", feature, \": f1-macro\", mean(knn1_f1_macro_scores))\n",
    "    print(\"Average KNN 5 - \", feature, \": f1-micro\", mean(knn5_f1_micro_scores))\n",
    "    print(\"Average KNN 5 - \", feature, \": f1-macro\", mean(knn5_f1_macro_scores))\n",
    "    \n",
    "    print(\"Average NB - \", feature, \": f1-micro\", mean(nb_f1_micro_scores))\n",
    "    print(\"Average NB - \", feature, \": f1-macro\", mean(nb_f1_macro_scores))\n",
    "    \n",
    "    print(\"Average RF - \", feature, \": f1-micro\", mean(rf_f1_micro_scores))\n",
    "    print(\"Average RF - \", feature, \": f1-macro\", mean(rf_f1_macro_scores))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN 1 k -  FastText Asthma : f1-macro 0.45945945945945943\n",
      "KNN 1 k -  FastText Asthma : f1-micro 0.85\n",
      "KNN 5 k -  FastText Asthma : f1-macro 0.45945945945945943\n",
      "KNN 5 k -  FastText Asthma : f1-micro 0.85\n",
      "KNN 1 k -  FastText CHF : f1-macro 0.41724137931034483\n",
      "KNN 1 k -  FastText CHF : f1-micro 0.7159763313609467\n",
      "KNN 5 k -  FastText CHF : f1-macro 0.41724137931034483\n",
      "KNN 5 k -  FastText CHF : f1-micro 0.7159763313609467\n",
      "KNN 1 k -  FastText Depression : f1-macro 0.1947194719471947\n",
      "KNN 1 k -  FastText Depression : f1-micro 0.24180327868852458\n",
      "KNN 5 k -  FastText Depression : f1-macro 0.4312354312354313\n",
      "KNN 5 k -  FastText Depression : f1-micro 0.7581967213114754\n",
      "KNN 1 k -  FastText Diabetes : f1-macro 0.40243902439024387\n",
      "KNN 1 k -  FastText Diabetes : f1-micro 0.673469387755102\n",
      "KNN 5 k -  FastText Diabetes : f1-macro 0.40243902439024387\n",
      "KNN 5 k -  FastText Diabetes : f1-micro 0.673469387755102\n",
      "KNN 1 k -  FastText Gallstones : f1-macro 0.12280701754385964\n",
      "KNN 1 k -  FastText Gallstones : f1-micro 0.14\n",
      "KNN 5 k -  FastText Gallstones : f1-macro 0.12280701754385964\n",
      "KNN 5 k -  FastText Gallstones : f1-micro 0.14\n",
      "KNN 1 k -  FastText Gout : f1-macro 0.4555314533622559\n",
      "KNN 1 k -  FastText Gout : f1-micro 0.8366533864541833\n",
      "KNN 5 k -  FastText Gout : f1-macro 0.4555314533622559\n",
      "KNN 5 k -  FastText Gout : f1-micro 0.8366533864541833\n",
      "KNN 1 k -  FastText Hypercholesterolemia : f1-macro 0.2980769230769231\n",
      "KNN 1 k -  FastText Hypercholesterolemia : f1-micro 0.42465753424657526\n",
      "KNN 5 k -  FastText Hypercholesterolemia : f1-macro 0.2980769230769231\n",
      "KNN 5 k -  FastText Hypercholesterolemia : f1-micro 0.42465753424657526\n",
      "KNN 1 k -  FastText Hypertriglyceridemia : f1-macro 0.48210526315789476\n",
      "KNN 1 k -  FastText Hypertriglyceridemia : f1-micro 0.9308943089430894\n",
      "KNN 5 k -  FastText Hypertriglyceridemia : f1-macro 0.48210526315789476\n",
      "KNN 5 k -  FastText Hypertriglyceridemia : f1-micro 0.9308943089430894\n",
      "KNN 1 k -  FastText OA : f1-macro 0.4323040380047506\n",
      "KNN 1 k -  FastText OA : f1-micro 0.7615062761506276\n",
      "KNN 5 k -  FastText OA : f1-macro 0.4323040380047506\n",
      "KNN 5 k -  FastText OA : f1-micro 0.7615062761506276\n",
      "KNN 1 k -  FastText OSA : f1-macro 0.4577006507592191\n",
      "KNN 1 k -  FastText OSA : f1-micro 0.844\n",
      "KNN 5 k -  FastText OSA : f1-macro 0.4577006507592191\n",
      "KNN 5 k -  FastText OSA : f1-micro 0.844\n",
      "KNN 1 k -  FastText Obesity : f1-macro 0.35694822888283373\n",
      "KNN 1 k -  FastText Obesity : f1-micro 0.5550847457627118\n",
      "KNN 5 k -  FastText Obesity : f1-macro 0.30791788856304986\n",
      "KNN 5 k -  FastText Obesity : f1-micro 0.4449152542372881\n",
      "KNN 1 k -  FastText CAD : f1-macro 0.37760416666666663\n",
      "KNN 1 k -  FastText CAD : f1-micro 0.606694560669456\n",
      "KNN 5 k -  FastText CAD : f1-macro 0.37922077922077924\n",
      "KNN 5 k -  FastText CAD : f1-micro 0.6108786610878661\n",
      "KNN 1 k -  FastText Hypertension : f1-macro 0.45517241379310347\n",
      "KNN 1 k -  FastText Hypertension : f1-micro 0.8354430379746836\n",
      "KNN 5 k -  FastText Hypertension : f1-macro 0.45517241379310347\n",
      "KNN 5 k -  FastText Hypertension : f1-micro 0.8354430379746836\n",
      "KNN 1 k -  FastText PVD : f1-macro 0.13970588235294118\n",
      "KNN 1 k -  FastText PVD : f1-micro 0.1623931623931624\n",
      "KNN 5 k -  FastText PVD : f1-macro 0.45581395348837206\n",
      "KNN 5 k -  FastText PVD : f1-micro 0.8376068376068376\n",
      "KNN 1 k -  FastText Venous Insufficiency : f1-macro 0.4795180722891566\n",
      "KNN 1 k -  FastText Venous Insufficiency : f1-micro 0.9212962962962963\n",
      "KNN 5 k -  FastText Venous Insufficiency : f1-macro 0.4795180722891566\n",
      "KNN 5 k -  FastText Venous Insufficiency : f1-micro 0.9212962962962963\n",
      "KNN 1 k -  FastText GERD : f1-macro 0.43169398907103823\n",
      "KNN 1 k -  FastText GERD : f1-micro 0.7596153846153846\n",
      "KNN 5 k -  FastText GERD : f1-macro 0.43169398907103823\n",
      "KNN 5 k -  FastText GERD : f1-micro 0.7596153846153846\n"
     ]
    },
    {
     "ename": "StatisticsError",
     "evalue": "mean requires at least one data point",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStatisticsError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m     knn1_f1_micro_scores\u001b[38;5;241m.\u001b[39mappend(knn_f1_micro1)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m#knn5_f1_macro_scores.append(knn_f1_macro5)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m#knn5_f1_micro_scores.append(knn_f1_micro5)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m#rf_f1_macro_scores.append(rf_f1_macro)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m#rf_f1_micro_scores.append(rf_f1_micro)\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage SVM - \u001b[39m\u001b[38;5;124m\"\u001b[39m, embedding, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: f1-micro\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43msvm_f1_micro_scores\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage SVM - \u001b[39m\u001b[38;5;124m\"\u001b[39m, embedding, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: f1-macro\u001b[39m\u001b[38;5;124m\"\u001b[39m, mean(svm_f1_macro_scores))\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage KNN 1 - \u001b[39m\u001b[38;5;124m\"\u001b[39m, embedding, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: f1-micro\u001b[39m\u001b[38;5;124m\"\u001b[39m, mean(knn1_f1_micro_scores))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\statistics.py:315\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    313\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StatisticsError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean requires at least one data point\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    316\u001b[0m T, total, count \u001b[38;5;241m=\u001b[39m _sum(data)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m count \u001b[38;5;241m==\u001b[39m n\n",
      "\u001b[1;31mStatisticsError\u001b[0m: mean requires at least one data point"
     ]
    }
   ],
   "source": [
    "file = 'Embeddings'\n",
    "result_time = datetime.datetime.now()\n",
    "result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "batch_name = f'CML_tfidf_results_{result_name}'\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Tfidf_vect = TfidfVectorizer(max_features=600)\n",
    "Tfidf_vect_NS = TfidfVectorizer(max_features = 600, stop_words = cachedStopWords)\n",
    "\n",
    "svm_f1_micro_scores = []\n",
    "svm_f1_macro_scores = []\n",
    "knn1_f1_micro_scores = []\n",
    "knn1_f1_macro_scores = []\n",
    "knn5_f1_micro_scores = []\n",
    "knn5_f1_macro_scores = []\n",
    "nb_f1_micro_scores = []\n",
    "nb_f1_macro_scores = []\n",
    "rf_f1_micro_scores = []\n",
    "rf_f1_macro_scores = []\n",
    "\n",
    "max_tokens = 600\n",
    "\n",
    "embedding_list = ['FastText']\n",
    "\n",
    "for _, embedding in enumerate(embedding_list):\n",
    "    for _,disease in enumerate(disease_list):\n",
    "        disease_data_df = all_df_expanded [all_df_expanded ['disease'] == disease]\n",
    "\n",
    "        if embedding == 'GloVe':\n",
    "            X_train, X_test, y_train, y_test = train_test_split(disease_data_df['vector_tokenized'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "            X_train = vectorize_batch_GloVe(X_train)\n",
    "            X_test = vectorize_batch_GloVe(X_test)\n",
    "        if embedding == 'FastText':\n",
    "            X_train, X_test, y_train, y_test = train_test_split(disease_data_df['vector_tokenized'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "            X_train = vectorize_batch_FastText(X_train)\n",
    "            X_test = vectorize_batch_FastText(X_test)\n",
    "        if embedding == 'USE':\n",
    "            X_train, X_test, y_train, y_test = train_test_split(disease_data_df['sentence_tokenized'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "            X_train = vectorize_batch_USE(X_train)\n",
    "            X_test = vectorize_batch_USE(X_test)\n",
    "\n",
    "        Train_Y  = Encoder.fit_transform(y_train)\n",
    "        Test_Y  = Encoder.fit_transform(y_test)\n",
    "\n",
    "        svm_f1_macro, svm_f1_micro = performSVM(X_train, X_test, Train_Y, Test_Y, embedding, disease)\n",
    "        svm_f1_macro_scores.append(svm_f1_macro)\n",
    "        svm_f1_micro_scores.append(svm_f1_micro)\n",
    "\n",
    "        knn_f1_macro1, knn_f1_micro1, knn_f1_macro5, knn_f1_micro5 = performKNN(X_train, X_test, Train_Y, Test_Y, embedding, disease)\n",
    "        knn1_f1_macro_scores.append(knn_f1_macro1)\n",
    "        knn1_f1_micro_scores.append(knn_f1_micro1)\n",
    "        knn5_f1_macro_scores.append(knn_f1_macro5)\n",
    "        knn5_f1_micro_scores.append(knn_f1_micro5)\n",
    "\n",
    "        nb_f1_macro, nb_f1_micro = performNB(X_train, X_test, Train_Y, Test_Y, embedding, disease)\n",
    "        nb_f1_macro_scores.append(nb_f1_macro)\n",
    "        nb_f1_micro_scores.append(nb_f1_micro)\n",
    "\n",
    "        rf_f1_macro, rf_f1_micro = performRF(X_train, X_test, Train_Y, Test_Y, embedding, disease)\n",
    "        rf_f1_macro_scores.append(rf_f1_macro)\n",
    "        rf_f1_micro_scores.append(rf_f1_micro)\n",
    "        \n",
    "    print(\"Average SVM - \", embedding, \": f1-micro\", mean(svm_f1_micro_scores))\n",
    "    print(\"Average SVM - \", embedding, \": f1-macro\", mean(svm_f1_macro_scores))\n",
    "\n",
    "    print(\"Average KNN 1 - \", embedding, \": f1-micro\", mean(knn1_f1_micro_scores))\n",
    "    print(\"Average KNN 1 - \", embedding, \": f1-macro\", mean(knn1_f1_macro_scores))\n",
    "    print(\"Average KNN 5 - \", embedding, \": f1-micro\", mean(knn5_f1_micro_scores))\n",
    "    print(\"Average KNN 5 - \", embedding, \": f1-macro\", mean(knn5_f1_macro_scores))\n",
    "\n",
    "    print(\"Average NB - \", embedding, \": f1-micro\", mean(nb_f1_micro_scores))\n",
    "    print(\"Average NB - \", embedding, \": f1-macro\", mean(nb_f1_macro_scores))\n",
    "\n",
    "    print(\"Average RF - \", embedding, \": f1-micro\", mean(rf_f1_micro_scores))\n",
    "    print(\"Average RF - \", embedding, \": f1-macro\", mean(rf_f1_macro_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10pK5od01jfTHJyLN94dJxEube3sJszFm",
     "timestamp": 1678482671183
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
