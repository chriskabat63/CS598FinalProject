{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"3d1ehXFWM5aE"},"source":["This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n","***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n","\n","This notebook is for creating the multiple embeddings formats as described in the study.\n","\n"," "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gv360l2IkNfO"},"source":["The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub.\n","\n","We are only creating embeddings for data that includes stop words."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: torchtext in c:\\users\\chris\\appdata\\roaming\\python\\python39\\site-packages (0.15.1)\n","Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (4.64.1)\n","Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (1.21.5)\n","Requirement already satisfied: torchdata==0.6.0 in c:\\users\\chris\\appdata\\roaming\\python\\python39\\site-packages (from torchtext) (0.6.0)\n","Requirement already satisfied: torch==2.0.0 in c:\\users\\chris\\appdata\\roaming\\python\\python39\\site-packages (from torchtext) (2.0.0)\n","Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (2.28.1)\n","Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (2.8.4)\n","Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (1.10.1)\n","Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (2.11.3)\n","Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (4.3.0)\n","Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (3.6.0)\n","Requirement already satisfied: urllib3>=1.25 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchdata==0.6.0->torchtext) (1.26.11)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (2022.9.14)\n","Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.3)\n","Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->torchtext) (0.4.5)\n","Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.0->torchtext) (2.0.1)\n","Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.0->torchtext) (1.2.1)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install torchtext"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1250,"status":"ok","timestamp":1679769727418,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"zyXrAo2dsJqf","outputId":"0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["   Min        Mean   Max         Std  MeanPlusStd\n","0  206  991.907298  3124  437.621770       1430.0\n","1  113  958.774141  3748  444.819254       1404.0\n","Max Tokens: 1430\n","Test: 79 out of 507\n","Train: 72 out of 611\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","DATA_PATH = './obesity_data/'\n","test_df = pd.read_pickle(DATA_PATH + '/test_df.pkl')\n","train_df = pd.read_pickle(DATA_PATH + '/train_df.pkl')\n","test_annot_all_df_clean = pd.read_pickle(DATA_PATH + '/test_annot_all_df_clean.pkl') \n","train_annot_all_df_clean = pd.read_pickle(DATA_PATH + '/train_annot_all_df_clean.pkl') \n","\n","\n","test_df['word_count'] = test_df['lower_text'].apply(lambda x: len(x.split()))\n","train_df['word_count'] = train_df['lower_text'].apply(lambda x: len(x.split()))\n","\n","df_print = pd.DataFrame()\n","df_print['Min'] = [np.min(test_df['word_count']), np.min(train_df['word_count'])]\n","df_print['Mean'] = [np.mean(test_df['word_count']), np.mean(train_df['word_count'])]\n","df_print['Max'] = [np.max(test_df['word_count']), np.max(train_df['word_count'])]\n","df_print['Std'] = [np.std(test_df['word_count']), np.std(train_df['word_count'])]\n","df_print['MeanPlusStd'] = round(df_print['Mean'] + df_print['Std'],0)\n","token_max = int(round(np.max(df_print['MeanPlusStd']),0))\n","\n","print(df_print)\n","print('Max Tokens:',token_max)\n","print('Test:', sum(test_df['word_count'] > token_max), \"out of\", len(test_df))\n","print('Train:', sum(train_df['word_count'] > token_max), \"out of\", len(train_df))\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We are going to split these larger text blocks into 2 notes of size max_token or below.  Note, there are 4 notes (1 in test and 3 in train) that are bigger than 2 times x tokens.  For now, we will ignore, but may want to add in later (either loop or have left/middle/right)."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test: 0 out of 586\n","Train: 0 out of 683\n"]}],"source":["test_df_ok = test_df[test_df['word_count'] <= token_max].copy()\n","test_df_large_right = test_df[test_df['word_count'] > token_max].copy()\n","test_df_large_left = test_df[test_df['word_count'] > token_max].copy()\n","\n","train_df_ok = train_df[train_df['word_count'] <= token_max].copy()\n","train_df_large_right = train_df[train_df['word_count'] > token_max].copy()\n","train_df_large_left = train_df[train_df['word_count'] > token_max].copy()\n","\n","#Get the right words and the left words and then concatenate all 3 and recacluate \n","test_df_large_left['lower_text'] = test_df_large_left['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[:(token_max-1)]]))\n","test_df_large_right['lower_text'] = test_df_large_right['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[token_max:(2*token_max)]]))\n","train_df_large_left['lower_text'] = train_df_large_left['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[:(token_max-1)]]))\n","train_df_large_right['lower_text'] = train_df_large_right['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[token_max:(2*token_max)]]))\n","\n","test_df_expanded = pd.concat([test_df_ok,test_df_large_right,test_df_large_left])\n","test_df_expanded['word_count'] = test_df_expanded['lower_text'].apply(lambda x: len(x.split()))\n","train_df_expanded = pd.concat([train_df_ok,train_df_large_right,train_df_large_left])\n","train_df_expanded['word_count'] = train_df_expanded['lower_text'].apply(lambda x: len(x.split()))\n","\n","print('Test:', sum(test_df_expanded['word_count'] > token_max), \"out of\", len(test_df_expanded))\n","print('Train:', sum(train_df_expanded['word_count'] > token_max), \"out of\", len(train_df_expanded))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We need to create a one hot vector given a vocabulary and pad it with the padding character."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["id                                                                  4\n","text                368346277 | EMH | 64927307 | | 815098 | 3/29/1...\n","no_punc_text        368346277  EMH  64927307   815098  3291993 120...\n","no_numerics_text      emh         am  discharge summary  signed  d...\n","lower_text          emh am discharge summary signed dis admission ...\n","word_count                                                        413\n","one_hot             [7366, 78, 20, 147, 131, 152, 25, 60, 119, 38,...\n","Name: 2, dtype: object\n"]}],"source":["from typing import Union, Iterable\n","import torchtext, torch, torch.nn.functional as F\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","#shouldn't have to do this, need to see why lower_text is removing a bunch of data\n","test_df_expanded[\"no_numerics_text\"] = test_df_expanded['no_numerics_text'].apply(str.lower)\n","train_df_expanded[\"no_numerics_text\"] = train_df_expanded['no_numerics_text'].apply(str.lower)\n","\n","#corpus = pd.concat([test_df_expanded['lower_text'],train_df_expanded['lower_text']])\n","corpus = pd.concat([test_df_expanded['no_numerics_text'],train_df_expanded['no_numerics_text']])\n","tokenizer = get_tokenizer(\"basic_english\")\n","tokens = [tokenizer(doc) for doc in corpus]\n","\n","voc = build_vocab_from_iterator(tokens, specials = ['<pad>'])\n","\n","#so need to create one hot encoding but add <pad> to reach max_tokens\n","#need to do for both stop words and non stop words???\n","def encode_and_pad(vocab, input_tokens, token_max):\n","    pad_zeros = token_max - len(input_tokens)\n","    result = vocab.lookup_indices(input_tokens)\n","    if pad_zeros > 0:\n","        result.extend(np.zeros(pad_zeros, dtype=int))\n","    return result\n","\n","#train_df_expanded['one_hot'] = train_df_expanded['lower_text'].apply(lambda x: encode_and_pad(voc, x.split(), token_max))\n","#test_df_expanded['one_hot'] = test_df_expanded['lower_text'].apply(lambda x: encode_and_pad(voc, x.split(), token_max))\n","train_df_expanded['one_hot'] = train_df_expanded['no_numerics_text'].apply(lambda x: encode_and_pad(voc, x.split(), token_max))\n","test_df_expanded['one_hot'] = test_df_expanded['no_numerics_text'].apply(lambda x: encode_and_pad(voc, x.split(), token_max))\n","\n","print(train_df_expanded.iloc[0])\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Join the test data documents with their associated annotations.  Verify the number of records are the same."]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test: 7542 507 7542\n","Train: 8783 611 8783\n","Test Expanded: 7542 586 8701\n","Train Expanded: 8783 683 9809\n"]}],"source":["test_with_annot_df = pd.merge(test_annot_all_df_clean,test_df, on='id')\n","train_with_annot_df = pd.merge(train_annot_all_df_clean,train_df, on='id')\n","\n","\n","test_with_annot_df_expanded = pd.merge(test_annot_all_df_clean,test_df_expanded,on='id')\n","train_with_annot_df_expanded = pd.merge(train_annot_all_df_clean,train_df_expanded,on='id')\n","\n","\n","\n","print(\"Test:\", len(test_annot_all_df_clean), len(test_df), len(test_with_annot_df))\n","print(\"Train:\", len(train_annot_all_df_clean), len(train_df), len(train_with_annot_df))\n","\n","print(\"Test Expanded:\", len(test_annot_all_df_clean), len(test_df_expanded), len(test_with_annot_df_expanded))\n","print(\"Train Expanded:\", len(train_annot_all_df_clean), len(train_df_expanded), len(train_with_annot_df_expanded))\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Try and validate the numbers are close with the original papers.  You can see the counts are higher for some reason but the percentage occurrence of each disease doesn't change too much so we are good to use the expanded set."]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_29108\\1675168617.py:4: FutureWarning: In a future version of pandas all arguments of Series.sort_index will be keyword-only.\n","  df_before = pd.concat([all_df['disease'].value_counts().sort_index(0),all_df[all_df['judgment']==True]['disease'].value_counts().sort_index(0)/all_df['disease'].value_counts().sort_index(0)],axis =1)\n","C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_29108\\1675168617.py:4: FutureWarning: In a future version of pandas all arguments of Series.sort_index will be keyword-only.\n","  df_before = pd.concat([all_df['disease'].value_counts().sort_index(0),all_df[all_df['judgment']==True]['disease'].value_counts().sort_index(0)/all_df['disease'].value_counts().sort_index(0)],axis =1)\n","C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_29108\\1675168617.py:4: FutureWarning: In a future version of pandas all arguments of Series.sort_index will be keyword-only.\n","  df_before = pd.concat([all_df['disease'].value_counts().sort_index(0),all_df[all_df['judgment']==True]['disease'].value_counts().sort_index(0)/all_df['disease'].value_counts().sort_index(0)],axis =1)\n","C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_29108\\1675168617.py:5: FutureWarning: In a future version of pandas all arguments of Series.sort_index will be keyword-only.\n","  df_after = pd.concat([all_df_extended['disease'].value_counts().sort_index(0),all_df_extended[all_df_extended['judgment']==True]['disease'].value_counts().sort_index(0)/all_df_extended['disease'].value_counts().sort_index(0)],axis =1)\n","C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_29108\\1675168617.py:5: FutureWarning: In a future version of pandas all arguments of Series.sort_index will be keyword-only.\n","  df_after = pd.concat([all_df_extended['disease'].value_counts().sort_index(0),all_df_extended[all_df_extended['judgment']==True]['disease'].value_counts().sort_index(0)/all_df_extended['disease'].value_counts().sort_index(0)],axis =1)\n","C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_29108\\1675168617.py:5: FutureWarning: In a future version of pandas all arguments of Series.sort_index will be keyword-only.\n","  df_after = pd.concat([all_df_extended['disease'].value_counts().sort_index(0),all_df_extended[all_df_extended['judgment']==True]['disease'].value_counts().sort_index(0)/all_df_extended['disease'].value_counts().sort_index(0)],axis =1)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>disease</th>\n","      <th>disease</th>\n","      <th>disease</th>\n","      <th>disease</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Asthma</th>\n","      <td>1057</td>\n","      <td>0.142857</td>\n","      <td>1195</td>\n","      <td>0.143933</td>\n","    </tr>\n","    <tr>\n","      <th>CAD</th>\n","      <td>1044</td>\n","      <td>0.606322</td>\n","      <td>1187</td>\n","      <td>0.608256</td>\n","    </tr>\n","    <tr>\n","      <th>CHF</th>\n","      <td>723</td>\n","      <td>0.672199</td>\n","      <td>838</td>\n","      <td>0.693317</td>\n","    </tr>\n","    <tr>\n","      <th>Depression</th>\n","      <td>1068</td>\n","      <td>0.220974</td>\n","      <td>1211</td>\n","      <td>0.230388</td>\n","    </tr>\n","    <tr>\n","      <th>Diabetes</th>\n","      <td>1070</td>\n","      <td>0.702804</td>\n","      <td>1216</td>\n","      <td>0.718750</td>\n","    </tr>\n","    <tr>\n","      <th>GERD</th>\n","      <td>924</td>\n","      <td>0.239177</td>\n","      <td>1036</td>\n","      <td>0.247104</td>\n","    </tr>\n","    <tr>\n","      <th>Gallstones</th>\n","      <td>1097</td>\n","      <td>0.164084</td>\n","      <td>1244</td>\n","      <td>0.171222</td>\n","    </tr>\n","    <tr>\n","      <th>Gout</th>\n","      <td>1102</td>\n","      <td>0.131579</td>\n","      <td>1250</td>\n","      <td>0.136800</td>\n","    </tr>\n","    <tr>\n","      <th>Hypercholesterolemia</th>\n","      <td>961</td>\n","      <td>0.548387</td>\n","      <td>1088</td>\n","      <td>0.553309</td>\n","    </tr>\n","    <tr>\n","      <th>Hypertension</th>\n","      <td>1037</td>\n","      <td>0.812922</td>\n","      <td>1177</td>\n","      <td>0.816483</td>\n","    </tr>\n","    <tr>\n","      <th>Hypertriglyceridemia</th>\n","      <td>1079</td>\n","      <td>0.059314</td>\n","      <td>1225</td>\n","      <td>0.060408</td>\n","    </tr>\n","    <tr>\n","      <th>OA</th>\n","      <td>1047</td>\n","      <td>0.203438</td>\n","      <td>1187</td>\n","      <td>0.206403</td>\n","    </tr>\n","    <tr>\n","      <th>OSA</th>\n","      <td>1094</td>\n","      <td>0.147166</td>\n","      <td>1243</td>\n","      <td>0.147224</td>\n","    </tr>\n","    <tr>\n","      <th>Obesity</th>\n","      <td>1037</td>\n","      <td>0.446480</td>\n","      <td>1174</td>\n","      <td>0.443782</td>\n","    </tr>\n","    <tr>\n","      <th>PVD</th>\n","      <td>1030</td>\n","      <td>0.157282</td>\n","      <td>1165</td>\n","      <td>0.166524</td>\n","    </tr>\n","    <tr>\n","      <th>Venous Insufficiency</th>\n","      <td>955</td>\n","      <td>0.078534</td>\n","      <td>1074</td>\n","      <td>0.081006</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                      disease   disease  disease   disease\n","Asthma                   1057  0.142857     1195  0.143933\n","CAD                      1044  0.606322     1187  0.608256\n","CHF                       723  0.672199      838  0.693317\n","Depression               1068  0.220974     1211  0.230388\n","Diabetes                 1070  0.702804     1216  0.718750\n","GERD                      924  0.239177     1036  0.247104\n","Gallstones               1097  0.164084     1244  0.171222\n","Gout                     1102  0.131579     1250  0.136800\n","Hypercholesterolemia      961  0.548387     1088  0.553309\n","Hypertension             1037  0.812922     1177  0.816483\n","Hypertriglyceridemia     1079  0.059314     1225  0.060408\n","OA                       1047  0.203438     1187  0.206403\n","OSA                      1094  0.147166     1243  0.147224\n","Obesity                  1037  0.446480     1174  0.443782\n","PVD                      1030  0.157282     1165  0.166524\n","Venous Insufficiency      955  0.078534     1074  0.081006"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["all_df = pd.concat([test_with_annot_df, train_with_annot_df])\n","all_df_extended = pd.concat([test_with_annot_df_expanded, train_with_annot_df_expanded])\n","\n","df_before = pd.concat([all_df['disease'].value_counts().sort_index(0),all_df[all_df['judgment']==True]['disease'].value_counts().sort_index(0)/all_df['disease'].value_counts().sort_index(0)],axis =1)\n","df_after = pd.concat([all_df_extended['disease'].value_counts().sort_index(0),all_df_extended[all_df_extended['judgment']==True]['disease'].value_counts().sort_index(0)/all_df_extended['disease'].value_counts().sort_index(0)],axis =1)\n","df_all = pd.concat([df_before,df_after], axis=1)\n","\n","df_all\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["![Note occurrences](images\\note_occurrences.gif)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Save the final test/train dataset"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["test_with_annot_df_expanded.to_pickle(DATA_PATH + '/test.pkl') \n","train_with_annot_df_expanded.to_pickle(DATA_PATH + '/train.pkl') \n","corpus.to_pickle(DATA_PATH + '/corpus.pkl')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Testing the GloVe embedding"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[7366, 78, 20, 147, 131, 152, 25, 60, 119, 38, 131, 20, 60, 5344, 105, 87, 77, 54, 107, 693, 464, 400, 54, 123, 176, 16, 279, 511, 176, 26, 3, 182, 236, 1, 12, 18, 8, 442, 429, 721, 23121, 50, 30190, 3315, 7, 8, 864, 26, 3, 514, 19, 23, 99, 264, 11, 1, 352, 5289, 4682, 10, 393, 7, 3584, 2626, 10, 2698, 402, 3, 22, 87, 77, 54, 1, 12, 23, 1251, 2, 23, 99, 507, 189, 1721, 13944, 3, 1, 514, 2406, 677, 22, 514, 23, 2069, 2, 19, 2024, 5, 2021, 343, 793, 722, 19, 4, 1690, 10, 1624, 71, 214, 100, 90, 26, 366, 10, 29, 527, 3, 41, 27, 11, 61, 123, 2, 26, 3, 464, 400, 54, 7, 2392, 194, 148, 134, 6, 148, 161, 1, 74, 319, 4, 56, 80, 505, 674, 2, 370, 161, 931, 406, 196, 3518, 56, 142, 80, 2, 132, 16, 712, 1492, 239, 290, 451, 206, 16, 117, 23, 31056, 2373, 386, 3, 405, 22, 148, 161, 4, 422, 6, 22, 3017, 25, 5, 1, 71, 153, 380, 66, 3607, 1116, 17, 1624, 71, 214, 332, 134, 22, 25, 332, 161, 4, 1435, 10, 8, 68, 1350, 2, 1544, 494, 161, 22, 157, 66, 71, 214, 574, 3074, 133, 1379, 11, 972, 665, 2, 187, 384, 583, 11, 1555, 46, 59, 6, 1624, 71, 214, 1, 12, 4, 102, 5, 52, 8, 1488, 40, 472, 724, 87, 77, 421, 19, 23, 157, 198, 3506, 23, 41, 27, 17, 214, 19, 4, 1690, 10, 3017, 87, 77, 345, 1316, 29, 2288, 1746, 670, 4, 243, 19, 4, 444, 4324, 5, 1, 837, 165, 532, 8, 1089, 87, 77, 345, 4, 422, 84, 36, 16, 2969, 285, 897, 1, 12, 111, 4912, 62, 13944, 3, 22, 6735, 903, 19, 23, 16, 400, 285, 22, 2288, 1746, 670, 4, 834, 114, 2263, 2, 19, 23, 16, 2841, 2155, 285, 22, 430, 1819, 17, 20, 4, 654, 2247, 19, 1697, 436, 2318, 3824, 1531, 116, 23, 641, 2, 230, 35, 4, 185, 170, 5, 841, 2755, 1044, 207, 19, 4, 144, 53, 7, 29, 6311, 961, 876, 7, 173, 1889, 17, 3835, 1531, 58, 22, 20, 58, 723, 108, 8, 33, 490, 344, 2829, 9, 85, 33, 2, 578, 69, 19, 49, 173, 7, 22, 637, 1531, 116, 2, 7, 71, 211, 138, 24, 6263, 815, 21321, 39, 226, 104, 6637, 352, 18392, 39, 14146, 11571, 546, 558, 16, 31307, 128, 133, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","torch.Size([1430, 300])\n"]}],"source":["from torchtext.vocab import vocab\n","\n","\n","vec = torchtext.vocab.GloVe(name='6B', dim=300)\n","\n","one_hot_test = train_df_expanded.iloc[0]['one_hot']\n","\n","print(one_hot_test)\n","ret = vec.get_vecs_by_tokens(voc.lookup_tokens(one_hot_test))\n","print(ret.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Testing the FastText embedding."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[7366, 78, 20, 147, 131, 152, 25, 60, 119, 38, 131, 20, 60, 5344, 105, 87, 77, 54, 107, 693, 464, 400, 54, 123, 176, 16, 279, 511, 176, 26, 3, 182, 236, 1, 12, 18, 8, 442, 429, 721, 23121, 50, 30190, 3315, 7, 8, 864, 26, 3, 514, 19, 23, 99, 264, 11, 1, 352, 5289, 4682, 10, 393, 7, 3584, 2626, 10, 2698, 402, 3, 22, 87, 77, 54, 1, 12, 23, 1251, 2, 23, 99, 507, 189, 1721, 13944, 3, 1, 514, 2406, 677, 22, 514, 23, 2069, 2, 19, 2024, 5, 2021, 343, 793, 722, 19, 4, 1690, 10, 1624, 71, 214, 100, 90, 26, 366, 10, 29, 527, 3, 41, 27, 11, 61, 123, 2, 26, 3, 464, 400, 54, 7, 2392, 194, 148, 134, 6, 148, 161, 1, 74, 319, 4, 56, 80, 505, 674, 2, 370, 161, 931, 406, 196, 3518, 56, 142, 80, 2, 132, 16, 712, 1492, 239, 290, 451, 206, 16, 117, 23, 31056, 2373, 386, 3, 405, 22, 148, 161, 4, 422, 6, 22, 3017, 25, 5, 1, 71, 153, 380, 66, 3607, 1116, 17, 1624, 71, 214, 332, 134, 22, 25, 332, 161, 4, 1435, 10, 8, 68, 1350, 2, 1544, 494, 161, 22, 157, 66, 71, 214, 574, 3074, 133, 1379, 11, 972, 665, 2, 187, 384, 583, 11, 1555, 46, 59, 6, 1624, 71, 214, 1, 12, 4, 102, 5, 52, 8, 1488, 40, 472, 724, 87, 77, 421, 19, 23, 157, 198, 3506, 23, 41, 27, 17, 214, 19, 4, 1690, 10, 3017, 87, 77, 345, 1316, 29, 2288, 1746, 670, 4, 243, 19, 4, 444, 4324, 5, 1, 837, 165, 532, 8, 1089, 87, 77, 345, 4, 422, 84, 36, 16, 2969, 285, 897, 1, 12, 111, 4912, 62, 13944, 3, 22, 6735, 903, 19, 23, 16, 400, 285, 22, 2288, 1746, 670, 4, 834, 114, 2263, 2, 19, 23, 16, 2841, 2155, 285, 22, 430, 1819, 17, 20, 4, 654, 2247, 19, 1697, 436, 2318, 3824, 1531, 116, 23, 641, 2, 230, 35, 4, 185, 170, 5, 841, 2755, 1044, 207, 19, 4, 144, 53, 7, 29, 6311, 961, 876, 7, 173, 1889, 17, 3835, 1531, 58, 22, 20, 58, 723, 108, 8, 33, 490, 344, 2829, 9, 85, 33, 2, 578, 69, 19, 49, 173, 7, 22, 637, 1531, 116, 2, 7, 71, 211, 138, 24, 6263, 815, 21321, 39, 226, 104, 6637, 352, 18392, 39, 14146, 11571, 546, 558, 16, 31307, 128, 133, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","torch.Size([1430, 300])\n"]}],"source":["from torchtext.vocab import vocab\n","\n","vec = torchtext.vocab.FastText()\n","\n","one_hot_test = train_df_expanded.iloc[0]['one_hot']\n","\n","print(one_hot_test)\n","ret = vec.get_vecs_by_tokens(voc.lookup_tokens(one_hot_test))\n","print(ret.shape)"]}],"metadata":{"colab":{"provenance":[{"file_id":"10pK5od01jfTHJyLN94dJxEube3sJszFm","timestamp":1678482671183}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
