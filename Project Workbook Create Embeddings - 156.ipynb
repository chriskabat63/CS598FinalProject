{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"3d1ehXFWM5aE"},"source":["This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n","***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n","\n","This notebook is for creating the multiple embeddings formats as described in the study.\n","\n"," "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gv360l2IkNfO"},"source":["The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub.\n","\n","We are only creating embeddings for data that includes stop words."]},{"cell_type":"code","execution_count":72,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1250,"status":"ok","timestamp":1679769727418,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"zyXrAo2dsJqf","outputId":"0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["   Min        Mean   Max         Std  MeanPlusStd\n","0  206  991.907298  3124  437.621770       1430.0\n","1  113  958.774141  3748  444.819254       1404.0\n","Max Tokens: 1430\n","Test: 79 out of 507\n","Train: 72 out of 611\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","DATA_PATH = './obesity_data/'\n","test_df = pd.read_pickle(DATA_PATH + '/test_df.pkl')\n","train_df = pd.read_pickle(DATA_PATH + '/train_df.pkl')\n","\n","test_df['word_count'] = test_df['lower_text'].apply(lambda x: len(x.split()))\n","train_df['word_count'] = train_df['lower_text'].apply(lambda x: len(x.split()))\n","\n","df_print = pd.DataFrame()\n","df_print['Min'] = [np.min(test_df['word_count']), np.min(train_df['word_count'])]\n","df_print['Mean'] = [np.mean(test_df['word_count']), np.mean(train_df['word_count'])]\n","df_print['Max'] = [np.max(test_df['word_count']), np.max(train_df['word_count'])]\n","df_print['Std'] = [np.std(test_df['word_count']), np.std(train_df['word_count'])]\n","df_print['MeanPlusStd'] = round(df_print['Mean'] + df_print['Std'],0)\n","token_max = int(round(np.max(df_print['MeanPlusStd']),0))\n","\n","print(df_print)\n","print('Max Tokens:',token_max)\n","print('Test:', sum(test_df['word_count'] > token_max), \"out of\", len(test_df))\n","print('Train:', sum(train_df['word_count'] > token_max), \"out of\", len(train_df))\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We are going to split these larger text blocks into 2 notes of size max_token or below.  Note, there are 4 notes (1 in test and 3 in train) that are bigger than 2 times x tokens.  For now, we will ignore, but may want to add in later (either loop or have left/middle/right)."]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test: 0 out of 586\n","Train: 0 out of 683\n"]}],"source":["test_df_ok = test_df[test_df['word_count'] <= token_max].copy()\n","test_df_large_right = test_df[test_df['word_count'] > token_max].copy()\n","test_df_large_left = test_df[test_df['word_count'] > token_max].copy()\n","\n","train_df_ok = train_df[train_df['word_count'] <= token_max].copy()\n","train_df_large_right = train_df[train_df['word_count'] > token_max].copy()\n","train_df_large_left = train_df[train_df['word_count'] > token_max].copy()\n","\n","#Get the right words and the left words and then concatenate all 3 and recacluate \n","test_df_large_left['lower_text'] = test_df_large_left['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[:(token_max-1)]]))\n","test_df_large_right['lower_text'] = test_df_large_right['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[token_max:(2*token_max)]]))\n","train_df_large_left['lower_text'] = train_df_large_left['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[:(token_max-1)]]))\n","train_df_large_right['lower_text'] = train_df_large_right['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[token_max:(2*token_max)]]))\n","\n","test_df_expanded = pd.concat([test_df_ok,test_df_large_right,test_df_large_left])\n","test_df_expanded['word_count'] = test_df_expanded['lower_text'].apply(lambda x: len(x.split()))\n","train_df_expanded = pd.concat([train_df_ok,train_df_large_right,train_df_large_left])\n","train_df_expanded['word_count'] = train_df_expanded['lower_text'].apply(lambda x: len(x.split()))\n","\n","print('Test:', sum(test_df_expanded['word_count'] > token_max), \"out of\", len(test_df_expanded))\n","print('Train:', sum(train_df_expanded['word_count'] > token_max), \"out of\", len(train_df_expanded))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["![Note occurrences](images\\note_occurrences.gif)\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"10pK5od01jfTHJyLN94dJxEube3sJszFm","timestamp":1678482671183}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
