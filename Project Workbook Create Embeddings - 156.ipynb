{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"3d1ehXFWM5aE"},"source":["This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n","***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n","\n","This notebook is for creating the multiple embeddings formats as described in the study.\n","\n"," "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gv360l2IkNfO"},"source":["The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub.\n","\n","We are only creating embeddings for data that includes stop words."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1250,"status":"ok","timestamp":1679769727418,"user":{"displayName":"Matthew Lopes","userId":"01980291092524472313"},"user_tz":240},"id":"zyXrAo2dsJqf","outputId":"0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["   Min        Mean   Max         Std  MeanPlusStd\n","0  206  991.907298  3124  437.621770       1430.0\n","1  113  958.774141  3748  444.819254       1404.0\n","Max Tokens: 1430\n","Test: 79 out of 507\n","Train: 72 out of 611\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","DATA_PATH = './obesity_data/'\n","test_df = pd.read_pickle(DATA_PATH + '/test_df.pkl')\n","train_df = pd.read_pickle(DATA_PATH + '/train_df.pkl')\n","\n","test_df['word_count'] = test_df['lower_text'].apply(lambda x: len(x.split()))\n","train_df['word_count'] = train_df['lower_text'].apply(lambda x: len(x.split()))\n","\n","df_print = pd.DataFrame()\n","df_print['Min'] = [np.min(test_df['word_count']), np.min(train_df['word_count'])]\n","df_print['Mean'] = [np.mean(test_df['word_count']), np.mean(train_df['word_count'])]\n","df_print['Max'] = [np.max(test_df['word_count']), np.max(train_df['word_count'])]\n","df_print['Std'] = [np.std(test_df['word_count']), np.std(train_df['word_count'])]\n","df_print['MeanPlusStd'] = round(df_print['Mean'] + df_print['Std'],0)\n","token_max = int(round(np.max(df_print['MeanPlusStd']),0))\n","\n","print(df_print)\n","print('Max Tokens:',token_max)\n","print('Test:', sum(test_df['word_count'] > token_max), \"out of\", len(test_df))\n","print('Train:', sum(train_df['word_count'] > token_max), \"out of\", len(train_df))\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We are going to split these larger text blocks into 2 notes of size max_token or below.  Note, there are 4 notes (1 in test and 3 in train) that are bigger than 2 times x tokens.  For now, we will ignore, but may want to add in later (either loop or have left/middle/right)."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test: 0 out of 586\n","Train: 0 out of 683\n"]}],"source":["test_df_ok = test_df[test_df['word_count'] <= token_max].copy()\n","test_df_large_right = test_df[test_df['word_count'] > token_max].copy()\n","test_df_large_left = test_df[test_df['word_count'] > token_max].copy()\n","\n","train_df_ok = train_df[train_df['word_count'] <= token_max].copy()\n","train_df_large_right = train_df[train_df['word_count'] > token_max].copy()\n","train_df_large_left = train_df[train_df['word_count'] > token_max].copy()\n","\n","#Get the right words and the left words and then concatenate all 3 and recacluate \n","test_df_large_left['lower_text'] = test_df_large_left['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[:(token_max-1)]]))\n","test_df_large_right['lower_text'] = test_df_large_right['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[token_max:(2*token_max)]]))\n","train_df_large_left['lower_text'] = train_df_large_left['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[:(token_max-1)]]))\n","train_df_large_right['lower_text'] = train_df_large_right['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[token_max:(2*token_max)]]))\n","\n","test_df_expanded = pd.concat([test_df_ok,test_df_large_right,test_df_large_left])\n","test_df_expanded['word_count'] = test_df_expanded['lower_text'].apply(lambda x: len(x.split()))\n","train_df_expanded = pd.concat([train_df_ok,train_df_large_right,train_df_large_left])\n","train_df_expanded['word_count'] = train_df_expanded['lower_text'].apply(lambda x: len(x.split()))\n","\n","print('Test:', sum(test_df_expanded['word_count'] > token_max), \"out of\", len(test_df_expanded))\n","print('Train:', sum(train_df_expanded['word_count'] > token_max), \"out of\", len(train_df_expanded))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["![Note occurrences](images\\note_occurrences.gif)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We need to create a one hot vector given a vocabulary and pad it with the padding character."]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["id                                                                  4\n","text                368346277 | EMH | 64927307 | | 815098 | 3/29/1...\n","no_punc_text        368346277  EMH  64927307   815098  3291993 120...\n","no_numerics_text      EMH         AM  Discharge Summary  Signed  D...\n","lower_text          emh am discharge summary signed dis admission ...\n","tokenized_text      [emh, am, discharge, summary, signed, dis, adm...\n","tok_lem_text        [emh, am, discharge, summary, signed, dis, adm...\n","word_count                                                        413\n","one_hot             [7206, 73, 18, 126, 123, 138, 26, 53, 108, 36,...\n","Name: 2, dtype: object\n"]}],"source":["from typing import Union, Iterable\n","import torchtext, torch, torch.nn.functional as F\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","corpus = pd.concat([test_df_expanded['lower_text'],train_df_expanded['lower_text']])\n","tokenizer = get_tokenizer(\"basic_english\")\n","tokens = [tokenizer(doc) for doc in corpus]\n","\n","voc = build_vocab_from_iterator(tokens, specials = ['<pad>'])\n","\n","#so need to create one hot encoding but add <pad> to reach max_tokens\n","#need to do for both stop words and non stop words???\n","def encode_and_pad(vocab, input_tokens, token_max):\n","    pad_zeros = token_max - len(input_tokens)\n","    result = vocab.lookup_indices(input_tokens)\n","    if pad_zeros > 0:\n","        result.extend(np.zeros(pad_zeros, dtype=int))\n","    return result\n","\n","train_df_expanded['one_hot'] = train_df_expanded['lower_text'].apply(lambda x: encode_and_pad(voc, x.split(), token_max))\n","test_df_expanded['one_hot'] = test_df_expanded['lower_text'].apply(lambda x: encode_and_pad(voc, x.split(), token_max))\n","\n","print(train_df_expanded.iloc[0])\n","\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[7206, 73, 18, 126, 123, 138, 26, 53, 108, 36, 123, 18, 53, 5276, 93, 77, 72, 55, 100, 725, 463, 428, 55, 119, 169, 15, 273, 504, 169, 25, 3, 180, 228, 1, 13, 20, 8, 396, 392, 671, 20859, 56, 29809, 3176, 7, 8, 871, 25, 3, 451, 19, 24, 111, 280, 11, 1, 360, 4936, 4478, 10, 380, 7, 3258, 2715, 10, 2840, 398, 3, 22, 77, 72, 55, 1, 13, 24, 1291, 2, 24, 111, 536, 200, 1654, 11662, 3, 1, 451, 2274, 673, 22, 451, 24, 2076, 2, 19, 2147, 5, 2028, 374, 817, 753, 19, 4, 1590, 10, 1477, 66, 202, 97, 86, 25, 417, 10, 30, 498, 3, 41, 27, 11, 60, 119, 2, 25, 3, 463, 428, 55, 7, 2289, 197, 148, 130, 6, 148, 162, 1, 84, 340, 4, 57, 80, 494, 688, 2, 379, 162, 889, 389, 196, 3811, 57, 143, 80, 2, 135, 15, 689, 1475, 239, 288, 452, 215, 15, 114, 24, 30894, 2087, 366, 3, 433, 22, 148, 162, 4, 455, 6, 22, 2768, 26, 5, 1, 66, 157, 395, 63, 3705, 1032, 17, 1477, 66, 202, 313, 130, 22, 26, 313, 162, 4, 1403, 10, 8, 67, 1374, 2, 1736, 534, 162, 22, 150, 63, 66, 202, 657, 2841, 129, 1339, 11, 932, 607, 2, 193, 375, 573, 11, 1370, 45, 58, 6, 1477, 66, 202, 1, 13, 4, 117, 5, 59, 8, 1379, 43, 439, 635, 77, 72, 393, 19, 24, 150, 194, 3686, 24, 41, 27, 17, 202, 19, 4, 1590, 10, 2768, 77, 72, 285, 1171, 30, 2162, 1645, 692, 4, 263, 19, 4, 431, 4451, 5, 1, 820, 167, 514, 8, 1033, 77, 72, 285, 4, 455, 95, 39, 15, 2719, 253, 840, 1, 13, 121, 4535, 65, 11662, 3, 22, 7185, 994, 19, 24, 15, 428, 253, 22, 2162, 1645, 692, 4, 824, 109, 2111, 2, 19, 24, 15, 2863, 1988, 253, 22, 438, 1886, 17, 18, 4, 669, 2245, 19, 1585, 465, 2145, 4382, 1543, 116, 24, 684, 2, 246, 42, 4, 203, 183, 5, 802, 2794, 1047, 191, 19, 4, 142, 51, 7, 30, 6604, 1015, 975, 7, 171, 1897, 17, 3610, 1543, 54, 22, 18, 54, 727, 107, 8, 31, 503, 363, 2540, 9, 81, 31, 2, 585, 70, 19, 48, 171, 7, 22, 609, 1543, 116, 2, 7, 66, 216, 128, 23, 5970, 861, 18607, 33, 251, 99, 6368, 360, 14893, 33, 11766, 10418, 469, 511, 15, 31212, 120, 129, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","torch.Size([1430, 300])\n"]}],"source":["from torchtext.vocab import vocab\n","\n","\n","vec = torchtext.vocab.GloVe(name='6B', dim=300)\n","\n","one_hot_test = train_df_expanded.iloc[0]['one_hot']\n","\n","print(one_hot_test)\n","ret = vec.get_vecs_by_tokens(voc.lookup_tokens(one_hot_test))\n","print(ret.shape)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["int"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["type(train_df_expanded.iloc[0]['one_hot'][0])"]}],"metadata":{"colab":{"provenance":[{"file_id":"10pK5od01jfTHJyLN94dJxEube3sJszFm","timestamp":1678482671183}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
