{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3d1ehXFWM5aE"
   },
   "source": [
    "Title:  Project Workbook Create Embeddings\n",
    "\n",
    "Authors:  Matthew Lopes and Chris Kabat\n",
    "\n",
    "This notebook was created to allow for word/sentence embeddings to be created to support our CS 598 DLH project. We do not actually create the embeddings in this notebook to avoid saving large files, but prepare the data for the creation of them. The paper we have chosen for the reproducibility project is:\n",
    "***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n",
    "\n",
    "Abstract:  The main goal of the paper is to extract Morbidity from clinical notes.  The idea was to use a combination of classical and deep learning methods to determine the best approach for classifying these notes in one or more of 16 morbidity conditions.  These models used a combination of NLP techniques including embeddings and bag of words implementations.  It also measured the effect including of stop words.  Lastly, it used ensemble techniques to tie together a number of the classical and deep learning models to provide the most accurate results.\n",
    "\n",
    "The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub.\n",
    "\n",
    "We are only creating embeddings for data that includes stop words.\n",
    "\n",
    "In this workbook, we are taking the following steps:\n",
    "\n",
    "* Split large documents into smaller sections (left and right)\n",
    "* Create a one-hot encoding representation of the text\n",
    "* Create a tokenized and padded representation of the words and sentences\n",
    "\n",
    " First we load the required libraries and create a new fields that are the count of words and sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1250,
     "status": "ok",
     "timestamp": 1679769727418,
     "user": {
      "displayName": "Matthew Lopes",
      "userId": "01980291092524472313"
     },
     "user_tz": 240
    },
    "id": "zyXrAo2dsJqf",
    "outputId": "0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sentences: 380\n",
      "   Min        Mean   Max         Std  MeanPlusStd\n",
      "0  113  973.840787  3748  441.912616       1416.0\n",
      "1  113  973.840787  3748  441.912616       1416.0\n",
      "Max Tokens: 1416\n",
      "All: 156 out of 1118\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_PATH = './obesity_data/'\n",
    "\n",
    "#Field to tokenize on\n",
    "#tokenize_field = 'lower_text'\n",
    "tokenize_field = 'tok_lem_text'\n",
    "#tokenize_field = 'word_tokenized'\n",
    "isTokenized = True\n",
    "\n",
    "#Don't need to do this for the one with no stop words\n",
    "alldocs_df = pd.read_pickle(DATA_PATH + '/alldocs_df.pkl')\n",
    "allannot_df= pd.read_pickle(DATA_PATH + '/allannot_df.pkl')\n",
    "\n",
    "\n",
    "alldocs_df['sentence_count'] = alldocs_df['sentence_tokenized'].apply(lambda x: len(x))\n",
    "sentence_max = np.max(alldocs_df['sentence_count'])\n",
    "print('Max Sentences:', sentence_max)\n",
    "\n",
    "if isTokenized:\n",
    "    alldocs_df['word_count'] = alldocs_df[tokenize_field].apply(lambda x: len(x))\n",
    "    alldocs_df['word_count'] = alldocs_df[tokenize_field].apply(lambda x: len(x))\n",
    "else:\n",
    "    alldocs_df['word_count'] = alldocs_df[tokenize_field].apply(lambda x: len(x.split()))\n",
    "    alldocs_df['word_count'] = alldocs_df[tokenize_field].apply(lambda x: len(x.split()))\n",
    "\n",
    "df_print = pd.DataFrame()\n",
    "df_print['Min'] = [np.min(alldocs_df['word_count']), np.min(alldocs_df['word_count'])]\n",
    "df_print['Mean'] = [np.mean(alldocs_df['word_count']), np.mean(alldocs_df['word_count'])]\n",
    "df_print['Max'] = [np.max(alldocs_df['word_count']), np.max(alldocs_df['word_count'])]\n",
    "df_print['Std'] = [np.std(alldocs_df['word_count']), np.std(alldocs_df['word_count'])]\n",
    "df_print['MeanPlusStd'] = round(df_print['Mean'] + df_print['Std'],0)\n",
    "token_max = int(round(np.max(df_print['MeanPlusStd']),0))\n",
    "\n",
    "print(df_print)\n",
    "print('Max Tokens:',token_max)\n",
    "print('All:', sum(alldocs_df['word_count'] > token_max), \"out of\", len(alldocs_df))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to split these larger text blocks into 2 notes of size max_token or below.  Note, there are 4 notes (1 in test and 3 in train) that are bigger than 2 times x tokens.  In those cases we are only taking the top and bottom of the document (left and right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: 0 out of 1274\n"
     ]
    }
   ],
   "source": [
    "alldocs_df_ok = alldocs_df[alldocs_df['word_count'] <= token_max].copy()\n",
    "alldocs_df_large_left = alldocs_df[alldocs_df['word_count'] > token_max].copy()\n",
    "alldocs_df_large_right = alldocs_df[alldocs_df['word_count'] > token_max].copy()\n",
    "\n",
    "#Get the right words and the left words and then concatenate all 3 and recacluate \n",
    "if isTokenized:\n",
    "    #alldocs_df_large_left[tokenize_field] = alldocs_df_large_left[tokenize_field].apply(lambda x: [word for word in x[:(token_max-1)]])\n",
    "    #alldocs_df_large_right[tokenize_field] = alldocs_df_large_right[tokenize_field].apply(lambda x: [word for word in x[token_max:(2*token_max)]])\n",
    "    alldocs_df_large_left[tokenize_field] = alldocs_df_large_left[tokenize_field].apply(lambda x: [word for word in x[:(token_max)]])\n",
    "    alldocs_df_large_right[tokenize_field] = alldocs_df_large_right[tokenize_field].apply(lambda x: [word for word in x[(len(x)-token_max):len(x)]])   \n",
    "else:\n",
    "    #alldocs_df_large_left[tokenize_field] = alldocs_df_large_left[tokenize_field].apply(lambda x: ' '.join([word for word in x.split()[:(token_max-1)]]))\n",
    "    #alldocs_df_large_right[tokenize_field] = alldocs_df_large_right[tokenize_field].apply(lambda x: ' '.join([word for word in x.split()[token_max:(2*token_max)]]))\n",
    "    alldocs_df_large_left[tokenize_field] = alldocs_df_large_left[tokenize_field].apply(lambda x: ' '.join([word for word in x.split()[:(token_max)]]))\n",
    "    alldocs_df_large_right[tokenize_field] = alldocs_df_large_right[tokenize_field].apply(lambda x: ' '.join([word for word in x.split()[(len(x.split())-token_max):len(x.split())]]))\n",
    "\n",
    "alldocs_df_expanded = pd.concat([alldocs_df_ok,alldocs_df_large_right,alldocs_df_large_left])\n",
    "\n",
    "if isTokenized:\n",
    "    alldocs_df_expanded['word_count'] = alldocs_df_expanded[tokenize_field].apply(lambda x: len(x))\n",
    "else:\n",
    "    alldocs_df_expanded['word_count'] = alldocs_df_expanded[tokenize_field].apply(lambda x: len(x.split()))\n",
    "\n",
    "print('All:', sum(alldocs_df_expanded['word_count'] > token_max), \"out of\", len(alldocs_df_expanded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentence_tokenized</th>\n",
       "      <th>word_tokenized</th>\n",
       "      <th>no_punc_text</th>\n",
       "      <th>no_numerics_text</th>\n",
       "      <th>lower_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tok_lem_text</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>490646815 | WMC | 31530471 | | 9629480 | 11/23...</td>\n",
       "      <td>[wmc am anemia sign di admiss date report stat...</td>\n",
       "      <td>[wmc, am, anemia, sign, di, admiss, date, repo...</td>\n",
       "      <td>490646815  WMC  31530471   9629480  11232006 1...</td>\n",
       "      <td>WMC         AM  ANEMIA  Signed  DIS  Admissi...</td>\n",
       "      <td>wmc am anemia signed dis admission date report...</td>\n",
       "      <td>[wmc, am, anemia, signed, dis, admission, date...</td>\n",
       "      <td>[wmc, am, anemia, signed, dis, admission, date...</td>\n",
       "      <td>110</td>\n",
       "      <td>1474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0   1  490646815 | WMC | 31530471 | | 9629480 | 11/23...   \n",
       "\n",
       "                                  sentence_tokenized  \\\n",
       "0  [wmc am anemia sign di admiss date report stat...   \n",
       "\n",
       "                                      word_tokenized  \\\n",
       "0  [wmc, am, anemia, sign, di, admiss, date, repo...   \n",
       "\n",
       "                                        no_punc_text  \\\n",
       "0  490646815  WMC  31530471   9629480  11232006 1...   \n",
       "\n",
       "                                    no_numerics_text  \\\n",
       "0    WMC         AM  ANEMIA  Signed  DIS  Admissi...   \n",
       "\n",
       "                                          lower_text  \\\n",
       "0  wmc am anemia signed dis admission date report...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [wmc, am, anemia, signed, dis, admission, date...   \n",
       "\n",
       "                                        tok_lem_text  sentence_count  \\\n",
       "0  [wmc, am, anemia, signed, dis, admission, date...             110   \n",
       "\n",
       "   word_count  \n",
       "0        1474  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldocs_df[alldocs_df['id']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentence_tokenized</th>\n",
       "      <th>word_tokenized</th>\n",
       "      <th>no_punc_text</th>\n",
       "      <th>no_numerics_text</th>\n",
       "      <th>lower_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tok_lem_text</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>490646815 | WMC | 31530471 | | 9629480 | 11/23...</td>\n",
       "      <td>[wmc am anemia sign di admiss date report stat...</td>\n",
       "      <td>[wmc, am, anemia, sign, di, admiss, date, repo...</td>\n",
       "      <td>490646815  WMC  31530471   9629480  11232006 1...</td>\n",
       "      <td>WMC         AM  ANEMIA  Signed  DIS  Admissi...</td>\n",
       "      <td>wmc am anemia signed dis admission date report...</td>\n",
       "      <td>[wmc, am, anemia, signed, dis, admission, date...</td>\n",
       "      <td>[with, ejection, fraction, of, to, who, presen...</td>\n",
       "      <td>110</td>\n",
       "      <td>1416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>490646815 | WMC | 31530471 | | 9629480 | 11/23...</td>\n",
       "      <td>[wmc am anemia sign di admiss date report stat...</td>\n",
       "      <td>[wmc, am, anemia, sign, di, admiss, date, repo...</td>\n",
       "      <td>490646815  WMC  31530471   9629480  11232006 1...</td>\n",
       "      <td>WMC         AM  ANEMIA  Signed  DIS  Admissi...</td>\n",
       "      <td>wmc am anemia signed dis admission date report...</td>\n",
       "      <td>[wmc, am, anemia, signed, dis, admission, date...</td>\n",
       "      <td>[wmc, am, anemia, signed, dis, admission, date...</td>\n",
       "      <td>110</td>\n",
       "      <td>1416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0   1  490646815 | WMC | 31530471 | | 9629480 | 11/23...   \n",
       "0   1  490646815 | WMC | 31530471 | | 9629480 | 11/23...   \n",
       "\n",
       "                                  sentence_tokenized  \\\n",
       "0  [wmc am anemia sign di admiss date report stat...   \n",
       "0  [wmc am anemia sign di admiss date report stat...   \n",
       "\n",
       "                                      word_tokenized  \\\n",
       "0  [wmc, am, anemia, sign, di, admiss, date, repo...   \n",
       "0  [wmc, am, anemia, sign, di, admiss, date, repo...   \n",
       "\n",
       "                                        no_punc_text  \\\n",
       "0  490646815  WMC  31530471   9629480  11232006 1...   \n",
       "0  490646815  WMC  31530471   9629480  11232006 1...   \n",
       "\n",
       "                                    no_numerics_text  \\\n",
       "0    WMC         AM  ANEMIA  Signed  DIS  Admissi...   \n",
       "0    WMC         AM  ANEMIA  Signed  DIS  Admissi...   \n",
       "\n",
       "                                          lower_text  \\\n",
       "0  wmc am anemia signed dis admission date report...   \n",
       "0  wmc am anemia signed dis admission date report...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [wmc, am, anemia, signed, dis, admission, date...   \n",
       "0  [wmc, am, anemia, signed, dis, admission, date...   \n",
       "\n",
       "                                        tok_lem_text  sentence_count  \\\n",
       "0  [with, ejection, fraction, of, to, who, presen...             110   \n",
       "0  [wmc, am, anemia, signed, dis, admission, date...             110   \n",
       "\n",
       "   word_count  \n",
       "0        1416  \n",
       "0        1416  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldocs_df_expanded[alldocs_df_expanded['id']==1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a one hot vector given a vocabulary and pad it with the padding character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emh', 'am', 'discharge', 'summary', 'signed', 'dis', 'admission', 'date', 'report', 'status', 'signed', 'discharge', 'date', 'principle', 'diagnosis', 'coronary', 'artery', 'disease', 'other', 'diagnosis', 'peripheral', 'vascular', 'disease', 'hypertension', 'allergy', 'no', 'known', 'drug', 'allergy', 'history', 'of', 'present', 'illness', 'the', 'patient', 'is', 'a', 'year', 'old', 'male', 'immigrant', 'from', 'tope', 'ri', 'with', 'a', 'long', 'history', 'of', 'angina', 'he', 'had', 'been', 'followed', 'in', 'the', 'o', 'lake', 'jack', 'for', 'year', 'with', 'strong', 'indication', 'for', 'interventional', 'evaluation', 'of', 'his', 'coronary', 'artery', 'disease', 'the', 'patient', 'had', 'refused', 'and', 'had', 'been', 'being', 'treated', 'medically', 'inspite', 'of', 'the', 'angina', 'pattern', 'recently', 'his', 'angina', 'had', 'worsened', 'and', 'he', 'agreed', 'to', 'undergo', 'more', 'intensive', 'workup']\n",
      "[7287, 78, 20, 151, 149, 161, 25, 63, 97, 42, 149, 20, 63, 5312, 85, 81, 73, 57, 105, 85, 501, 434, 57, 125, 87, 16, 287, 428, 87, 26, 3, 140, 260, 1, 10, 19, 6, 186, 425, 713, 20861, 51, 28648, 1958, 8, 6, 884, 26, 3, 505, 18, 22, 109, 276, 12, 1, 321, 4955, 4548, 11, 186, 8, 3243, 2249, 11, 2498, 424, 3, 21, 81, 73, 57, 1, 10, 22, 1206, 2, 22, 109, 535, 200, 1600, 12067, 3, 1, 505, 2191, 684, 21, 505, 22, 2026, 2, 18, 2027, 5, 1881, 373, 831, 727]\n",
      "['emh', 'am', 'discharge', 'summary', 'signed', 'dis', 'admission', 'date', 'report', 'status', 'signed', 'discharge', 'date', 'principle', 'diagnosis', 'coronary', 'artery', 'disease', 'other', 'diagnosis', 'peripheral', 'vascular', 'disease', 'hypertension', 'allergy', 'no', 'known', 'drug', 'allergy', 'history', 'of', 'present', 'illness', 'the', 'patient', 'is', 'a', 'year', 'old', 'male', 'immigrant', 'from', 'tope', 'ri', 'with', 'a', 'long', 'history', 'of', 'angina', 'he', 'had', 'been', 'followed', 'in', 'the', 'o', 'lake', 'jack', 'for', 'year', 'with', 'strong', 'indication', 'for', 'interventional', 'evaluation', 'of', 'his', 'coronary', 'artery', 'disease', 'the', 'patient', 'had', 'refused', 'and', 'had', 'been', 'being', 'treated', 'medically', 'inspite', 'of', 'the', 'angina', 'pattern', 'recently', 'his', 'angina', 'had', 'worsened', 'and', 'he', 'agreed', 'to', 'undergo', 'more', 'intensive', 'workup']\n",
      "['emh am discharg summari sign di admiss date report statu sign discharg date principl diagnosi coronari arteri diseas', 'other diagnos peripher vascular diseas hypertens', 'allergi no known drug allergi', 'histori of present ill the patient is a year old male immigr from tope ri with a long histori of angina', 'he had been follow in the o lake jack for year with strong indic for intervent evalu of hi coronari arteri diseas', 'the patient had refus and had been be treat medic inspit of the angina pattern', 'recent hi angina had worsen and he agre to undergo more intens workup', 'he wa refer for elect cardiac catheter', 'past medic histori hospit for an episod of chest pain in s hypertens and histori of peripher vascular diseas with claudic symptom', \"physic examin on physic exam the patient' temperatur wa heart rate\", 'heent head and neck exam unremark', 'lung clear anteriorli', 'heart regular rate and rhythm no murmur appreci', 'abdomen soft nontend', 'extrem no edema', 'had weakli doppler pul', 'of note hi physic exam wa perform on hi emerg admiss to the cardiac care unit after becom unstabl at elect cardiac catheter', 'laboratori examin hi admiss laboratori exam wa remark for a normal cbc and serum gener exam', 'hi ekg after cardiac catheter demonstr invert t wave in iii f and some st depress in vv', 'hospit cours on elect cardiac catheter the patient wa note to have a ostial left anterior descend coronari arteri lesion', 'he had ekg chang symptomat had chest pain at catheter', 'he wa refer for emerg coronari arteri bypass graft', 'an intraaort balloon pump wa place', 'he wa taken emerg to the oper room where a vessel coronari arteri bypass wa perform', 'there were no intraop complic', 'postop the patient did remark well inspit of hi dramat present', 'he had no vascular complic', 'hi intraaort balloon pump wa remov without incid and he had no specif cardiopulmonari complic', 'hi onli issu at discharg wa urinari retent', 'he fail sever void trial', 'urolog servic had consult and felt thi wa like secondari to benign prostat hypertrophi', 'disposit he wa discharg home with an indwel foley cathet with followup arrang at amc urolog', 'medic hi discharg medic includ aspirin a day iron colac mevacor mg q day and tylenol prn', 'he will followup with hi cardiologist urolog servic and with cardiac surgeri', 'dictat by gail g fahls md', 'ct attend berri o bjornberg md', 'kq zi batch index no', 'wolhxc d t', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "from typing import Union, Iterable\n",
    "import torchtext, torch, torch.nn.functional as F\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "##Words\n",
    "if isTokenized:\n",
    "    voc = build_vocab_from_iterator(alldocs_df_expanded[tokenize_field].to_list(), specials = ['<pad>'])\n",
    "else:\n",
    "    corpus = alldocs_df_expanded[tokenize_field]\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    tokens = [tokenizer(doc) for doc in corpus]\n",
    "    voc = build_vocab_from_iterator(tokens, specials = ['<pad>'])\n",
    "\n",
    "#need to create one hot encoding but add <pad> to reach max_tokens\n",
    "def encode_and_pad(vocab, input_tokens, token_max):\n",
    "    pad_zeros = token_max - len(input_tokens)\n",
    "    result = vocab.lookup_indices(input_tokens)\n",
    "    if pad_zeros > 0:\n",
    "        result.extend(np.zeros(pad_zeros, dtype=int))\n",
    "    return result\n",
    "\n",
    "#need to create tokens add <pad> to reach max_tokens\n",
    "def token_and_pad(vocab, input_tokens, token_max):\n",
    "    pad_zeros = token_max - len(input_tokens)\n",
    "    result = input_tokens\n",
    "    if pad_zeros > 0:\n",
    "        zeros = []\n",
    "        for i in range(pad_zeros):\n",
    "            zeros.append('<pad>')\n",
    "        result.extend(zeros)\n",
    "    return result\n",
    "\n",
    "#need to create tokens add '\\n' to reach max_sentences\n",
    "def token_and_pad_sentence(input_sentences, sentence_max):\n",
    "    pad_spaces = sentence_max - len(input_sentences)\n",
    "    result = input_sentences\n",
    "    if pad_spaces > 0:\n",
    "        for i in range(pad_spaces):\n",
    "            result.append('\\n')\n",
    "\n",
    "    return result\n",
    "\n",
    "if isTokenized:\n",
    "    alldocs_df_expanded['one_hot'] = alldocs_df_expanded[tokenize_field].apply(lambda x: encode_and_pad(voc, x, token_max))\n",
    "    alldocs_df_expanded['vector_tokenized'] = alldocs_df_expanded[tokenize_field].apply(lambda x: token_and_pad(voc, x, token_max))\n",
    "else:\n",
    "    alldocs_df_expanded['one_hot'] = alldocs_df_expanded[tokenize_field].apply(lambda x: encode_and_pad(voc, x.split(), token_max))\n",
    "    alldocs_df_expanded['vector_tokenized'] = alldocs_df_expanded[tokenize_field].apply(lambda x: token_and_pad(voc, x.split(), token_max))\n",
    "\n",
    "alldocs_df_expanded['sentence_tokenized'] = alldocs_df_expanded['sentence_tokenized'].apply(lambda x: token_and_pad_sentence(x, sentence_max))\n",
    "\n",
    "\n",
    "\n",
    "print(alldocs_df_expanded.iloc[0][tokenize_field][0:100])\n",
    "print(alldocs_df_expanded.iloc[0]['one_hot'][0:100])\n",
    "print(alldocs_df_expanded.iloc[0]['vector_tokenized'][0:100])\n",
    "print(alldocs_df_expanded.iloc[0]['sentence_tokenized'][0:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the test data documents with their associated annotations.  Verify the number of records are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: 16325 1118 16325\n",
      "All Expanded: 16325 1274 18584\n"
     ]
    }
   ],
   "source": [
    "all_df = pd.merge(allannot_df,alldocs_df, on='id')\n",
    "all_df_expanded= pd.merge(allannot_df,alldocs_df_expanded,on='id')\n",
    "\n",
    "print(\"All:\", len(allannot_df), len(alldocs_df), len(all_df))\n",
    "print(\"All Expanded:\", len(allannot_df), len(alldocs_df_expanded), len(all_df_expanded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and validate the numbers are close with the original papers.  You can see the counts are higher for some reason but the percentage occurrence of each disease doesn't change too much so we are good to use the expanded set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count Before</th>\n",
       "      <th>% Before</th>\n",
       "      <th>Count After</th>\n",
       "      <th>% After</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Asthma</th>\n",
       "      <td>1057</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.143333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAD</th>\n",
       "      <td>1044</td>\n",
       "      <td>0.606322</td>\n",
       "      <td>1192</td>\n",
       "      <td>0.608221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHF</th>\n",
       "      <td>723</td>\n",
       "      <td>0.672199</td>\n",
       "      <td>841</td>\n",
       "      <td>0.693222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depression</th>\n",
       "      <td>1068</td>\n",
       "      <td>0.220974</td>\n",
       "      <td>1216</td>\n",
       "      <td>0.231086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diabetes</th>\n",
       "      <td>1070</td>\n",
       "      <td>0.702804</td>\n",
       "      <td>1221</td>\n",
       "      <td>0.719902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GERD</th>\n",
       "      <td>924</td>\n",
       "      <td>0.239177</td>\n",
       "      <td>1039</td>\n",
       "      <td>0.246391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gallstones</th>\n",
       "      <td>1097</td>\n",
       "      <td>0.164084</td>\n",
       "      <td>1249</td>\n",
       "      <td>0.172938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gout</th>\n",
       "      <td>1102</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>1255</td>\n",
       "      <td>0.136255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypercholesterolemia</th>\n",
       "      <td>961</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>1092</td>\n",
       "      <td>0.553114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypertension</th>\n",
       "      <td>1037</td>\n",
       "      <td>0.812922</td>\n",
       "      <td>1182</td>\n",
       "      <td>0.816413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypertriglyceridemia</th>\n",
       "      <td>1079</td>\n",
       "      <td>0.059314</td>\n",
       "      <td>1230</td>\n",
       "      <td>0.060163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OA</th>\n",
       "      <td>1047</td>\n",
       "      <td>0.203438</td>\n",
       "      <td>1192</td>\n",
       "      <td>0.205537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSA</th>\n",
       "      <td>1094</td>\n",
       "      <td>0.147166</td>\n",
       "      <td>1248</td>\n",
       "      <td>0.147436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obesity</th>\n",
       "      <td>1037</td>\n",
       "      <td>0.446480</td>\n",
       "      <td>1179</td>\n",
       "      <td>0.445293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PVD</th>\n",
       "      <td>1030</td>\n",
       "      <td>0.157282</td>\n",
       "      <td>1170</td>\n",
       "      <td>0.167521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Venous Insufficiency</th>\n",
       "      <td>955</td>\n",
       "      <td>0.078534</td>\n",
       "      <td>1078</td>\n",
       "      <td>0.080705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Count Before  % Before  Count After   % After\n",
       "Asthma                        1057  0.142857         1200  0.143333\n",
       "CAD                           1044  0.606322         1192  0.608221\n",
       "CHF                            723  0.672199          841  0.693222\n",
       "Depression                    1068  0.220974         1216  0.231086\n",
       "Diabetes                      1070  0.702804         1221  0.719902\n",
       "GERD                           924  0.239177         1039  0.246391\n",
       "Gallstones                    1097  0.164084         1249  0.172938\n",
       "Gout                          1102  0.131579         1255  0.136255\n",
       "Hypercholesterolemia           961  0.548387         1092  0.553114\n",
       "Hypertension                  1037  0.812922         1182  0.816413\n",
       "Hypertriglyceridemia          1079  0.059314         1230  0.060163\n",
       "OA                            1047  0.203438         1192  0.205537\n",
       "OSA                           1094  0.147166         1248  0.147436\n",
       "Obesity                       1037  0.446480         1179  0.445293\n",
       "PVD                           1030  0.157282         1170  0.167521\n",
       "Venous Insufficiency           955  0.078534         1078  0.080705"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_before = pd.concat([all_df['disease'].value_counts().sort_index(0),all_df[all_df['judgment']==True]['disease'].value_counts().sort_index(0)/all_df['disease'].value_counts().sort_index(0)],axis =1)\n",
    "df_after = pd.concat([all_df_expanded['disease'].value_counts().sort_index(0),all_df_expanded[all_df_expanded['judgment']==True]['disease'].value_counts().sort_index(0)/all_df_expanded['disease'].value_counts().sort_index(0)],axis =1)\n",
    "df_all = pd.concat([df_before,df_after], axis=1)\n",
    "\n",
    "mapping = {df_all.columns[0]:'new0', df_all.columns[1]: 'new1'}\n",
    "\n",
    "df_all.columns.values[0] = 'Count Before'\n",
    "df_all.columns.values[1] = '% Before'\n",
    "df_all.columns.values[2] = 'Count After'\n",
    "df_all.columns.values[3] = '% After'\n",
    "df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Note occurrences](images\\note_occurrences.gif)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the final test/train dataset.  We are also saving the vocabulary used and the max number of tokens and sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_pickle(DATA_PATH + '/all_df.pkl') \n",
    "all_df_expanded.to_pickle(DATA_PATH + '/all_df_expanded.pkl') \n",
    "torch.save(voc, DATA_PATH + '/voc.obj')\n",
    "torch.save((token_max, sentence_max), DATA_PATH + '/counts.obj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 318, 351, 3, 5, 153, 140, 51, 403, 8, 6, 1288, 691, 3, 911, 2, 800, 11, 90, 80, 14, 22, 22, 544, 47, 1204, 2, 618, 27, 42, 91, 6, 355, 618, 543, 330, 186, 472, 43, 22, 109, 544, 11, 108, 80, 2, 14, 31, 61, 684, 1187, 6, 59, 3, 1483, 11, 672, 913, 170, 14, 343, 5, 52, 12879, 1093, 1561, 3, 911, 2, 800, 11, 90, 80, 14, 31, 22, 201, 226, 27, 12, 6, 7105, 2315, 970, 15, 47, 867, 14, 4, 333, 5, 54, 6, 179, 3, 469, 51, 1790, 23, 472]\n",
      "['with', 'ejection', 'fraction', 'of', 'to', 'who', 'present', 'from', 'clinic', 'with', 'a', 'chief', 'complaint', 'of', 'fatigue', 'and', 'weakness', 'for', 'one', 'week', 'she', 'had', 'had', 'worsening', 'right', 'groin', 'and', 'hip', 'pain', 'status', 'post', 'a', 'total', 'hip', 'replacement', 'approximately', 'year', 'ago', 'which', 'had', 'been', 'worsening', 'for', 'two', 'week', 'and', 'she', 'ha', 'also', 'recently', 'completed', 'a', 'course', 'of', 'levaquin', 'for', 'urinary', 'tract', 'infection', 'she', 'presented', 'to', 'dr', 'parrent', 'office', 'complaining', 'of', 'fatigue', 'and', 'weakness', 'for', 'one', 'week', 'she', 'ha', 'had', 'some', 'abdominal', 'pain', 'in', 'a', 'bandlike', 'distribution', 'around', 'her', 'right', 'side', 'she', 'wa', 'found', 'to', 'have', 'a', 'hematocrit', 'of', 'down', 'from', 'eight', 'day', 'ago']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "\n",
    "vec = torchtext.vocab.GloVe(name='6B', dim=300)\n",
    "\n",
    "one_hot_test = all_df_expanded.iloc[0]['one_hot']\n",
    "vector_tokenized_test = all_df_expanded.iloc[0]['vector_tokenized']\n",
    "\n",
    "print(one_hot_test[0:100])\n",
    "ret = vec.get_vecs_by_tokens(voc.lookup_tokens(one_hot_test))\n",
    "print(vector_tokenized_test[0:100])\n",
    "ret = vec.get_vecs_by_tokens(vector_tokenized_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the FastText embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 318, 351, 3, 5, 153, 140, 51, 403, 8, 6, 1288, 691, 3, 911, 2, 800, 11, 90, 80, 14, 22, 22, 544, 47, 1204, 2, 618, 27, 42, 91, 6, 355, 618, 543, 330, 186, 472, 43, 22, 109, 544, 11, 108, 80, 2, 14, 31, 61, 684, 1187, 6, 59, 3, 1483, 11, 672, 913, 170, 14, 343, 5, 52, 12879, 1093, 1561, 3, 911, 2, 800, 11, 90, 80, 14, 31, 22, 201, 226, 27, 12, 6, 7105, 2315, 970, 15, 47, 867, 14, 4, 333, 5, 54, 6, 179, 3, 469, 51, 1790, 23, 472]\n",
      "['with', 'ejection', 'fraction', 'of', 'to', 'who', 'present', 'from', 'clinic', 'with', 'a', 'chief', 'complaint', 'of', 'fatigue', 'and', 'weakness', 'for', 'one', 'week', 'she', 'had', 'had', 'worsening', 'right', 'groin', 'and', 'hip', 'pain', 'status', 'post', 'a', 'total', 'hip', 'replacement', 'approximately', 'year', 'ago', 'which', 'had', 'been', 'worsening', 'for', 'two', 'week', 'and', 'she', 'ha', 'also', 'recently', 'completed', 'a', 'course', 'of', 'levaquin', 'for', 'urinary', 'tract', 'infection', 'she', 'presented', 'to', 'dr', 'parrent', 'office', 'complaining', 'of', 'fatigue', 'and', 'weakness', 'for', 'one', 'week', 'she', 'ha', 'had', 'some', 'abdominal', 'pain', 'in', 'a', 'bandlike', 'distribution', 'around', 'her', 'right', 'side', 'she', 'wa', 'found', 'to', 'have', 'a', 'hematocrit', 'of', 'down', 'from', 'eight', 'day', 'ago']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "vec = torchtext.vocab.FastText()\n",
    "\n",
    "one_hot_test = all_df_expanded.iloc[0]['one_hot']\n",
    "\n",
    "print(one_hot_test[0:100])\n",
    "ret = vec.get_vecs_by_tokens(voc.lookup_tokens(one_hot_test))\n",
    "print(vector_tokenized_test[0:100])\n",
    "ret = vec.get_vecs_by_tokens(vector_tokenized_test)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10pK5od01jfTHJyLN94dJxEube3sJszFm",
     "timestamp": 1678482671183
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
