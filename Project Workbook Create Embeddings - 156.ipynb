{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d1ehXFWM5aE"
   },
   "source": [
    "This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n",
    "***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n",
    "\n",
    "This notebook is for creating the multiple embeddings formats as described in the study.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv360l2IkNfO"
   },
   "source": [
    "The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub.\n",
    "\n",
    "We are only creating embeddings for data that includes stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1250,
     "status": "ok",
     "timestamp": 1679769727418,
     "user": {
      "displayName": "Matthew Lopes",
      "userId": "01980291092524472313"
     },
     "user_tz": 240
    },
    "id": "zyXrAo2dsJqf",
    "outputId": "0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = './obesity_data/'\n",
    "\n",
    "#Field to tokenize on\n",
    "#tokenize_field = 'lower_text'\n",
    "tokenize_field = 'tok_lem_text'\n",
    "isTokenized = True\n",
    "\n",
    "#Don't need to do this for the one with no stop words\n",
    "alldocs_df = pd.read_pickle(DATA_PATH + '/alldocs_df.pkl')\n",
    "allannot_df= pd.read_pickle(DATA_PATH + '/allannot_df.pkl')\n",
    "\n",
    "\n",
    "alldocs_df['sentence_count'] = alldocs_df['sentence_tokenized'].apply(lambda x: len(x))\n",
    "sentence_max = np.max(alldocs_df['sentence_count'])\n",
    "print('Max Sentences:', sentence_max)\n",
    "\n",
    "if isTokenized:\n",
    "    alldocs_df['word_count'] = alldocs_df[tokenize_field].apply(lambda x: len(x))\n",
    "    alldocs_df['word_count'] = alldocs_df[tokenize_field].apply(lambda x: len(x))\n",
    "else:\n",
    "    alldocs_df['word_count'] = alldocs_df[tokenize_field].apply(lambda x: len(x.split()))\n",
    "    alldocs_df['word_count'] = alldocs_df[tokenize_field].apply(lambda x: len(x.split()))\n",
    "\n",
    "df_print = pd.DataFrame()\n",
    "df_print['Min'] = [np.min(alldocs_df['word_count']), np.min(alldocs_df['word_count'])]\n",
    "df_print['Mean'] = [np.mean(alldocs_df['word_count']), np.mean(alldocs_df['word_count'])]\n",
    "df_print['Max'] = [np.max(alldocs_df['word_count']), np.max(alldocs_df['word_count'])]\n",
    "df_print['Std'] = [np.std(alldocs_df['word_count']), np.std(alldocs_df['word_count'])]\n",
    "df_print['MeanPlusStd'] = round(df_print['Mean'] + df_print['Std'],0)\n",
    "token_max = int(round(np.max(df_print['MeanPlusStd']),0))\n",
    "\n",
    "print(df_print)\n",
    "print('Max Tokens:',token_max)\n",
    "print('All:', sum(alldocs_df['word_count'] > token_max), \"out of\", len(alldocs_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to split these larger text blocks into 2 notes of size max_token or below.  Note, there are 4 notes (1 in test and 3 in train) that are bigger than 2 times x tokens.  For now, we will ignore, but may want to add in later (either loop or have left/middle/right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldocs_df_ok = alldocs_df[alldocs_df['word_count'] <= token_max].copy()\n",
    "alldocs_df_large_left = alldocs_df[alldocs_df['word_count'] > token_max].copy()\n",
    "alldocs_df_large_right = alldocs_df[alldocs_df['word_count'] > token_max].copy()\n",
    "\n",
    "#Get the right words and the left words and then concatenate all 3 and recacluate \n",
    "if isTokenized:\n",
    "    alldocs_df_large_left[tokenize_field] = alldocs_df_large_left[tokenize_field].apply(lambda x: [word for word in x[:(token_max-1)]])\n",
    "    alldocs_df_large_right[tokenize_field] = alldocs_df_large_right[tokenize_field].apply(lambda x: [word for word in x[token_max:(2*token_max)]])\n",
    "else:\n",
    "    alldocs_df_large_left[tokenize_field] = alldocs_df_large_left[tokenize_field].apply(lambda x: ' '.join([word for word in x.split()[:(token_max-1)]]))\n",
    "    alldocs_df_large_right[tokenize_field] = alldocs_df_large_right[tokenize_field].apply(lambda x: ' '.join([word for word in x.split()[token_max:(2*token_max)]]))\n",
    "\n",
    "alldocs_df_expanded = pd.concat([alldocs_df_ok,alldocs_df_large_right,alldocs_df_large_left])\n",
    "\n",
    "if isTokenized:\n",
    "    alldocs_df_expanded['word_count'] = alldocs_df_expanded[tokenize_field].apply(lambda x: len(x))\n",
    "else:\n",
    "    alldocs_df_expanded['word_count'] = alldocs_df_expanded[tokenize_field].apply(lambda x: len(x.split()))\n",
    "\n",
    "print('All:', sum(alldocs_df_expanded['word_count'] > token_max), \"out of\", len(alldocs_df_expanded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a one hot vector given a vocabulary and pad it with the padding character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Iterable\n",
    "import torchtext, torch, torch.nn.functional as F\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "##Words\n",
    "if isTokenized:\n",
    "    voc = build_vocab_from_iterator(alldocs_df_expanded[tokenize_field].to_list(), specials = ['<pad>'])\n",
    "else:\n",
    "    corpus = alldocs_df_expanded[tokenize_field]\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    tokens = [tokenizer(doc) for doc in corpus]\n",
    "    voc = build_vocab_from_iterator(tokens, specials = ['<pad>'])\n",
    "\n",
    "#need to create one hot encoding but add <pad> to reach max_tokens\n",
    "def encode_and_pad(vocab, input_tokens, token_max):\n",
    "    pad_zeros = token_max - len(input_tokens)\n",
    "    result = vocab.lookup_indices(input_tokens)\n",
    "    if pad_zeros > 0:\n",
    "        result.extend(np.zeros(pad_zeros, dtype=int))\n",
    "    return result\n",
    "\n",
    "#need to create tokens add <pad> to reach max_tokens\n",
    "def token_and_pad(vocab, input_tokens, token_max):\n",
    "    pad_zeros = token_max - len(input_tokens)\n",
    "    result = input_tokens\n",
    "    if pad_zeros > 0:\n",
    "        zeros = []\n",
    "        for i in range(pad_zeros):\n",
    "            zeros.append('<pad>')\n",
    "        result.extend(zeros)\n",
    "    return result\n",
    "\n",
    "#need to create tokens add '\\n' to reach max_sentences\n",
    "def token_and_pad_sentence(input_sentences, sentence_max):\n",
    "    pad_spaces = sentence_max - len(input_sentences)\n",
    "    result = input_sentences\n",
    "    if pad_spaces > 0:\n",
    "        for i in range(pad_spaces):\n",
    "            result.append('\\n')\n",
    "\n",
    "    return result\n",
    "\n",
    "if isTokenized:\n",
    "    alldocs_df_expanded['one_hot'] = alldocs_df_expanded[tokenize_field].apply(lambda x: encode_and_pad(voc, x, token_max))\n",
    "    alldocs_df_expanded['vector_tokenized'] = alldocs_df_expanded[tokenize_field].apply(lambda x: token_and_pad(voc, x, token_max))\n",
    "else:\n",
    "    alldocs_df_expanded['one_hot'] = alldocs_df_expanded[tokenize_field].apply(lambda x: encode_and_pad(voc, x.split(), token_max))\n",
    "    alldocs_df_expanded['vector_tokenized'] = alldocs_df_expanded[tokenize_field].apply(lambda x: token_and_pad(voc, x.split(), token_max))\n",
    "\n",
    "alldocs_df_expanded['sentence_tokenized'] = alldocs_df_expanded['sentence_tokenized'].apply(lambda x: token_and_pad_sentence(x, sentence_max))\n",
    "\n",
    "\n",
    "\n",
    "print(alldocs_df_expanded.iloc[0][tokenize_field])\n",
    "print(alldocs_df_expanded.iloc[0]['one_hot'])\n",
    "print(alldocs_df_expanded.iloc[0]['vector_tokenized'])\n",
    "print(alldocs_df_expanded.iloc[0]['sentence_tokenized'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the test data documents with their associated annotations.  Verify the number of records are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.merge(allannot_df,alldocs_df, on='id')\n",
    "all_df_expanded= pd.merge(allannot_df,alldocs_df_expanded,on='id')\n",
    "\n",
    "\n",
    "\n",
    "print(\"All:\", len(allannot_df), len(alldocs_df), len(all_df))\n",
    "print(\"All Expanded:\", len(allannot_df), len(alldocs_df_expanded), len(all_df_expanded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and validate the numbers are close with the original papers.  You can see the counts are higher for some reason but the percentage occurrence of each disease doesn't change too much so we are good to use the expanded set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = pd.concat([all_df['disease'].value_counts().sort_index(0),all_df[all_df['judgment']==True]['disease'].value_counts().sort_index(0)/all_df['disease'].value_counts().sort_index(0)],axis =1)\n",
    "df_after = pd.concat([all_df_expanded['disease'].value_counts().sort_index(0),all_df_expanded[all_df_expanded['judgment']==True]['disease'].value_counts().sort_index(0)/all_df_expanded['disease'].value_counts().sort_index(0)],axis =1)\n",
    "df_all = pd.concat([df_before,df_after], axis=1)\n",
    "\n",
    "df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Note occurrences](images\\note_occurrences.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the final test/train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_pickle(DATA_PATH + '/all_df.pkl') \n",
    "all_df_expanded.to_pickle(DATA_PATH + '/all_df_expanded.pkl') \n",
    "#corpus.to_pickle(DATA_PATH + '/corpus.pkl')\n",
    "torch.save(voc, DATA_PATH + '/voc.obj')\n",
    "torch.save((token_max, sentence_max), DATA_PATH + '/counts.obj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "\n",
    "vec = torchtext.vocab.GloVe(name='6B', dim=300)\n",
    "\n",
    "one_hot_test = all_df_expanded.iloc[0]['one_hot']\n",
    "vector_tokenized_test = all_df_expanded.iloc[0]['vector_tokenized']\n",
    "\n",
    "print(one_hot_test)\n",
    "ret = vec.get_vecs_by_tokens(voc.lookup_tokens(one_hot_test))\n",
    "print(ret.shape)\n",
    "#print(ret[0])\n",
    "print(vector_tokenized_test)\n",
    "ret = vec.get_vecs_by_tokens(vector_tokenized_test)\n",
    "print(ret.shape)\n",
    "#print(ret[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the FastText embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "vec = torchtext.vocab.FastText()\n",
    "\n",
    "one_hot_test = all_df_expanded.iloc[0]['one_hot']\n",
    "\n",
    "print(one_hot_test)\n",
    "ret = vec.get_vecs_by_tokens(voc.lookup_tokens(one_hot_test))\n",
    "print(ret.shape)\n",
    "print(vector_tokenized_test)\n",
    "ret = vec.get_vecs_by_tokens(vector_tokenized_test)\n",
    "print(ret.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10pK5od01jfTHJyLN94dJxEube3sJszFm",
     "timestamp": 1678482671183
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
