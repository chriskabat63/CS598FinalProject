{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d1ehXFWM5aE"
   },
   "source": [
    "This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n",
    "***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n",
    "\n",
    "This notebook is for creating the multiple embeddings formats as described in the study.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv360l2IkNfO"
   },
   "source": [
    "The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub.\n",
    "\n",
    "We are only creating embeddings for data that includes stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torchtext in c:\\users\\chris\\appdata\\roaming\\python\\python39\\site-packages (0.15.1)\n",
      "Requirement already satisfied: torch==2.0.0 in c:\\users\\chris\\appdata\\roaming\\python\\python39\\site-packages (from torchtext) (2.0.0)\n",
      "Requirement already satisfied: torchdata==0.6.0 in c:\\users\\chris\\appdata\\roaming\\python\\python39\\site-packages (from torchtext) (0.6.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (2.28.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (1.21.5)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (2.8.4)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (4.3.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (2.11.3)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchdata==0.6.0->torchtext) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->torchtext) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.0->torchtext) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.0->torchtext) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1250,
     "status": "ok",
     "timestamp": 1679769727418,
     "user": {
      "displayName": "Matthew Lopes",
      "userId": "01980291092524472313"
     },
     "user_tz": 240
    },
    "id": "zyXrAo2dsJqf",
    "outputId": "0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Min        Mean   Max         Std  MeanPlusStd\n",
      "0  206  991.907298  3124  437.621770       1430.0\n",
      "1  113  958.774141  3748  444.819254       1404.0\n",
      "Max Tokens: 1430\n",
      "Test: 79 out of 507\n",
      "Train: 72 out of 611\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = './obesity_data/'\n",
    "test_df = pd.read_pickle(DATA_PATH + '/test_df.pkl')\n",
    "train_df = pd.read_pickle(DATA_PATH + '/train_df.pkl')\n",
    "test_annot_all_df_clean = pd.read_pickle(DATA_PATH + '/test_annot_all_df_clean.pkl') \n",
    "train_annot_all_df_clean = pd.read_pickle(DATA_PATH + '/train_annot_all_df_clean.pkl') \n",
    "\n",
    "test_df['word_count'] = test_df['lower_text'].apply(lambda x: len(x.split()))\n",
    "train_df['word_count'] = train_df['lower_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "df_print = pd.DataFrame()\n",
    "df_print['Min'] = [np.min(test_df['word_count']), np.min(train_df['word_count'])]\n",
    "df_print['Mean'] = [np.mean(test_df['word_count']), np.mean(train_df['word_count'])]\n",
    "df_print['Max'] = [np.max(test_df['word_count']), np.max(train_df['word_count'])]\n",
    "df_print['Std'] = [np.std(test_df['word_count']), np.std(train_df['word_count'])]\n",
    "df_print['MeanPlusStd'] = round(df_print['Mean'] + df_print['Std'],0)\n",
    "token_max = int(round(np.max(df_print['MeanPlusStd']),0))\n",
    "\n",
    "print(df_print)\n",
    "print('Max Tokens:',token_max)\n",
    "print('Test:', sum(test_df['word_count'] > token_max), \"out of\", len(test_df))\n",
    "print('Train:', sum(train_df['word_count'] > token_max), \"out of\", len(train_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to split these larger text blocks into 2 notes of size max_token or below.  Note, there are 4 notes (1 in test and 3 in train) that are bigger than 2 times x tokens.  For now, we will ignore, but may want to add in later (either loop or have left/middle/right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0 out of 586\n",
      "Train: 0 out of 683\n"
     ]
    }
   ],
   "source": [
    "test_df_ok = test_df[test_df['word_count'] <= token_max].copy()\n",
    "test_df_large_right = test_df[test_df['word_count'] > token_max].copy()\n",
    "test_df_large_left = test_df[test_df['word_count'] > token_max].copy()\n",
    "\n",
    "train_df_ok = train_df[train_df['word_count'] <= token_max].copy()\n",
    "train_df_large_right = train_df[train_df['word_count'] > token_max].copy()\n",
    "train_df_large_left = train_df[train_df['word_count'] > token_max].copy()\n",
    "\n",
    "#Get the right words and the left words and then concatenate all 3 and recacluate \n",
    "test_df_large_left['lower_text'] = test_df_large_left['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[:(token_max-1)]]))\n",
    "test_df_large_right['lower_text'] = test_df_large_right['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[token_max:(2*token_max)]]))\n",
    "train_df_large_left['lower_text'] = train_df_large_left['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[:(token_max-1)]]))\n",
    "train_df_large_right['lower_text'] = train_df_large_right['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[token_max:(2*token_max)]]))\n",
    "\n",
    "test_df_expanded = pd.concat([test_df_ok,test_df_large_right,test_df_large_left])\n",
    "test_df_expanded['word_count'] = test_df_expanded['lower_text'].apply(lambda x: len(x.split()))\n",
    "train_df_expanded = pd.concat([train_df_ok,train_df_large_right,train_df_large_left])\n",
    "train_df_expanded['word_count'] = train_df_expanded['lower_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print('Test:', sum(test_df_expanded['word_count'] > token_max), \"out of\", len(test_df_expanded))\n",
    "print('Train:', sum(train_df_expanded['word_count'] > token_max), \"out of\", len(train_df_expanded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a one hot vector given a vocabulary and pad it with the padding character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emh am discharge summary signed dis admission date report status signed discharge date principle diagnosis coronary artery disease other diagnoses peripheral vascular disease hypertension allergies no known drug allergies history of present illness the patient is a year old male immigrant from tope ri with a long history of angina he had been followed in the o lake jack for years with strong indication for interventional evaluation of his coronary artery disease the patient had refused and had been being treated medically inspite of the angina pattern recently his angina had worsened and he agreed to undergo more intensive workup he was referred for elective cardiac catheterization past medical history hospitalization for an episode of chest pain in s hypertension and history of peripheral vascular disease with claudication symptoms physical examination on physical exam the patients temperature was heart rate heent head and neck exam unremarkable lungs clear anteriorly heart regular rate and rhythm no murmurs appreciated abdomen soft nontender extremities no edema had weakly dopplerable pulses of note his physical exam was performed on his emergent admission to the cardiac care unit after becoming unstable at elective cardiac catheterization laboratory examination his admission laboratory exam was remarkable for a normal cbc and serum general exam his ekg after cardiac catheterization demonstrated inverted t waves in iii f and some st depression in vv hospital course on elective cardiac catheterization the patient was noted to have a ostial left anterior descending coronary artery lesion he had ekg changes symptomatically had chest pain at catheterization he was referred for emergent coronary artery bypass grafting an intraaortic balloon pump was placed he was taken emergently to the operating room where a vessel coronary artery bypass was performed there were no intraoperative complications postoperatively the patient did remarkably well inspite of his dramatic presentation he had no vascular complications his intraaortic balloon pump was removed without incident and he had no specific cardiopulmonary complications his only issue at discharge was urinary retention he failed several voiding trials urology service had consulted and felt this was likely secondary to benign prostatic hypertrophy disposition he was discharged home with an indwelling foley catheter with followup arranged at amc urology medications his discharge medications include aspirin a day iron colace mevacor mg q day and tylenol prn he will followup with his cardiologist urology service and with cardiac surgery dictated by gail g fahlsing md ct attending berry o bjornberg md kq zi batch index no wolhxc d t\n",
      "[7206, 73, 18, 126, 123, 138, 26, 53, 108, 36, 123, 18, 53, 5276, 93, 77, 72, 55, 100, 725, 463, 428, 55, 119, 169, 15, 273, 504, 169, 25, 3, 180, 228, 1, 13, 20, 8, 396, 392, 671, 20859, 56, 29809, 3176, 7, 8, 871, 25, 3, 451, 19, 24, 111, 280, 11, 1, 360, 4936, 4478, 10, 380, 7, 3258, 2715, 10, 2840, 398, 3, 22, 77, 72, 55, 1, 13, 24, 1291, 2, 24, 111, 536, 200, 1654, 11662, 3, 1, 451, 2274, 673, 22, 451, 24, 2076, 2, 19, 2147, 5, 2028, 374, 817, 753, 19, 4, 1590, 10, 1477, 66, 202, 97, 86, 25, 417, 10, 30, 498, 3, 41, 27, 11, 60, 119, 2, 25, 3, 463, 428, 55, 7, 2289, 197, 148, 130, 6, 148, 162, 1, 84, 340, 4, 57, 80, 494, 688, 2, 379, 162, 889, 389, 196, 3811, 57, 143, 80, 2, 135, 15, 689, 1475, 239, 288, 452, 215, 15, 114, 24, 30894, 2087, 366, 3, 433, 22, 148, 162, 4, 455, 6, 22, 2768, 26, 5, 1, 66, 157, 395, 63, 3705, 1032, 17, 1477, 66, 202, 313, 130, 22, 26, 313, 162, 4, 1403, 10, 8, 67, 1374, 2, 1736, 534, 162, 22, 150, 63, 66, 202, 657, 2841, 129, 1339, 11, 932, 607, 2, 193, 375, 573, 11, 1370, 45, 58, 6, 1477, 66, 202, 1, 13, 4, 117, 5, 59, 8, 1379, 43, 439, 635, 77, 72, 393, 19, 24, 150, 194, 3686, 24, 41, 27, 17, 202, 19, 4, 1590, 10, 2768, 77, 72, 285, 1171, 30, 2162, 1645, 692, 4, 263, 19, 4, 431, 4451, 5, 1, 820, 167, 514, 8, 1033, 77, 72, 285, 4, 455, 95, 39, 15, 2719, 253, 840, 1, 13, 121, 4535, 65, 11662, 3, 22, 7185, 994, 19, 24, 15, 428, 253, 22, 2162, 1645, 692, 4, 824, 109, 2111, 2, 19, 24, 15, 2863, 1988, 253, 22, 438, 1886, 17, 18, 4, 669, 2245, 19, 1585, 465, 2145, 4382, 1543, 116, 24, 684, 2, 246, 42, 4, 203, 183, 5, 802, 2794, 1047, 191, 19, 4, 142, 51, 7, 30, 6604, 1015, 975, 7, 171, 1897, 17, 3610, 1543, 54, 22, 18, 54, 727, 107, 8, 31, 503, 363, 2540, 9, 81, 31, 2, 585, 70, 19, 48, 171, 7, 22, 609, 1543, 116, 2, 7, 66, 216, 128, 23, 5970, 861, 18607, 33, 251, 99, 6368, 360, 14893, 33, 11766, 10418, 469, 511, 15, 31212, 120, 129, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['emh', 'am', 'discharge', 'summary', 'signed', 'dis', 'admission', 'date', 'report', 'status', 'signed', 'discharge', 'date', 'principle', 'diagnosis', 'coronary', 'artery', 'disease', 'other', 'diagnoses', 'peripheral', 'vascular', 'disease', 'hypertension', 'allergies', 'no', 'known', 'drug', 'allergies', 'history', 'of', 'present', 'illness', 'the', 'patient', 'is', 'a', 'year', 'old', 'male', 'immigrant', 'from', 'tope', 'ri', 'with', 'a', 'long', 'history', 'of', 'angina', 'he', 'had', 'been', 'followed', 'in', 'the', 'o', 'lake', 'jack', 'for', 'years', 'with', 'strong', 'indication', 'for', 'interventional', 'evaluation', 'of', 'his', 'coronary', 'artery', 'disease', 'the', 'patient', 'had', 'refused', 'and', 'had', 'been', 'being', 'treated', 'medically', 'inspite', 'of', 'the', 'angina', 'pattern', 'recently', 'his', 'angina', 'had', 'worsened', 'and', 'he', 'agreed', 'to', 'undergo', 'more', 'intensive', 'workup', 'he', 'was', 'referred', 'for', 'elective', 'cardiac', 'catheterization', 'past', 'medical', 'history', 'hospitalization', 'for', 'an', 'episode', 'of', 'chest', 'pain', 'in', 's', 'hypertension', 'and', 'history', 'of', 'peripheral', 'vascular', 'disease', 'with', 'claudication', 'symptoms', 'physical', 'examination', 'on', 'physical', 'exam', 'the', 'patients', 'temperature', 'was', 'heart', 'rate', 'heent', 'head', 'and', 'neck', 'exam', 'unremarkable', 'lungs', 'clear', 'anteriorly', 'heart', 'regular', 'rate', 'and', 'rhythm', 'no', 'murmurs', 'appreciated', 'abdomen', 'soft', 'nontender', 'extremities', 'no', 'edema', 'had', 'weakly', 'dopplerable', 'pulses', 'of', 'note', 'his', 'physical', 'exam', 'was', 'performed', 'on', 'his', 'emergent', 'admission', 'to', 'the', 'cardiac', 'care', 'unit', 'after', 'becoming', 'unstable', 'at', 'elective', 'cardiac', 'catheterization', 'laboratory', 'examination', 'his', 'admission', 'laboratory', 'exam', 'was', 'remarkable', 'for', 'a', 'normal', 'cbc', 'and', 'serum', 'general', 'exam', 'his', 'ekg', 'after', 'cardiac', 'catheterization', 'demonstrated', 'inverted', 't', 'waves', 'in', 'iii', 'f', 'and', 'some', 'st', 'depression', 'in', 'vv', 'hospital', 'course', 'on', 'elective', 'cardiac', 'catheterization', 'the', 'patient', 'was', 'noted', 'to', 'have', 'a', 'ostial', 'left', 'anterior', 'descending', 'coronary', 'artery', 'lesion', 'he', 'had', 'ekg', 'changes', 'symptomatically', 'had', 'chest', 'pain', 'at', 'catheterization', 'he', 'was', 'referred', 'for', 'emergent', 'coronary', 'artery', 'bypass', 'grafting', 'an', 'intraaortic', 'balloon', 'pump', 'was', 'placed', 'he', 'was', 'taken', 'emergently', 'to', 'the', 'operating', 'room', 'where', 'a', 'vessel', 'coronary', 'artery', 'bypass', 'was', 'performed', 'there', 'were', 'no', 'intraoperative', 'complications', 'postoperatively', 'the', 'patient', 'did', 'remarkably', 'well', 'inspite', 'of', 'his', 'dramatic', 'presentation', 'he', 'had', 'no', 'vascular', 'complications', 'his', 'intraaortic', 'balloon', 'pump', 'was', 'removed', 'without', 'incident', 'and', 'he', 'had', 'no', 'specific', 'cardiopulmonary', 'complications', 'his', 'only', 'issue', 'at', 'discharge', 'was', 'urinary', 'retention', 'he', 'failed', 'several', 'voiding', 'trials', 'urology', 'service', 'had', 'consulted', 'and', 'felt', 'this', 'was', 'likely', 'secondary', 'to', 'benign', 'prostatic', 'hypertrophy', 'disposition', 'he', 'was', 'discharged', 'home', 'with', 'an', 'indwelling', 'foley', 'catheter', 'with', 'followup', 'arranged', 'at', 'amc', 'urology', 'medications', 'his', 'discharge', 'medications', 'include', 'aspirin', 'a', 'day', 'iron', 'colace', 'mevacor', 'mg', 'q', 'day', 'and', 'tylenol', 'prn', 'he', 'will', 'followup', 'with', 'his', 'cardiologist', 'urology', 'service', 'and', 'with', 'cardiac', 'surgery', 'dictated', 'by', 'gail', 'g', 'fahlsing', 'md', 'ct', 'attending', 'berry', 'o', 'bjornberg', 'md', 'kq', 'zi', 'batch', 'index', 'no', 'wolhxc', 'd', 't', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "from typing import Union, Iterable\n",
    "import torchtext, torch, torch.nn.functional as F\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "corpus = pd.concat([test_df_expanded['lower_text'],train_df_expanded['lower_text']])\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = [tokenizer(doc) for doc in corpus]\n",
    "\n",
    "voc = build_vocab_from_iterator(tokens, specials = ['<pad>'])\n",
    "\n",
    "#need to create one hot encoding but add <pad> to reach max_tokens\n",
    "def encode_and_pad(vocab, input_tokens, token_max):\n",
    "    pad_zeros = token_max - len(input_tokens)\n",
    "    result = vocab.lookup_indices(input_tokens)\n",
    "    if pad_zeros > 0:\n",
    "        result.extend(np.zeros(pad_zeros, dtype=int))\n",
    "    return result\n",
    "\n",
    "#need to create tokens add <pad> to reach max_tokens\n",
    "def token_and_pad(vocab, input_tokens, token_max):\n",
    "    pad_zeros = token_max - len(input_tokens)\n",
    "    result = input_tokens\n",
    "    if pad_zeros > 0:\n",
    "        zeros = []\n",
    "        for i in range(pad_zeros):\n",
    "            zeros.append('<pad>')\n",
    "        result.extend(zeros)\n",
    "    return result\n",
    "\n",
    "train_df_expanded['one_hot'] = train_df_expanded['lower_text'].apply(lambda x: encode_and_pad(voc, x.split(), token_max))\n",
    "test_df_expanded['one_hot'] = test_df_expanded['lower_text'].apply(lambda x: encode_and_pad(voc, x.split(), token_max))\n",
    "\n",
    "train_df_expanded['vector_tokenized'] = train_df_expanded['lower_text'].apply(lambda x: token_and_pad(voc, x.split(), token_max))\n",
    "test_df_expanded['vector_tokenized'] = test_df_expanded['lower_text'].apply(lambda x: token_and_pad(voc, x.split(), token_max))\n",
    "\n",
    "print(train_df_expanded.iloc[0]['lower_text'])\n",
    "print(train_df_expanded.iloc[0]['one_hot'])\n",
    "print(train_df_expanded.iloc[0]['vector_tokenized'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the test data documents with their associated annotations.  Verify the number of records are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 9641 507 9641\n",
      "Train: 11273 611 11273\n",
      "Test Expanded: 9641 586 11190\n",
      "Train Expanded: 11273 683 12641\n"
     ]
    }
   ],
   "source": [
    "test_with_annot_df = pd.merge(test_annot_all_df_clean,test_df, on='id')\n",
    "train_with_annot_df = pd.merge(train_annot_all_df_clean,train_df, on='id')\n",
    "\n",
    "\n",
    "test_with_annot_df_expanded = pd.merge(test_annot_all_df_clean,test_df_expanded,on='id')\n",
    "train_with_annot_df_expanded = pd.merge(train_annot_all_df_clean,train_df_expanded,on='id')\n",
    "\n",
    "\n",
    "\n",
    "print(\"Test:\", len(test_annot_all_df_clean), len(test_df), len(test_with_annot_df))\n",
    "print(\"Train:\", len(train_annot_all_df_clean), len(train_df), len(train_with_annot_df))\n",
    "\n",
    "print(\"Test Expanded:\", len(test_annot_all_df_clean), len(test_df_expanded), len(test_with_annot_df_expanded))\n",
    "print(\"Train Expanded:\", len(train_annot_all_df_clean), len(train_df_expanded), len(train_with_annot_df_expanded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and validate the numbers are close with the original papers.  You can see the counts are higher for some reason but the percentage occurrence of each disease doesn't change too much so we are good to use the expanded set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mclop\\AppData\\Local\\Temp\\ipykernel_2544\\1675168617.py:4: FutureWarning: In a future version of pandas all arguments of Series.sort_index will be keyword-only.\n",
      "  df_before = pd.concat([all_df['disease'].value_counts().sort_index(0),all_df[all_df['judgment']==True]['disease'].value_counts().sort_index(0)/all_df['disease'].value_counts().sort_index(0)],axis =1)\n",
      "C:\\Users\\mclop\\AppData\\Local\\Temp\\ipykernel_2544\\1675168617.py:4: FutureWarning: In a future version of pandas all arguments of Series.sort_index will be keyword-only.\n",
      "  df_before = pd.concat([all_df['disease'].value_counts().sort_index(0),all_df[all_df['judgment']==True]['disease'].value_counts().sort_index(0)/all_df['disease'].value_counts().sort_index(0)],axis =1)\n",
      "C:\\Users\\mclop\\AppData\\Local\\Temp\\ipykernel_2544\\1675168617.py:4: FutureWarning: In a future version of pandas all arguments of Series.sort_index will be keyword-only.\n",
      "  df_before = pd.concat([all_df['disease'].value_counts().sort_index(0),all_df[all_df['judgment']==True]['disease'].value_counts().sort_index(0)/all_df['disease'].value_counts().sort_index(0)],axis =1)\n",
      "C:\\Users\\mclop\\AppData\\Local\\Temp\\ipykernel_2544\\1675168617.py:5: FutureWarning: In a future version of pandas all arguments of Series.sort_index will be keyword-only.\n",
      "  df_after = pd.concat([all_df_extended['disease'].value_counts().sort_index(0),all_df_extended[all_df_extended['judgment']==True]['disease'].value_counts().sort_index(0)/all_df_extended['disease'].value_counts().sort_index(0)],axis =1)\n",
      "C:\\Users\\mclop\\AppData\\Local\\Temp\\ipykernel_2544\\1675168617.py:5: FutureWarning: In a future version of pandas all arguments of Series.sort_index will be keyword-only.\n",
      "  df_after = pd.concat([all_df_extended['disease'].value_counts().sort_index(0),all_df_extended[all_df_extended['judgment']==True]['disease'].value_counts().sort_index(0)/all_df_extended['disease'].value_counts().sort_index(0)],axis =1)\n",
      "C:\\Users\\mclop\\AppData\\Local\\Temp\\ipykernel_2544\\1675168617.py:5: FutureWarning: In a future version of pandas all arguments of Series.sort_index will be keyword-only.\n",
      "  df_after = pd.concat([all_df_extended['disease'].value_counts().sort_index(0),all_df_extended[all_df_extended['judgment']==True]['disease'].value_counts().sort_index(0)/all_df_extended['disease'].value_counts().sort_index(0)],axis =1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disease</th>\n",
       "      <th>disease</th>\n",
       "      <th>disease</th>\n",
       "      <th>disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Asthma</th>\n",
       "      <td>1189</td>\n",
       "      <td>0.236333</td>\n",
       "      <td>1343</td>\n",
       "      <td>0.236783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAD</th>\n",
       "      <td>1653</td>\n",
       "      <td>0.730188</td>\n",
       "      <td>1884</td>\n",
       "      <td>0.729830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHF</th>\n",
       "      <td>1138</td>\n",
       "      <td>0.783831</td>\n",
       "      <td>1338</td>\n",
       "      <td>0.799701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depression</th>\n",
       "      <td>1217</td>\n",
       "      <td>0.316352</td>\n",
       "      <td>1391</td>\n",
       "      <td>0.329978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diabetes</th>\n",
       "      <td>1807</td>\n",
       "      <td>0.811289</td>\n",
       "      <td>2072</td>\n",
       "      <td>0.822394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GERD</th>\n",
       "      <td>1080</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>1217</td>\n",
       "      <td>0.357436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gallstones</th>\n",
       "      <td>1267</td>\n",
       "      <td>0.272297</td>\n",
       "      <td>1446</td>\n",
       "      <td>0.282158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gout</th>\n",
       "      <td>1219</td>\n",
       "      <td>0.214930</td>\n",
       "      <td>1389</td>\n",
       "      <td>0.223182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypercholesterolemia</th>\n",
       "      <td>1407</td>\n",
       "      <td>0.684435</td>\n",
       "      <td>1599</td>\n",
       "      <td>0.689181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypertension</th>\n",
       "      <td>1809</td>\n",
       "      <td>0.885572</td>\n",
       "      <td>2064</td>\n",
       "      <td>0.888081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hypertriglyceridemia</th>\n",
       "      <td>1098</td>\n",
       "      <td>0.075592</td>\n",
       "      <td>1246</td>\n",
       "      <td>0.076244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OA</th>\n",
       "      <td>1197</td>\n",
       "      <td>0.303258</td>\n",
       "      <td>1364</td>\n",
       "      <td>0.309384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSA</th>\n",
       "      <td>1240</td>\n",
       "      <td>0.247581</td>\n",
       "      <td>1409</td>\n",
       "      <td>0.247693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obesity</th>\n",
       "      <td>1449</td>\n",
       "      <td>0.603175</td>\n",
       "      <td>1638</td>\n",
       "      <td>0.600733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PVD</th>\n",
       "      <td>1167</td>\n",
       "      <td>0.256213</td>\n",
       "      <td>1331</td>\n",
       "      <td>0.270473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Venous Insufficiency</th>\n",
       "      <td>977</td>\n",
       "      <td>0.099284</td>\n",
       "      <td>1100</td>\n",
       "      <td>0.102727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      disease   disease  disease   disease\n",
       "Asthma                   1189  0.236333     1343  0.236783\n",
       "CAD                      1653  0.730188     1884  0.729830\n",
       "CHF                      1138  0.783831     1338  0.799701\n",
       "Depression               1217  0.316352     1391  0.329978\n",
       "Diabetes                 1807  0.811289     2072  0.822394\n",
       "GERD                     1080  0.347222     1217  0.357436\n",
       "Gallstones               1267  0.272297     1446  0.282158\n",
       "Gout                     1219  0.214930     1389  0.223182\n",
       "Hypercholesterolemia     1407  0.684435     1599  0.689181\n",
       "Hypertension             1809  0.885572     2064  0.888081\n",
       "Hypertriglyceridemia     1098  0.075592     1246  0.076244\n",
       "OA                       1197  0.303258     1364  0.309384\n",
       "OSA                      1240  0.247581     1409  0.247693\n",
       "Obesity                  1449  0.603175     1638  0.600733\n",
       "PVD                      1167  0.256213     1331  0.270473\n",
       "Venous Insufficiency      977  0.099284     1100  0.102727"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.concat([test_with_annot_df, train_with_annot_df])\n",
    "all_df_extended = pd.concat([test_with_annot_df_expanded, train_with_annot_df_expanded])\n",
    "\n",
    "df_before = pd.concat([all_df['disease'].value_counts().sort_index(0),all_df[all_df['judgment']==True]['disease'].value_counts().sort_index(0)/all_df['disease'].value_counts().sort_index(0)],axis =1)\n",
    "df_after = pd.concat([all_df_extended['disease'].value_counts().sort_index(0),all_df_extended[all_df_extended['judgment']==True]['disease'].value_counts().sort_index(0)/all_df_extended['disease'].value_counts().sort_index(0)],axis =1)\n",
    "df_all = pd.concat([df_before,df_after], axis=1)\n",
    "\n",
    "df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Note occurrences](images\\note_occurrences.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the final test/train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_with_annot_df_expanded.to_pickle(DATA_PATH + '/test.pkl') \n",
    "train_with_annot_df_expanded.to_pickle(DATA_PATH + '/train.pkl') \n",
    "#corpus.to_pickle(DATA_PATH + '/corpus.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip:  63%|██████▎   | 540M/862M [01:50<01:00, 5.33MB/s]    "
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "\n",
    "vec = torchtext.vocab.GloVe(name='6B', dim=300)\n",
    "\n",
    "one_hot_test = train_df_expanded.iloc[0]['one_hot']\n",
    "vector_tokenized_test = train_df_expanded.iloc[0]['vector_tokenized']\n",
    "\n",
    "print(one_hot_test)\n",
    "ret = vec.get_vecs_by_tokens(voc.lookup_tokens(one_hot_test))\n",
    "print(ret.shape)\n",
    "#print(ret[0])\n",
    "print(vector_tokenized_test)\n",
    "ret = vec.get_vecs_by_tokens(voc.lookup_tokens(one_hot_test))\n",
    "print(ret.shape)\n",
    "#print(ret[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the FastText embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "vec = torchtext.vocab.FastText()\n",
    "\n",
    "one_hot_test = train_df_expanded.iloc[0]['one_hot']\n",
    "\n",
    "print(one_hot_test)\n",
    "ret = vec.get_vecs_by_tokens(voc.lookup_tokens(one_hot_test))\n",
    "print(ret.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10pK5od01jfTHJyLN94dJxEube3sJszFm",
     "timestamp": 1678482671183
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
