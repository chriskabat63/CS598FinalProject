{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d1ehXFWM5aE"
   },
   "source": [
    "This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n",
    "***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n",
    "\n",
    "This notebook is for creating the multiple embeddings formats as described in the study.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv360l2IkNfO"
   },
   "source": [
    "The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub.\n",
    "\n",
    "We are only creating embeddings for data that includes stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in c:\\users\\mclop\\anaconda3\\lib\\site-packages (0.15.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from torchtext) (2.27.1)\n",
      "Requirement already satisfied: torchdata==0.6.0 in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from torchtext) (0.6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from torchtext) (4.64.0)\n",
      "Requirement already satisfied: torch==2.0.0 in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from torchtext) (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from torchtext) (1.24.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (2.11.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (4.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchtext) (3.6.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from torchdata==0.6.0->torchtext) (1.26.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.0->torchtext) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from requests->torchtext) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.0->torchtext) (1.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mclop\\anaconda3\\lib\\site-packages (from tqdm->torchtext) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mclop\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mclop\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mclop\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mclop\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mclop\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mclop\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1250,
     "status": "ok",
     "timestamp": 1679769727418,
     "user": {
      "displayName": "Matthew Lopes",
      "userId": "01980291092524472313"
     },
     "user_tz": 240
    },
    "id": "zyXrAo2dsJqf",
    "outputId": "0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mclop\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './obesity_data//allannot_df.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#test_df = pd.read_pickle(DATA_PATH + '/test_df.pkl')\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#train_df = pd.read_pickle(DATA_PATH + '/train_df.pkl')\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#test_annot_all_df_clean = pd.read_pickle(DATA_PATH + '/test_annot_all_df_clean.pkl') \u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#train_annot_all_df_clean = pd.read_pickle(DATA_PATH + '/train_annot_all_df_clean.pkl') \u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#Don't need to do this for the one with no stop words\u001b[39;00m\n\u001b[0;32m     12\u001b[0m alldocs_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(DATA_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/alldocs_df.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m allannot_df\u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/allannot_df.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m alldocs_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m alldocs_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit()))\n\u001b[0;32m     16\u001b[0m alldocs_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m alldocs_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit()))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py:187\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    186\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    194\u001b[0m \n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:798\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    794\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './obesity_data//allannot_df.pkl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "DATA_PATH = './obesity_data/'\n",
    "#test_df = pd.read_pickle(DATA_PATH + '/test_df.pkl')\n",
    "#train_df = pd.read_pickle(DATA_PATH + '/train_df.pkl')\n",
    "#test_annot_all_df_clean = pd.read_pickle(DATA_PATH + '/test_annot_all_df_clean.pkl') \n",
    "#train_annot_all_df_clean = pd.read_pickle(DATA_PATH + '/train_annot_all_df_clean.pkl') \n",
    "\n",
    "#Don't need to do this for the one with no stop words\n",
    "alldocs_df = pd.read_pickle(DATA_PATH + '/alldocs_df.pkl')\n",
    "allannot_df= pd.read_pickle(DATA_PATH + '/allannot_df.pkl')\n",
    "\n",
    "alldocs_df['word_count'] = alldocs_df['lower_text'].apply(lambda x: len(x.split()))\n",
    "alldocs_df['word_count'] = alldocs_df['lower_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "alldocs_df['sentence_count'] = alldocs_df['text'].apply(lambda x: len(sent_tokenize(x)))\n",
    "\n",
    "df_print = pd.DataFrame()\n",
    "df_print['Min'] = [np.min(alldocs_df['word_count']), np.min(alldocs_df['word_count'])]\n",
    "df_print['Mean'] = [np.mean(alldocs_df['word_count']), np.mean(alldocs_df['word_count'])]\n",
    "df_print['Max'] = [np.max(alldocs_df['word_count']), np.max(alldocs_df['word_count'])]\n",
    "df_print['Std'] = [np.std(alldocs_df['word_count']), np.std(alldocs_df['word_count'])]\n",
    "df_print['MeanPlusStd'] = round(df_print['Mean'] + df_print['Std'],0)\n",
    "token_max = int(round(np.max(df_print['MeanPlusStd']),0))\n",
    "\n",
    "print(df_print)\n",
    "print('Max Tokens:',token_max)\n",
    "print('All:', sum(alldocs_df['word_count'] > token_max), \"out of\", len(alldocs_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to split these larger text blocks into 2 notes of size max_token or below.  Note, there are 4 notes (1 in test and 3 in train) that are bigger than 2 times x tokens.  For now, we will ignore, but may want to add in later (either loop or have left/middle/right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldocs_df_ok = alldocs_df[alldocs_df['word_count'] <= token_max].copy()\n",
    "alldocs_df_large_left = alldocs_df[alldocs_df['word_count'] > token_max].copy()\n",
    "alldocs_df_large_right = alldocs_df[alldocs_df['word_count'] > token_max].copy()\n",
    "\n",
    "#Get the right words and the left words and then concatenate all 3 and recacluate \n",
    "alldocs_df_large_left['lower_text'] = alldocs_df_large_left['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[:(token_max-1)]]))\n",
    "alldocs_df_large_right['lower_text'] = alldocs_df_large_right['lower_text'].apply(lambda x: ' '.join([word for word in x.split()[token_max:(2*token_max)]]))\n",
    "\n",
    "alldocs_df_expanded = pd.concat([alldocs_df_ok,alldocs_df_large_right,alldocs_df_large_left])\n",
    "alldocs_df_expanded['word_count'] = alldocs_df_expanded['lower_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print('All:', sum(alldocs_df_expanded['word_count'] > token_max), \"out of\", len(alldocs_df_expanded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a one hot vector given a vocabulary and pad it with the padding character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Iterable\n",
    "import torchtext, torch, torch.nn.functional as F\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "sentence_max = np.max(alldocs_df['sentence_count'])\n",
    "print('Max Sentences:', sentence_max)\n",
    "\n",
    "corpus = alldocs_df_expanded['lower_text']\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = [tokenizer(doc) for doc in corpus]\n",
    "\n",
    "voc = build_vocab_from_iterator(tokens, specials = ['<pad>'])\n",
    "\n",
    "#need to create one hot encoding but add <pad> to reach max_tokens\n",
    "def encode_and_pad(vocab, input_tokens, token_max):\n",
    "    pad_zeros = token_max - len(input_tokens)\n",
    "    result = vocab.lookup_indices(input_tokens)\n",
    "    if pad_zeros > 0:\n",
    "        result.extend(np.zeros(pad_zeros, dtype=int))\n",
    "    return result\n",
    "\n",
    "#need to create tokens add <pad> to reach max_tokens\n",
    "def token_and_pad(vocab, input_tokens, token_max):\n",
    "    pad_zeros = token_max - len(input_tokens)\n",
    "    result = input_tokens\n",
    "    if pad_zeros > 0:\n",
    "        zeros = []\n",
    "        for i in range(pad_zeros):\n",
    "            zeros.append('<pad>')\n",
    "        result.extend(zeros)\n",
    "    return result\n",
    "\n",
    "#need to create tokens add '\\n' to reach max_sentences\n",
    "def token_and_pad_sentence(input_sentences, sentence_max):\n",
    "    pad_spaces = sentence_max - len(input_sentences)\n",
    "    result = input_sentences\n",
    "    if pad_spaces > 0:\n",
    "        for i in range(pad_spaces):\n",
    "            result.append('\\n')\n",
    "\n",
    "    return result\n",
    "\n",
    "alldocs_df_expanded['one_hot'] = alldocs_df_expanded['lower_text'].apply(lambda x: encode_and_pad(voc, x.split(), token_max))\n",
    "alldocs_df_expanded['vector_tokenized'] = alldocs_df_expanded['lower_text'].apply(lambda x: token_and_pad(voc, x.split(), token_max))\n",
    "alldocs_df_expanded['sentence_tokenized'] = alldocs_df_expanded['text'].apply(lambda x: token_and_pad_sentence(sent_tokenize(x), sentence_max))\n",
    "\n",
    "print(alldocs_df_expanded.iloc[0]['lower_text'])\n",
    "print(alldocs_df_expanded.iloc[0]['one_hot'])\n",
    "print(alldocs_df_expanded.iloc[0]['vector_tokenized'])\n",
    "print(alldocs_df_expanded.iloc[0]['sentence_tokenized'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the test data documents with their associated annotations.  Verify the number of records are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.merge(allannot_df,alldocs_df, on='id')\n",
    "all_df_expanded= pd.merge(allannot_df,alldocs_df_expanded,on='id')\n",
    "\n",
    "\n",
    "\n",
    "print(\"All:\", len(allannot_df), len(alldocs_df), len(all_df))\n",
    "print(\"All Expanded:\", len(allannot_df), len(alldocs_df_expanded), len(all_df_expanded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and validate the numbers are close with the original papers.  You can see the counts are higher for some reason but the percentage occurrence of each disease doesn't change too much so we are good to use the expanded set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = pd.concat([all_df['disease'].value_counts().sort_index(0),all_df[all_df['judgment']==True]['disease'].value_counts().sort_index(0)/all_df['disease'].value_counts().sort_index(0)],axis =1)\n",
    "df_after = pd.concat([all_df_expanded['disease'].value_counts().sort_index(0),all_df_expanded[all_df_expanded['judgment']==True]['disease'].value_counts().sort_index(0)/all_df_expanded['disease'].value_counts().sort_index(0)],axis =1)\n",
    "df_all = pd.concat([df_before,df_after], axis=1)\n",
    "\n",
    "df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Note occurrences](images\\note_occurrences.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the final test/train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_pickle(DATA_PATH + '/all_df.pkl') \n",
    "all_df_expanded.to_pickle(DATA_PATH + '/all_df_expanded.pkl') \n",
    "#corpus.to_pickle(DATA_PATH + '/corpus.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "\n",
    "vec = torchtext.vocab.GloVe(name='6B', dim=300)\n",
    "\n",
    "one_hot_test = all_df_expanded.iloc[0]['one_hot']\n",
    "vector_tokenized_test = all_df_expanded.iloc[0]['vector_tokenized']\n",
    "\n",
    "print(one_hot_test)\n",
    "ret = vec.get_vecs_by_tokens(voc.lookup_tokens(one_hot_test))\n",
    "print(ret.shape)\n",
    "#print(ret[0])\n",
    "print(vector_tokenized_test)\n",
    "ret = vec.get_vecs_by_tokens(voc.lookup_tokens(one_hot_test))\n",
    "print(ret.shape)\n",
    "#print(ret[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the FastText embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "vec = torchtext.vocab.FastText()\n",
    "\n",
    "one_hot_test = all_df_expanded.iloc[0]['one_hot']\n",
    "\n",
    "print(one_hot_test)\n",
    "ret = vec.get_vecs_by_tokens(voc.lookup_tokens(one_hot_test))\n",
    "print(ret.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10pK5od01jfTHJyLN94dJxEube3sJszFm",
     "timestamp": 1678482671183
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
