{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d1ehXFWM5aE"
   },
   "source": [
    "This notebook was created to support the data preparation required to support our CS 598 DLH project.  The paper we have chosen for the reproducibility project is:\n",
    "***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv360l2IkNfO"
   },
   "source": [
    "The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1250,
     "status": "ok",
     "timestamp": 1679769727418,
     "user": {
      "displayName": "Matthew Lopes",
      "userId": "01980291092524472313"
     },
     "user_tz": 240
    },
    "id": "zyXrAo2dsJqf",
    "outputId": "0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = './obesity_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - All Features**\n",
    "\n",
    "![CML TFIDF All](images\\cml-tfidf-all.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - ExtraTreesClassifier Features**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-tfidf-extra.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - InfoGain Features**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-tfidf-infogain.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - TF-IDF - SelectKBest Features**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-tfidf-selectkbest.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - Word Embeddings - No Stopwords**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-we-swno.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Machine Learning - Word Embeddings - Stopwords**\n",
    "\n",
    "![CML TFIDF ExtraTrees](images\\cml-we-swyes.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import torchtext\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection, svm, naive_bayes\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# define data path\n",
    "DATA_PATH = './obesity_data/'\n",
    "RESULTS_PATH = './results/'\n",
    "MODELS_PATH = './models/'\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "all_docs_df = pd.read_pickle(DATA_PATH + '/alldocs_df.pkl')\n",
    "all_docs_df_ns = pd.read_pickle(DATA_PATH + '/alldocs_df_ns.pkl')\n",
    "all_annot_df = pd.read_pickle(DATA_PATH + '/alannot_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Asthma', 'CHF', 'Depression', 'Diabetes', 'Gallstones', 'Gout', 'Hypercholesterolemia', 'Hypertriglyceridemia', 'OA', 'OSA', 'Obesity', 'CAD', 'Hypertension', 'PVD', 'Venous Insufficiency', 'GERD']\n"
     ]
    }
   ],
   "source": [
    "all_df = pd.merge(all_docs_df,all_annot_df, on='id')\n",
    "all_df_ns = pd.merge(all_docs_df_ns,all_annot_df, on='id')\n",
    "\n",
    "disease_list = all_df['disease'].unique().tolist()\n",
    "\n",
    "print(disease_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(all_df['tok_lem_text']):\n",
    "    Final_words = []\n",
    "    for word in entry:\n",
    "        Final_words.append(word)\n",
    "    all_df.loc[index, 'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(all_df_ns['tok_lem_text']):\n",
    "    Final_words = []\n",
    "    for word in entry:\n",
    "        Final_words.append(word)\n",
    "    all_df_ns.loc[index, 'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wmc', 'am', 'anemia', 'signed', 'dis', 'admission', 'date', 'report', 'status', 'signed', 'discharge', 'date', 'attending', 'truka', 'deon', 'xavier', 'md', 'service', 'bh', 'principal', 'diagnosis', 'anemia', 'and', 'gi', 'bleed', 'secondary', 'diagnosis', 'diabetes', 'mitral', 'valve', 'replacement', 'atrial', 'fibrillation', 'and', 'chronic', 'kidney', 'disease', 'history', 'of', 'present', 'illness', 'the', 'patient', 'is', 'an', 'yearold', 'woman', 'with', 'a', 'history', 'of', 'diabetes', 'chronic', 'kidney', 'disease', 'congestive', 'heart', 'failure', 'with', 'ejection', 'fraction', 'of', 'to', 'who', 'present', 'from', 'clinic', 'with', 'a', 'chief', 'complaint', 'of', 'fatigue', 'and', 'weakness', 'for', 'one', 'week', 'she', 'had', 'had', 'worsening', 'right', 'groin', 'and', 'hip', 'pain', 'status', 'post', 'a', 'total', 'hip', 'replacement', 'approximately', 'year', 'ago', 'which', 'had', 'been', 'worsening', 'for', 'two', 'week', 'and', 'she', 'ha', 'also', 'recently', 'completed', 'a', 'course', 'of', 'levaquin', 'for', 'urinary', 'tract', 'infection', 'she', 'presented', 'to', 'dr', 'parrent', 'office', 'complaining', 'of', 'fatigue', 'and', 'weakness', 'for', 'one', 'week', 'she', 'ha', 'had', 'some', 'abdominal', 'pain', 'in', 'a', 'bandlike', 'distribution', 'around', 'her', 'right', 'side', 'she', 'wa', 'found', 'to', 'have', 'a', 'hematocrit', 'of', 'down', 'from', 'eight', 'day', 'ago', 'and', 'wa', 'sent', 'to', 'the', 'emergency', 'department', 'for', 'transfusion', 'and', 'workup', 'of', 'her', 'anemia', 'preadmission', 'medication', 'caltrate', 'plus', 'd', 'one', 'tab', 'po', 'bid', 'lantus', 'unit', 'sc', 'qpm', 'novolog', 'unit', 'unit', 'unit', 'sc', 'tid', 'imdur', 'mg', 'bid', 'amlodipine', 'mg', 'bid', 'furosemide', 'mg', 'daily', 'valsartan', 'mg', 'daily', 'warfarin', 'mg', 'daily', 'iron', 'sulfate', 'mg', 'po', 'daily', 'and', 'multivitamin', 'daily', 'past', 'medical', 'history', 'chronic', 'kidney', 'disease', 'presumed', 'due', 'to', 'congestive', 'heart', 'failurediuresisrenal', 'artery', 'diseaseearly', 'diabetic', 'nephropathy', 'type', 'diabetes', 'previous', 'stroke', 'congestive', 'heart', 'failure', 'with', 'ejection', 'fraction', 'of', 'to', 'rheumatic', 'valvular', 'disease', 'with', 'mitral', 'valve', 'replacement', 'and', 'tricuspid', 'valve', 'repair', 'atrial', 'fibrillation', 'history', 'of', 'small', 'bowel', 'obstruction', 'status', 'post', 'right', 'total', 'hip', 'replacement', 'approximately', 'year', 'ago', 'family', 'history', 'no', 'family', 'history', 'of', 'kidney', 'disease', 'or', 'heart', 'disease', 'social', 'history', 'she', 'ha', 'child', 'life', 'alone', 'with', 'home', 'care', 'in', 'me', 'but', 'ha', 'moved', 'in', 'to', 'live', 'with', 'her', 'daughter', 'in', 'news', 'irv', 'in', 'she', 'denies', 'tobacco', 'use', 'and', 'drink', 'alcohol', 'rarely', 'allergy', 'codeine', 'and', 'benadryl', 'admission', 'physical', 'examination', 'vital', 'sign', 'were', 'temperature', 'heart', 'rate', 'blood', 'pressure', 'respiration', 'and', 'sao', 'on', 'room', 'air', 'the', 'patient', 'is', 'a', 'frail', 'elderly', 'woman', 'in', 'no', 'acute', 'distress', 'she', 'ha', 'poor', 'dentition', 'jvp', 'is', 'difficult', 'to', 'ass', 'secondary', 'to', 'tricuspid', 'regurgitation', 'lung', 'were', 'clear', 'to', 'auscultation', 'bilaterally', 'cardiovascular', 'exam', 'showed', 'bradycardia', 'with', 'heart', 'rate', 'in', 'the', 's', 'that', 'wa', 'irregular', 's', 'plus', 's', 'with', 'systolic', 'murmur', 'heard', 'throughout', 'with', 'mechanical', 'sounding', 's', 'abdomen', 'wa', 'mildly', 'tender', 'to', 'palpation', 'in', 'the', 'mid', 'epigastrium', 'with', 'no', 'rebound', 'or', 'guarding', 'extremity', 'showed', 'venous', 'stasis', 'change', 'in', 'her', 'lower', 'extremity', 'bilaterally', 'foot', 'were', 'cool', 'with', 'diminished', 'dp', 'and', 'pt', 'pulse', 'on', 'neurological', 'exam', 'she', 'wa', 'alert', 'and', 'oriented', 'x', 'and', 'cranial', 'nerve', 'ii', 'through', 'xii', 'were', 'intact', 'study', 'ekg', 'showed', 'atrial', 'fibrillation', 'with', 'slow', 'ventricular', 'response', 'with', 'heart', 'rate', 'of', 'widened', 'qrs', 'a', 'q', 'wave', 'in', 'avl', 'and', 'u', 'wave', 'in', 'the', 'lateral', 'lead', 'chest', 'xray', 'showed', 'improved', 'pleural', 'effusion', 'and', 'pulmonary', 'edema', 'stable', 'marked', 'cardiomegaly', 'xray', 'of', 'the', 'right', 'hip', 'showed', 'status', 'post', 'right', 'total', 'hip', 'arthroplasty', 'with', 'stable', 'periprosthetic', 'lucency', 'and', 'cortical', 'remodeling', 'severe', 'left', 'hip', 'osteoarthritis', 'diffuse', 'atherosclerosis', 'egd', 'on', 'showed', 'hiatal', 'hernia', 'fundic', 'polyp', 'with', 'pathology', 'showing', 'hypoplastic', 'and', 'inflammatory', 'lesion', 'mild', 'antral', 'erosion', 'with', 'pathology', 'demonstrating', 'mild', 'regeneration', 'that', 'wa', 'nonspecific', 'and', 'no', 'h', 'pylorus', 'and', 'duodenitis', 'with', 'pathology', 'showing', 'normal', 'mucosa', 'but', 'no', 'active', 'bleeding', 'or', 'sign', 'of', 'recent', 'bleeding', 'capsule', 'endoscopy', 'on', 'showed', 'a', 'healing', 'gastric', 'ulcer', 'likely', 'in', 'the', 'antrum', 'small', 'bowel', 'lymphangiectasia', 'and', 'angioectasia', 'in', 'the', 'distal', 'small', 'bowel', 'colonoscopy', 'on', 'demonstrated', 'cecal', 'diverticulum', 'approximately', 'mm', 'ascending', 'sessile', 'polyp', 'and', 'several', 'small', 'sessile', 'polyp', 'in', 'the', 'rectosigmoid', 'procedure', 'right', 'basilic', 'vein', 'transposition', 'on', 'by', 'dr', 'jacinto', 'goonez', 'hospital', 'course', 'by', 'problem', 'gi', 'bleed', 'in', 'the', 'emergency', 'department', 'the', 'patient', 'vital', 'sign', 'were', 'temperature', 'heart', 'rate', 'respiration', 'blood', 'pressure', 'with', 'sao', 'on', 'room', 'air', 'she', 'wa', 'found', 'to', 'have', 'black', 'guaiac', 'positive', 'stool', 'and', 'gi', 'wa', 'consulted', 'she', 'wa', 'started', 'on', 'iv', 'nexium', 'mg', 'bid', 'and', 'wa', 'given', 'vitamin', 'k', 'mg', 'subcutaneously', 'and', 'two', 'unit', 'of', 'ffp', 'and', 'she', 'wa', 'transfused', 'three', 'unit', 'of', 'packed', 'red', 'blood', 'cell', 'with', 'lasix', 'mg', 'iv', 'for', 'each', 'bag', 'of', 'note', 'the', 'patient', 'had', 'a', 'colonoscopy', 'in', 'wa', 'on', 'which', 'showed', 'bleeding', 'rectal', 'ulcer', 'with', 'biopsy', 'consistent', 'with', 'ischemic', 'colitis', 'an', 'egd', 'on', 'showed', 'a', 'hiatal', 'hernia', 'fundic', 'polyp', 'path', 'hypoplasticinflammatory', 'mild', 'antral', 'erosion', 'path', 'mild', 'regeneration', 'nonspecific', 'no', 'h', 'pylorus', 'duodenitis', 'path', 'normal', 'mucosa', 'but', 'no', 'active', 'bleeding', 'or', 'sign', 'of', 'recent', 'bleeding', 'capsule', 'endoscopy', 'on', 'showed', 'healing', 'gastric', 'ulcer', 'likely', 'in', 'the', 'antrum', 'small', 'bowel', 'lymphangiectasia', 'and', 'angioectasia', 'in', 'the', 'distal', 'small', 'bowel', 'which', 'were', 'considered', 'the', 'likely', 'source', 'of', 'bleeding', 'colonoscopy', 'wa', 'performed', 'on', 'to', 'search', 'for', 'angioectasias', 'for', 'which', 'intervention', 'would', 'be', 'possible', 'but', 'demonstrated', 'only', 'a', 'cecal', 'diverticulum', 'and', 'approximately', 'mm', 'ascending', 'sessile', 'polyp', 'and', 'several', 'small', 'sessile', 'polyp', 'in', 'the', 'rectosigmoid', 'the', 'patient', 'wa', 'started', 'on', 'aranesp', 'but', 'had', 'a', 'point', 'hematocrit', 'drop', 'from', 'to', 'on', 'for', 'which', 'she', 'required', 'another', 'two', 'unit', 'of', 'blood', 'along', 'with', 'lasix', 'the', 'patient', 'hematocrit', 'remained', 'stable', 'at', 'approximately', 'to', 'and', 'she', 'will', 'be', 'restarted', 'on', 'anticoagulation', 'on', 'thursday', 'one', 'week', 'after', 'her', 'av', 'fistula', 'surgery', 'renal', 'the', 'patient', 'ha', 'chronic', 'kidney', 'disease', 'and', 'is', 'being', 'considered', 'for', 'a', 'possible', 'hemodialysis', 'in', 'the', 'future', 'she', 'wa', 'continued', 'on', 'her', 'caltrate', 'plus', 'd', 'multivitamin', 'and', 'iron', 'supplementation', 'and', 'she', 'wa', 'started', 'on', 'aranesp', 'mcg', 'weekly', 'and', 'wa', 'given', 'sevelamer', 'mg', 'qac', 'for', 'elevated', 'phosphate', 'level', 'vein', 'mapping', 'study', 'wa', 'performed', 'on', 'and', 'a', 'right', 'basilic', 'vein', 'transposition', 'wa', 'performed', 'on', 'by', 'dr', 'landrie', 'the', 'patient', 'had', 'postoperative', 'right', 'hand', 'coolness', 'numbness', 'and', 'weakness', 'always', 'with', 'dopplerable', 'radial', 'pulse', 'from', 'steal', 'versus', 'neurapraxia', 'which', 'wa', 'improved', 'by', 'the', 'time', 'of', 'discharge', 'she', 'will', 'follow', 'up', 'with', 'dr', 'chanthasene', 'a', 'an', 'outpatient', 'the', 'patient', 'creatinine', 'wa', 'on', 'admission', 'improved', 'to', 'by', 'which', 'wa', 'the', 'day', 'of', 'her', 'surgery', 'and', 'increased', 'again', 'to', 'on', 'she', 'wa', 'given', 'iv', 'lasix', 'bolus', 'a', 'she', 'had', 'evidence', 'of', 'volume', 'overload', 'with', 'over', 'eightpound', 'weight', 'gain', 'during', 'her', 'hospitalization', 'and', 'creatinine', 'improved', 'to', 'by', 'the', 'day', 'of', 'discharge', 'cardiovascular', 'pump', 'the', 'patient', 'ejection', 'fraction', 'is', 'to', 'and', 'she', 'wa', 'given', 'mg', 'of', 'iv', 'lasix', 'for', 'each', 'unit', 'of', 'packed', 'red', 'blood', 'cell', 'she', 'received', 'along', 'with', 'mg', 'po', 'daily', 'with', 'iv', 'bolus', 'of', 'mg', 'a', 'needed', 'for', 'volume', 'overload', 'a', 'judged', 'by', 'an', 'increase', 'in', 'her', 'weight', 'antihypertensive', 'medication', 'were', 'originally', 'held', 'for', 'her', 'gi', 'bleed', 'and', 'they', 'were', 'restarted', 'on', 'with', 'systolic', 'blood', 'pressure', 'remaining', 'in', 'the', 's', 'to', 's', 'rhythm', 'the', 'patient', 'ha', 'atrial', 'fibrillation', 'with', 'slow', 'ventricular', 'response', 'with', 'heart', 'rate', 'a', 'low', 'a', 'the', 'upper', 's', 'occasionally', 'her', 'heart', 'rate', 'appeared', 'regular', 'and', 'wa', 'thought', 'to', 'be', 'junctional', 'escape', 'rhythm', 'she', 'wa', 'asymptomatic', 'throughout', 'her', 'hospitalization', 'she', 'wa', 'discussed', 'with', 'her', 'cardiologist', 'dr', 'fritz', 'who', 'will', 'consider', 'a', 'pacemaker', 'a', 'an', 'outpatient', 'endocrine', 'she', 'is', 'euthyroid', 'with', 'tsh', 'of', 'for', 'her', 'diabetes', 'she', 'received', 'nightly', 'lantus', 'with', 'aspart', 'qac', 'and', 'sliding', 'scale', 'when', 'eating', 'and', 'regular', 'insulin', 'sliding', 'scale', 'qh', 'when', 'npo', 'fingersticks', 'were', 'elevated', 'to', 's', 'early', 'during', 'this', 'admission', 'but', 'improved', 'to', 'the', 's', 'upon', 'increasing', 'her', 'insulin', 'dose', 'musculoskeletal', 'the', 'patient', 'is', 'status', 'post', 'right', 'total', 'hip', 'replacement', 'approximately', 'year', 'ago', 'and', 'complained', 'of', 'right', 'hip', 'pain', 'upon', 'admission', 'an', 'xray', 'showed', 'stable', 'arthroplasty', 'and', 'the', 'pain', 'wa', 'improved', 'by', 'the', 'morning', 'of', 'she', 'will', 'follow', 'with', 'physiatrist', 'dr', 'allan', 'kofoed', 'complication', 'right', 'hand', 'weakness', 'numbness', 'and', 'coolness', 'status', 'post', 'av', 'fistula', 'surgery', 'possibly', 'secondary', 'to', 'steal', 'or', 'neurapraxia', 'significantly', 'improved', 'by', 'the', 'time', 'of', 'discharge', 'consultant', 'dr', 'garfield', 'kiehne', 'from', 'vascular', 'surgery', 'dr', 'ambrose', 'moldrem', 'from', 'gastroenterology', 'physical', 'examination', 'on', 'discharge', 'stable', 'vital', 'sign', 'lung', 'with', 'bibasilar', 'crackle', 'systolic', 'murmur', 'heard', 'throughout', 'plus', 'a', 'mechanical', 's', 'abdomen', 'benign', 'lower', 'extremity', 'with', 'chronic', 'venous', 'stasis', 'change', 'right', 'upper', 'extremity', 'av', 'fistula', 'with', 'thrill', 'decreased', 'right', 'radial', 'pulse', 'with', 'warm', 'hand', 'distally', 'and', 'strength', 'in', 'hand', 'grip', 'of', 'the', 'right', 'hand', 'discharge', 'medication', 'norvasc', 'mg', 'daily', 'caltrate', 'plus', 'd', 'one', 'tablet', 'po', 'bid', 'aranesp', 'mcg', 'subcu', 'weekly', 'lovenox', 'mg', 'subcu', 'daily', 'starting', 'on', 'thursday', 'to', 'be', 'discontinued', 'when', 'patient', 'inr', 'is', 'therapeutic', 'on', 'coumadin', 'nexium', 'mg', 'po', 'bid', 'ferrous', 'sulfate', 'mg', 'po', 'bid', 'lasix', 'mg', 'po', 'daily', 'insulin', 'aspart', 'unit', 'subcu', 'every', 'meal', 'insulin', 'lantus', 'unit', 'subcu', 'qpm', 'imdur', 'mg', 'po', 'bid', 'sevelamer', 'mg', 'po', 'qac', 'multivitamin', 'one', 'tablet', 'daily', 'valsartan', 'mg', 'po', 'daily', 'and', 'coumadin', 'mg', 'po', 'qpm', 'starting', 'on', 'thursday', 'disposition', 'to', 'home', 'with', 'service', 'followup', 'appointment', 'the', 'patient', 'will', 'follow', 'up', 'with', 'dr', 'brendan', 'b', 'tordsen', 'in', 'one', 'to', 'two', 'week', 'with', 'dr', 'schaetzle', 'from', 'vascular', 'surgery', 'on', 'at', 'am', 'with', 'dr', 'salvatore', 'sherrod', 'from', 'physiatry', 'on', 'at', 'am', 'and', 'dr', 'margarito', 'nolting', 'on', 'at', 'am', 'code', 'status', 'the', 'patient', 'is', 'full', 'code', 'and', 'her', 'healthcare', 'proxy', 'is', 'her', 'daughter', 'shane', 'lutao', 'primary', 'care', 'physician', 'rufus', 'mannheimer', 'md', 'escription', 'document', 'cssten', 'tel', 'dictated', 'by', 'beier', 'julio', 'attending', 'hambric', 'margarito', 'kurt', 'dictation', 'id', 'd', 't']\n"
     ]
    }
   ],
   "source": [
    "#all_df = all_df[all_df['disease'] == 'CHF']\n",
    "print(all_df['tok_lem_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(df):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['text_final'], df['judgment'], test_size=0.20, shuffle=True)\n",
    "    X_train_ns, X_test_ns, y_train_ns, y_test_ns = train_test_split(df['text_final'], df['judgment'], test_size=0.20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "\n",
    "Train_Y  = Encoder.fit_transform(y_train)\n",
    "Test_Y  = Encoder.fit_transform(y_test)\n",
    "\n",
    "Train_Y_NS  = Encoder.fit_transform(y_train_ns)\n",
    "Test_Y_NS = Encoder.fit_transform(y_test_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=600)\n",
    "Tfidf_vect_NS = TfidfVectorizer(max_features = 600, stop_words = cachedStopWords)\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.fit_transform(X_train)\n",
    "Test_X_Tfidf = Tfidf_vect.fit_transform(X_test)\n",
    "\n",
    "Train_X_Tfidf_NS = Tfidf_vect_NS.fit_transform(X_train_ns)\n",
    "Test_X_Tfidf_NS = Tfidf_vect_NS.fit_transform(X_test_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "\n",
    "def getVocab(X_train, y_train, feature, max_tokens):\n",
    " \n",
    "    ## Step 1: Determine the Initial Vocabulary\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab = list(tokenizer.word_index.keys())\n",
    "\n",
    "    ## Step 2: Create term  matrix\n",
    "    vectors = tokenizer.texts_to_matrix(X_train, mode='count')\n",
    "\n",
    "    ## Do feature selection on term matrix (column headers are words)\n",
    "    X = vectors\n",
    "    y = y_train\n",
    "\n",
    "    ##Choose algorithm\n",
    "    if feature == 'SelectKBest':\n",
    "        selector = SelectKBest(score_func=f_classif, k=max_tokens).fit(X,y)\n",
    "    else: \n",
    "        if feature == 'InfoGainAttributeVal':\n",
    "            #This should be similar to the InfoGain?\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=max_tokens).fit(X,y)\n",
    "        else:\n",
    "            #default to ExtraTreeClassifier\n",
    "            estimator = ExtraTreeClassifier(random_state = seed)\n",
    "            #selector = SelectFromModel(estimator, max_features = tokens,threshold=-np.inf)\n",
    "            selector = SelectFromModel(estimator, max_features = max_tokens)\n",
    "            selector = selector.fit(X, y)\n",
    "\n",
    "    support_idx = selector.get_support(True)\n",
    "    \n",
    "    #print(\"Vocab:\", [vocab[i-1].replace(\"'\",\"\") for i in support_idx])\n",
    "    tokenizer2 = Tokenizer()\n",
    "    tokenizer2.fit_on_texts([vocab[i-1].replace(\"'\",\"\") for i in support_idx])\n",
    "    new_vocab = list(tokenizer2.word_index.keys())\n",
    "\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM -  Asthma : f1-score 0.0\n",
      "SVM -  Asthma : f1-macro 0.4752475247524752\n",
      "SVM -  Asthma : f1-micro 0.9056603773584906\n",
      "KNN -  Asthma : f1-score 0.0\n",
      "KNN -  Asthma : f1-macro 0.47000000000000003\n",
      "KNN -  Asthma : f1-micro 0.8867924528301887\n",
      "NB -  Asthma : f1-score 0.0\n",
      "NB -  Asthma : f1-macro 0.4752475247524752\n",
      "NB -  Asthma : f1-micro 0.9056603773584906\n",
      "Random Forest Accuracy Score ->  90.56603773584906\n",
      "RF -  Asthma : f1-score 0.0\n",
      "RF -  Asthma : f1-macro 0.4752475247524752\n",
      "RF -  Asthma : f1-micro 0.9056603773584906\n",
      "SVM -  CHF : f1-score 0.6298342541436464\n",
      "SVM -  CHF : f1-macro 0.5075776775305387\n",
      "SVM -  CHF : f1-micro 0.5379310344827586\n",
      "KNN -  CHF : f1-score 0.7522935779816514\n",
      "KNN -  CHF : f1-macro 0.5011467889908257\n",
      "KNN -  CHF : f1-micro 0.6275862068965518\n",
      "NB -  CHF : f1-score 0.771186440677966\n",
      "NB -  CHF : f1-macro 0.385593220338983\n",
      "NB -  CHF : f1-micro 0.6275862068965518\n",
      "Random Forest Accuracy Score ->  64.13793103448275\n",
      "RF -  CHF : f1-score 0.7739130434782608\n",
      "RF -  CHF : f1-macro 0.4536231884057971\n",
      "RF -  CHF : f1-micro 0.6413793103448275\n",
      "SVM -  Depression : f1-score 0.0\n",
      "SVM -  Depression : f1-macro 0.4308510638297872\n",
      "SVM -  Depression : f1-micro 0.7570093457943925\n",
      "KNN -  Depression : f1-score 0.0634920634920635\n",
      "KNN -  Depression : f1-macro 0.45092411393781257\n",
      "KNN -  Depression : f1-micro 0.7242990654205608\n",
      "NB -  Depression : f1-score 0.0\n",
      "NB -  Depression : f1-macro 0.4308510638297872\n",
      "NB -  Depression : f1-micro 0.7570093457943925\n",
      "Random Forest Accuracy Score ->  75.70093457943925\n",
      "RF -  Depression : f1-score 0.0\n",
      "RF -  Depression : f1-macro 0.4308510638297872\n",
      "RF -  Depression : f1-micro 0.7570093457943925\n",
      "SVM -  Diabetes : f1-score 0.5106382978723405\n",
      "SVM -  Diabetes : f1-macro 0.45739168779627387\n",
      "SVM -  Diabetes : f1-micro 0.46261682242990654\n",
      "KNN -  Diabetes : f1-score 0.8267477203647418\n",
      "KNN -  Diabetes : f1-macro 0.625495072303583\n",
      "KNN -  Diabetes : f1-micro 0.7336448598130842\n",
      "NB -  Diabetes : f1-score 0.8432432432432432\n",
      "NB -  Diabetes : f1-macro 0.4216216216216216\n",
      "NB -  Diabetes : f1-micro 0.7289719626168223\n",
      "Random Forest Accuracy Score ->  75.23364485981308\n",
      "RF -  Diabetes : f1-score 0.8555858310626703\n",
      "RF -  Diabetes : f1-macro 0.49336668602313843\n",
      "RF -  Diabetes : f1-micro 0.7523364485981309\n",
      "SVM -  Gallstones : f1-score 0.0\n",
      "SVM -  Gallstones : f1-macro 0.4472361809045226\n",
      "SVM -  Gallstones : f1-micro 0.8090909090909091\n",
      "KNN -  Gallstones : f1-score 0.0\n",
      "KNN -  Gallstones : f1-macro 0.4472361809045226\n",
      "KNN -  Gallstones : f1-micro 0.8090909090909091\n",
      "NB -  Gallstones : f1-score 0.0\n",
      "NB -  Gallstones : f1-macro 0.4472361809045226\n",
      "NB -  Gallstones : f1-micro 0.8090909090909091\n",
      "Random Forest Accuracy Score ->  80.9090909090909\n",
      "RF -  Gallstones : f1-score 0.0\n",
      "RF -  Gallstones : f1-macro 0.4472361809045226\n",
      "RF -  Gallstones : f1-micro 0.8090909090909091\n",
      "SVM -  Gout : f1-score 0.0\n",
      "SVM -  Gout : f1-macro 0.4674698795180723\n",
      "SVM -  Gout : f1-micro 0.8778280542986425\n",
      "KNN -  Gout : f1-score 0.0\n",
      "KNN -  Gout : f1-macro 0.4674698795180723\n",
      "KNN -  Gout : f1-micro 0.8778280542986425\n",
      "NB -  Gout : f1-score 0.0\n",
      "NB -  Gout : f1-macro 0.4674698795180723\n",
      "NB -  Gout : f1-micro 0.8778280542986425\n",
      "Random Forest Accuracy Score ->  87.78280542986425\n",
      "RF -  Gout : f1-score 0.0\n",
      "RF -  Gout : f1-macro 0.4674698795180723\n",
      "RF -  Gout : f1-micro 0.8778280542986425\n",
      "SVM -  Hypercholesterolemia : f1-score 0.4645161290322581\n",
      "SVM -  Hypercholesterolemia : f1-macro 0.5526043848624493\n",
      "SVM -  Hypercholesterolemia : f1-micro 0.5699481865284974\n",
      "KNN -  Hypercholesterolemia : f1-score 0.6861313868613139\n",
      "KNN -  Hypercholesterolemia : f1-macro 0.45913712200208556\n",
      "KNN -  Hypercholesterolemia : f1-micro 0.5544041450777202\n",
      "NB -  Hypercholesterolemia : f1-score 0.6785714285714285\n",
      "NB -  Hypercholesterolemia : f1-macro 0.6170634920634921\n",
      "NB -  Hypercholesterolemia : f1-micro 0.6269430051813472\n",
      "Random Forest Accuracy Score ->  63.212435233160626\n",
      "RF -  Hypercholesterolemia : f1-score 0.6978723404255319\n",
      "RF -  Hypercholesterolemia : f1-macro 0.6138368324644216\n",
      "RF -  Hypercholesterolemia : f1-micro 0.6321243523316062\n",
      "SVM -  Hypertriglyceridemia : f1-score 0.0\n",
      "SVM -  Hypertriglyceridemia : f1-macro 0.48325358851674644\n",
      "SVM -  Hypertriglyceridemia : f1-micro 0.9351851851851852\n",
      "KNN -  Hypertriglyceridemia : f1-score 0.0\n",
      "KNN -  Hypertriglyceridemia : f1-macro 0.48325358851674644\n",
      "KNN -  Hypertriglyceridemia : f1-micro 0.9351851851851852\n",
      "NB -  Hypertriglyceridemia : f1-score 0.0\n",
      "NB -  Hypertriglyceridemia : f1-macro 0.48325358851674644\n",
      "NB -  Hypertriglyceridemia : f1-micro 0.9351851851851852\n",
      "Random Forest Accuracy Score ->  93.51851851851852\n",
      "RF -  Hypertriglyceridemia : f1-score 0.0\n",
      "RF -  Hypertriglyceridemia : f1-macro 0.48325358851674644\n",
      "RF -  Hypertriglyceridemia : f1-micro 0.9351851851851852\n",
      "SVM -  OA : f1-score 0.0\n",
      "SVM -  OA : f1-macro 0.435483870967742\n",
      "SVM -  OA : f1-micro 0.7714285714285715\n",
      "KNN -  OA : f1-score 0.038461538461538464\n",
      "KNN -  OA : f1-macro 0.4512959866220736\n",
      "KNN -  OA : f1-micro 0.7619047619047619\n",
      "NB -  OA : f1-score 0.0\n",
      "NB -  OA : f1-macro 0.435483870967742\n",
      "NB -  OA : f1-micro 0.7714285714285715\n",
      "Random Forest Accuracy Score ->  77.14285714285715\n",
      "RF -  OA : f1-score 0.0\n",
      "RF -  OA : f1-macro 0.435483870967742\n",
      "RF -  OA : f1-micro 0.7714285714285715\n",
      "SVM -  OSA : f1-score 0.0\n",
      "SVM -  OSA : f1-macro 0.4632352941176471\n",
      "SVM -  OSA : f1-micro 0.863013698630137\n",
      "KNN -  OSA : f1-score 0.06666666666666667\n",
      "KNN -  OSA : f1-macro 0.49901960784313726\n",
      "KNN -  OSA : f1-micro 0.8721461187214612\n",
      "NB -  OSA : f1-score 0.0\n",
      "NB -  OSA : f1-macro 0.46454767726161367\n",
      "NB -  OSA : f1-micro 0.867579908675799\n",
      "Random Forest Accuracy Score ->  86.7579908675799\n",
      "RF -  OSA : f1-score 0.0\n",
      "RF -  OSA : f1-macro 0.46454767726161367\n",
      "RF -  OSA : f1-micro 0.867579908675799\n",
      "SVM -  Obesity : f1-score 0.5756457564575646\n",
      "SVM -  Obesity : f1-macro 0.3912711540908512\n",
      "SVM -  Obesity : f1-micro 0.44711538461538464\n",
      "KNN -  Obesity : f1-score 0.4807692307692307\n",
      "KNN -  Obesity : f1-macro 0.4807692307692307\n",
      "KNN -  Obesity : f1-micro 0.4807692307692308\n",
      "NB -  Obesity : f1-score 0.5714285714285714\n",
      "NB -  Obesity : f1-macro 0.4786967418546365\n",
      "NB -  Obesity : f1-micro 0.4951923076923077\n",
      "Random Forest Accuracy Score ->  56.25\n",
      "RF -  Obesity : f1-score 0.5955555555555555\n",
      "RF -  Obesity : f1-macro 0.5595578824898196\n",
      "RF -  Obesity : f1-micro 0.5625\n",
      "SVM -  CAD : f1-score 0.7295081967213116\n",
      "SVM -  CAD : f1-macro 0.6750989259468627\n",
      "SVM -  CAD : f1-micro 0.6842105263157895\n",
      "KNN -  CAD : f1-score 0.6844106463878327\n",
      "KNN -  CAD : f1-macro 0.5744633877100453\n",
      "KNN -  CAD : f1-micro 0.6028708133971292\n",
      "NB -  CAD : f1-score 0.7735849056603773\n",
      "NB -  CAD : f1-macro 0.5267924528301886\n",
      "NB -  CAD : f1-micro 0.6555023923444976\n",
      "Random Forest Accuracy Score ->  77.99043062200957\n",
      "RF -  CAD : f1-score 0.8391608391608391\n",
      "RF -  CAD : f1-macro 0.7453379953379953\n",
      "RF -  CAD : f1-micro 0.7799043062200957\n",
      "SVM -  Hypertension : f1-score 0.8877005347593583\n",
      "SVM -  Hypertension : f1-macro 0.44385026737967914\n",
      "SVM -  Hypertension : f1-micro 0.7980769230769231\n",
      "KNN -  Hypertension : f1-score 0.859504132231405\n",
      "KNN -  Hypertension : f1-macro 0.4486199906440044\n",
      "KNN -  Hypertension : f1-micro 0.7548076923076922\n",
      "NB -  Hypertension : f1-score 0.8877005347593583\n",
      "NB -  Hypertension : f1-macro 0.44385026737967914\n",
      "NB -  Hypertension : f1-micro 0.7980769230769231\n",
      "Random Forest Accuracy Score ->  79.8076923076923\n",
      "RF -  Hypertension : f1-score 0.8877005347593583\n",
      "RF -  Hypertension : f1-macro 0.44385026737967914\n",
      "RF -  Hypertension : f1-micro 0.7980769230769231\n",
      "SVM -  PVD : f1-score 0.0\n",
      "SVM -  PVD : f1-macro 0.44623655913978494\n",
      "SVM -  PVD : f1-micro 0.8058252427184465\n",
      "KNN -  PVD : f1-score 0.0\n",
      "KNN -  PVD : f1-macro 0.44623655913978494\n",
      "KNN -  PVD : f1-micro 0.8058252427184465\n",
      "NB -  PVD : f1-score 0.0\n",
      "NB -  PVD : f1-macro 0.44623655913978494\n",
      "NB -  PVD : f1-micro 0.8058252427184465\n",
      "Random Forest Accuracy Score ->  80.58252427184466\n",
      "RF -  PVD : f1-score 0.0\n",
      "RF -  PVD : f1-macro 0.44623655913978494\n",
      "RF -  PVD : f1-micro 0.8058252427184465\n",
      "SVM -  Venous Insufficiency : f1-score 0.0\n",
      "SVM -  Venous Insufficiency : f1-macro 0.47956403269754766\n",
      "SVM -  Venous Insufficiency : f1-micro 0.9214659685863874\n",
      "KNN -  Venous Insufficiency : f1-score 0.0\n",
      "KNN -  Venous Insufficiency : f1-macro 0.47956403269754766\n",
      "KNN -  Venous Insufficiency : f1-micro 0.9214659685863874\n",
      "NB -  Venous Insufficiency : f1-score 0.0\n",
      "NB -  Venous Insufficiency : f1-macro 0.47956403269754766\n",
      "NB -  Venous Insufficiency : f1-micro 0.9214659685863874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy Score ->  92.14659685863874\n",
      "RF -  Venous Insufficiency : f1-score 0.0\n",
      "RF -  Venous Insufficiency : f1-macro 0.47956403269754766\n",
      "RF -  Venous Insufficiency : f1-micro 0.9214659685863874\n",
      "SVM -  GERD : f1-score 0.0\n",
      "SVM -  GERD : f1-macro 0.42901234567901236\n",
      "SVM -  GERD : f1-micro 0.7513513513513513\n",
      "KNN -  GERD : f1-score 0.0\n",
      "KNN -  GERD : f1-macro 0.4254658385093168\n",
      "KNN -  GERD : f1-micro 0.7405405405405405\n",
      "NB -  GERD : f1-score 0.0\n",
      "NB -  GERD : f1-macro 0.42901234567901236\n",
      "NB -  GERD : f1-micro 0.7513513513513513\n",
      "Random Forest Accuracy Score ->  75.13513513513513\n",
      "RF -  GERD : f1-score 0.0\n",
      "RF -  GERD : f1-macro 0.42901234567901236\n",
      "RF -  GERD : f1-micro 0.7513513513513513\n"
     ]
    }
   ],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Tfidf_vect = TfidfVectorizer(max_features=600)\n",
    "Tfidf_vect_NS = TfidfVectorizer(max_features = 600, stop_words = cachedStopWords)\n",
    "\n",
    "for _,disease in enumerate(disease_list):\n",
    "    disease_data_df = all_df[all_df['disease'] == disease]\n",
    "    disease_data_ns_df = all_df_ns[all_df_ns['disease'] == disease]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(disease_data_df['text_final'], disease_data_df['judgment'], test_size=0.20, shuffle=True)\n",
    "    X_train_ns, X_test_ns, y_train_ns, y_test_ns = train_test_split(disease_data_ns_df['text_final'], disease_data_ns_df['judgment'], test_size=0.20, shuffle=True)\n",
    "    \n",
    "    Train_Y  = Encoder.fit_transform(y_train)\n",
    "    Test_Y  = Encoder.fit_transform(y_test)\n",
    "\n",
    "    Train_Y_NS  = Encoder.fit_transform(y_train_ns)\n",
    "    Test_Y_NS = Encoder.fit_transform(y_test_ns)\n",
    "    \n",
    "    Train_X_Tfidf = Tfidf_vect.fit_transform(X_train)\n",
    "    Test_X_Tfidf = Tfidf_vect.fit_transform(X_test)\n",
    "\n",
    "    Train_X_Tfidf_NS = Tfidf_vect_NS.fit_transform(X_train_ns)\n",
    "    Test_X_Tfidf_NS = Tfidf_vect_NS.fit_transform(X_test_ns)\n",
    "    \n",
    "    \n",
    "    #SVM\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf, y_train)\n",
    "    \n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_SVM)\n",
    "    f1_macro = f1_score(y_test, predictions_SVM,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_SVM,average='micro')\n",
    "\n",
    "    #print(\"SVM - \", disease, \": f1-score\", f1)\n",
    "    print(\"SVM - \", disease, \": f1-macro\", f1_macro)\n",
    "    print(\"SVM - \", disease, \": f1-micro\", f1_micro)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #KNN\n",
    "    # fit the training dataset on the KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=7)\n",
    "    clf = knn.fit(Train_X_Tfidf, y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_KNN = clf.predict(Test_X_Tfidf)\n",
    "\n",
    "    #auroc = roc_auc_score(truth, pred[:,1])\n",
    "    #f1 = f1_score(y_test, predictions_KNN)\n",
    "    f1_macro = f1_score(y_test, predictions_KNN,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_KNN,average='micro')\n",
    "\n",
    "    #print(\"KNN - \", disease, \": f1-score\", f1)\n",
    "    print(\"KNN - \", disease, \": f1-macro\", f1_macro)\n",
    "    print(\"KNN - \", disease, \": f1-micro\", f1_micro)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Naive Bayes\n",
    "    # fit the training dataset on the NB classifier\n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    Naive.fit(Train_X_Tfidf,y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_NB)\n",
    "    f1_macro = f1_score(y_test, predictions_NB,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_NB,average='micro')\n",
    "\n",
    "    #print(\"NB - \", disease, \": f1-score\", f1)\n",
    "    print(\"NB - \", disease, \": f1-macro\", f1_macro)\n",
    "    print(\"NB - \", disease, \": f1-micro\", f1_micro)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #RF\n",
    "    # fit the training dataset on the RF classifier\n",
    "    classifier=RandomForestClassifier(n_estimators =400,criterion=\"entropy\",random_state =0)\n",
    "    classifier.fit(Train_X_Tfidf,y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_RF = classifier.predict(Test_X_Tfidf)\n",
    "\n",
    "    #f1 = f1_score(y_test, predictions_RF)\n",
    "    f1_macro = f1_score(y_test, predictions_RF,average='macro')\n",
    "    f1_micro = f1_score(y_test, predictions_RF,average='micro')\n",
    "\n",
    "    #print(\"RF - \", disease, \": f1-score\", f1)\n",
    "    print(\"RF - \", disease, \": f1-macro\", f1_macro)\n",
    "    print(\"RF - \", disease, \": f1-micro\", f1_micro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://link.springer.com/article/10.1007/BF00994018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m estimator \u001b[38;5;241m=\u001b[39m ExtraTreesClassifier()\n\u001b[0;32m      5\u001b[0m selector \u001b[38;5;241m=\u001b[39m RFECV(estimator, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m selector \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrain_X_Tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(selector)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# fit the training dataset on the SVM classifier\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:710\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    707\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    708\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[1;32m--> 710\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    715\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[0;32m    716\u001b[0m scores_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:711\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    707\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    708\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[0;32m    710\u001b[0m scores \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m--> 711\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    713\u001b[0m )\n\u001b[0;32m    715\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[0;32m    716\u001b[0m scores_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:37\u001b[0m, in \u001b[0;36m_rfe_single_fit\u001b[1;34m(rfe, estimator, X, y, train, test, scorer)\u001b[0m\n\u001b[0;32m     35\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n\u001b[0;32m     36\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, test, train)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mscores_\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:283\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[1;32m--> 283\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[0;32m    286\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[0;32m    287\u001b[0m     estimator,\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[0;32m    289\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    290\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    439\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    442\u001b[0m ]\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:187\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:937\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m ):\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    411\u001b[0m         splitter,\n\u001b[0;32m    412\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[1;32m--> 420\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "#estimator = ExtraTreesClassifier()\n",
    "#selector = RFECV(estimator, step=1, cv=5)\n",
    "#selector = selector.fit(Train_X_Tfidf, y_train)\n",
    "\n",
    "#print(selector)\n",
    "\n",
    "# fit the training dataset on the SVM classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf, y_train)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy: \",accuracy_score(predictions_SVM, y_test)*100)\n",
    "\n",
    "f1 = f1_score(y_test, predictions_SVM)\n",
    "f1_macro = f1_score(y_test, predictions_SVM,average='macro')\n",
    "f1_micro = f1_score(y_test, predictions_SVM,average='micro')\n",
    "\n",
    "print(f1)\n",
    "print(f1_macro)\n",
    "print(f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the SVM classifier\n",
    "SVM_NS = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM_NS.fit(Train_X_Tfidf_NS, y_train_ns)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM_NS = SVM.predict(Test_X_Tfidf_NS)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM NS Accuracy: \",accuracy_score(predictions_SVM_NS, y_test_ns)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbours (kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://link.springer.com/article/10.1007/BF00153759"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN Accuracy:  64.82758620689654\n",
      "0.7733333333333333\n",
      "0.49435897435897436\n",
      "0.6482758620689655\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "clf = knn.fit(Train_X_Tfidf, y_train)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_KNN = clf.predict(Test_X_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"kNN Accuracy: \",accuracy_score(predictions_KNN, y_test)*100)\n",
    "\n",
    "#print(predictions_KNN)\n",
    "\n",
    "#auroc = roc_auc_score(truth, pred[:,1])\n",
    "f1 = f1_score(y_test, predictions_KNN)\n",
    "f1_macro = f1_score(y_test, predictions_KNN,average='macro')\n",
    "f1_micro = f1_score(y_test, predictions_KNN,average='micro')\n",
    "\n",
    "print(f1)\n",
    "print(f1_macro)\n",
    "print(f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the KNN classifier\n",
    "knn_ns = KNeighborsClassifier(n_neighbors=7)\n",
    "clf_ns = knn_ns.fit(Train_X_Tfidf_NS, y_train_ns)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_KNN_NS = clf_ns.predict(Test_X_Tfidf_NS)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"kNN NS Accuracy: \",accuracy_score(predictions_KNN_NS, y_test_ns)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1302.4964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  68.96551724137932\n",
      "0.8034934497816593\n",
      "0.5328942658744362\n",
      "0.6896551724137931\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,y_train)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, y_test)*100)\n",
    "\n",
    "f1 = f1_score(y_test, predictions_NB)\n",
    "f1_macro = f1_score(y_test, predictions_NB,average='macro')\n",
    "f1_micro = f1_score(y_test, predictions_NB,average='micro')\n",
    "\n",
    "print(f1)\n",
    "print(f1_macro)\n",
    "print(f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive_NS = naive_bayes.MultinomialNB()\n",
    "Naive_NS.fit(Train_X_Tfidf_NS, y_train_ns)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB_NS = Naive.predict(Test_X_Tfidf_NS)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes NS Accuracy Score -> \",accuracy_score(predictions_NB_NS, y_test_ns)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://link.springer.com/article/10.1023/A:1010933404324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy Score ->  73.79310344827587\n",
      "0.8362068965517241\n",
      "0.5905172413793103\n",
      "0.7379310344827587\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the RF classifier\n",
    "classifier=RandomForestClassifier(n_estimators =400,criterion=\"entropy\",random_state =0)\n",
    "classifier.fit(Train_X_Tfidf,y_train)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_RF = classifier.predict(Test_X_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Random Forest Accuracy Score -> \",accuracy_score(predictions_RF, y_test)*100)\n",
    "\n",
    "f1 = f1_score(y_test, predictions_RF)\n",
    "f1_macro = f1_score(y_test, predictions_RF,average='macro')\n",
    "f1_micro = f1_score(y_test, predictions_RF,average='micro')\n",
    "\n",
    "print(f1)\n",
    "print(f1_macro)\n",
    "print(f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the RF classifier\n",
    "classifier_ns = RandomForestClassifier(n_estimators = 400, criterion = \"entropy\", random_state = 0)\n",
    "classifier_ns.fit(Train_X_Tfidf_NS, y_train_ns)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_RF_NS = classifier_ns.predict(Test_X_Tfidf_NS)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Random Forest NS Accuracy Score -> \",accuracy_score(predictions_RF_NS, y_test_ns)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://onlinelibrary.wiley.com/doi/10.1002/rsa.3240050207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10pK5od01jfTHJyLN94dJxEube3sJszFm",
     "timestamp": 1678482671183
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
